{"/Fourier_Notes/notes/Fourier/Fourier%20Course%20Information": {
    "title": "Course notes in Fourier Analysis",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Fourier%20Course%20Information",
    "body": "Ofir David Description short description for Fourier analysis Syllabus Geometry in infinite dimensional spaces: Inner products, norms, distance, orthonormal systems Fourier series: Definitions, Dirichlet’s convergence theorem, … Fourier transforms: Definition,… Prerequisites Basic Linear Algebra: Linear system of equations, matrices, eigenvalues and eigenvectors. Inner Products: Inner products and norms, orthonormal bases, orthogonal projection. Calculus: Limits, continuity, derivatives and integrals on finite and infinite segments."
  },"/Fourier_Notes/notes/Fourier/Motivation": {
    "title": "Motivation",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Motivation",
    "body": "What do we analyze? The “Fourier Analysis” is one of the most useful tool when we try to study real (and complex) functions, which naturally appear in many places, for example: Physics: represent position, speed, force, temperature, etc. , Digital representations: like pixels in images, sound in audio file, or movement in animation “Abstract” quantities: stock market prices, or even measuring “popularity” in social media sites. These “real functions” come in several flavors, according to their domains, and the most common types are: Functions on the real line: $f:\\RR \\to \\RR$ , Periodic functions, namely $f:\\RR \\to \\RR$ , with some period $T>0$ , so that $f(x+T)=f(x)$ , Finite and discrete domain $f:\\{1,...,n\\} \\to \\RR$ , or Infinite and discrete domain $f: \\ZZ \\to \\RR$ , or as we usually call them sequences. Having so many “real functions” types, which represent so many objects in our world, it is very important to find mathematical structures and tools to study them. We already learned some of them in basic courses in calculus and algebra: First, we need to choose the type of functions: Bounded, Continuous, differentiable, piecewise continuous, integrable etc. Linear algebra: Usually this chosen family forms a vector space, allowing us to use all the tools from linear algebra. Calculus: Using the real line’s structure, we can look at continuity, derivative, integral, etc. Geometry: We can also measure the “size”, namely the norm, of a function in several ways. For example, the maximum distanance from zero $\\norm{f}_\\infty = \\sup|f(x)|$ , or measure the area beneath the graph of the function (in absolute value): [!figure]- Figure: These norms satisfy the well known triangle inequality $\\norm{f+g} \\leq \\norm f + \\norm g$ , which let us construct a geometry on our space, since we can measure distances, e.g. $$dist(f,g):=\\norm{f-g}.$$ Angles: More generally, we can think of “angles” between vectors, and draw intuition from the standard Euclidean spaces (e.g. the Pythagoras theorem). More formally, we add an inner product structure (as we shall see soon). The main idea in Fourier analysis, is that we add one more very interesting structure: Symmetry: These families usually have very nice “symmetries”. Usually we think of symmetries like reflections - we have the original object, we take its reflection though a mirror, and somehow get the “same” object (or at the very least it looks the same). Something similar happens with our real functions - for example, functions on the real line are “symmetric” to left and right translations: we can “move” functions to the left and right to get another function in the family : [!figure] Figure: $$ f(x) \\mapsto f(x+c) $$ We can think of this as advancing in position or time (depending on what our functions represent). This “symmetry” structure seems simple at first, however it leads to very interesting questions. Are there interesting “patterns” that our functions exhibit under this symmetries? For example, the function “repeats” itself when we move to the left or right? Are there “basic” patterns that we should look for when given such a function? This “pattern” search question is the main goal of our course, and in a sense the “Fourier analysis” can be thought of as pattern investigation of these functions. Remark: In our course we will mainly be interested in the periodic functions, and functions on the real line. In this study, we will naturally also see the space of infinite sequences. The finite discrete Fourier transforms are also very important, but will not be part of this course. However, they appear frequently in the theoretical side of computer science, if you wish to study more about them. “Real life” problems: Let see some examples for this pattern search in real life problems. Image compression In our modern day life, we take images and videos of almost everything that happens to us. More over, as our technology improves, we can save more and more information in each image. However, the problem with this approach is that this information takes a lot of space. One solution is to keep buying new space, whether it is a new hard drive or extra cloud space. Another solution is to try compressing the information instead. There are many compression algorithms and some of which (for example, the JPEG image compression) use ideas based on Fourier transforms. Pixelwise compression: Suppose we have a grayscale image file, where the color of each pixel is represented by an integer in $[0,255]$ going from black at $0$ all the way to white at $255$ . One very simple way to compress the image, is to change the pixel value from 255 options to something smaller, for example 16 or 8. This already gives us some compressions, and for some images it will be hard to notice the changes. However, as the number of pixel values decreases, we start to see jumps in colors. This is not too surprising, because we are trying to approximate our image by step (piecewise constant) functions. So even though two nearby pixels can have nearby color values originally, we can still have a jump in the color. This is because our compression looks at each pixel separately. To improve our compressions, we can try to exploit “patterns” in the image. For example, if there is a gradient section, namely the color changes linearly from one place to another, then we would only need two parameters to describe it (e.g. to write $ax+b$ ). Pattern compression: This idea of looking for patterns is measured in a sense in the Fourier transform coefficients. The Fourier transform “decomposes” the full image (and not each pixel separately) into sine and cosine wave patterns with weights which determines how “strong” that pattern is in the picture. For example, in a $10\\times10$ pixels image, there are $10\\cdot 10=100$ possible patterns, which you can see on the figure below on the left. Given any such image, we can for example keep only the “first” $[1,i]\\times [1,j]$ patterns, and plot it on the $(i,j)$ position as we see on the right with the cat picture. Note for example that the picture at the $(7,7)$ position only has $7\\cdot 7=49$ out of the $100$ patterns from the original picture (less than half!) and is already quite similar to the final image.     As in turns out, most weights in this pattern decomposition are quite small, and removing their corresponding patterns completely doesn’t change the image too much. Thus, in general we can throw away all the information about this small weight waves, and get a compression without losing too many details: For more details and some actual python code which runs the compressions above: Python image compression Some interesting YouTube videos about image compressions: Youtube: JPEG compression - By Computerfile Youtube: FFT image compression - By Steve Brunton To summarize, one way (of many) to think about Fourier transform is as extracting patterns from our data and the a weight for each pattern. If the data has nice “wavy” data, then the Fourier transform can help understanding it. Music and noise reduction When playing music on a piano (and many other instruments as well), we usually play different notes that sound “harmonious” together. For example, the chord C on the piano is a combination of the notes C,E,G. Each of these notes correspond to a “pure” sine sound wave with a given frequency, and when they are all played together, these waves sum up as can be seen in the image below.     Suppose now that we can hear this combination of notes together. Can we find out what are the notes being played? For example, can we extract from the C+E+G orange wave in the image, the C,E,G red, green and blue waves? This is one type of “pattern” search that Fourier transformation allows us to do. More over, when we are listening to the chord, there is probably a lot of background white noise, which we should think of as something “without any pattern”. Is it possible to extract from the “noisy” chord the original notes as well? This too can be done using Fourier transformation. See for example: Youtube: Denoising data with Fourier transformation - By Steve Brunton Youtube: Extract Musical Notes from Audio in Python with FFT - By Jeff Heaton Openprocessing: Visualizing Fourier transformation of audio Channel separation When opening a website online, usually our computer sends a request to our internet provider to send us exactly the information needed to view the website. However, when we watch television, or listen to the radio, our television and radio (usually) don’t send any requests, and instead get all the possible channels that we paid for, in a single stream of information. So the question is, how do they separate this single stream into the different channels? %% add some image with radio waves %% Here too, one of the ways of extracting these different channels is by in a sense coding them on different “Fourier patterns”. Once we find these patterns from the signal that we get, we can use it to reconstruct the original information sent over it. The tools of our craft What type of tools and ideas are we going to use? Divide and conquer - the orthonormal basis One of our main tools, is to decompose our large space of functions into smaller sections (=patterns), where we can understand each section separately, and once we do, combine them back together to understand our original function. You already learned about this idea back in your first course in linear algebra, which had the mysterious name of eigenvalues and eigenvectors. This was further expanded in the second course, about inner product spaces, where you saw the importance of geometry and working with orthonormal basis of eigenvalues. Here, we would take this idea to the next stage - the infinite dimensional space of functions - where our building blocks orthonormal basis would consist of the sine and cosine functions. Sine and cosine and the complex plane As mention several times, the sine and cosine functions are going to be our building blocks for the Fourier transform. At first glance, it is not clear why it happens and why they are so important, and in particular each one of one them on its own. However, they become quite interesting once combined - they are just two parts ( $x$ and $y$ coordinates) of the same very simple and very important process: Moving at constant speed on a circle. This clues us as to why these trigonometric functions arise so naturally - a lot of natural phenomena have natural “rotational” behavior, and when we see the sine or cosines function, it is usually just because we don’t see the “whole 2 dimensional picture”.     More over, this is also why many of the computations that will be done with sines and cosines in our course can be much more natural in the 2-dimensional complex plane. Recall (or learn in a few weeks) that by definition we have that $$e^{i\\theta} := \\cos(\\theta) + i\\sin(\\theta),$$ namely the sine and cosine functions are the imaginary and real parts of the nice (complex) exponential function. While complex functions seem a bit more complicated than real functions, they are in many ways much simpler, and in particular the exponential function is quite easy to work with. For example, it satisfies the well known exponential property of $e^{z+w}=e^z\\cdot e^w$ . While we won’t necessarily use this during the course, since complex functions are not a prerequisite, once you feel more at ease with the complex exponential function, you should go over the material and translate it to its complex notation. However, as a small taste for why this rotational behavior is so important, consider the following spring: Is the string moving to the right (which is our space’s “symmetry” action) or is it rotating around its axis? The rainbow colors might help to answer this question, but without them it would be impossible to decide. Indeed, the exponential property basically says that translation to the right and left $\\theta \\mapsto \\theta + \\phi$ is exactly the same as rotating since $e^{i(\\theta+\\phi)}=e^{i\\theta} \\cdot e^{i\\phi}$ (as an exercise, think what it means in the language of eigenvalues and eigenvectors). This transformation between left and right translation, and the simpler rotation is what stands at the mathematical heart of the Fourier transform. Some mathematical applications Differential equations Another important property that the sines and cosines (and even better, the exponential function) have, is that it is easy to differentiate them. More specifically $\\sin'(x)=\\cos(x)$ and $\\cos'(x)=-\\sin(x)$ . Such relations make it much simpler to solve differential equations where the functions are combinations of these sine and cosine functions. Luckily for us, the Fourier transform shows that most of the interesting functions can be written as such combinations, and this is quite useful for solving general differential equations. Number and group theory These Fourier transforms, and in particular it discrete finite versions, appear frequently in number theory when studying finite groups and fields. For example, they appear when studying Diophantine equations, namely integer solution for integral equations (e.g. solutions to $x^2 + y^2 = n$ for some integer $n$ ). In this area the “Fourier transforms” are usually called group representations."
  },"/Fourier_Notes/notes/Fourier/Inner%20product%20spaces%20-%20a%20reminder": {
    "title": "Inner product spaces - a reminder",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Inner%20product%20spaces%20-%20a%20reminder",
    "body": "The Euclidean prototype We start by recalling some elements from the theory of inner products, which are our ways to give standard vector spaces a new geometric structure. The prototype of this structure is the standard Euclidean dot product on $\\RR^3$ defined by $$\\angles {(x_1, x_2, x_3),(y_1,y_2,y_3)}=x_1y_1+x_2y_2+x_3y_2.$$ This dot product measures two very important quantities. First is the length of a vector: $$\\norm {(x_1,x_2,x_3)} := \\sqrt{\\angles {(x_1,x_2,x_3),(x_1,x_2,x_3)}} = \\sqrt{x_1^2+x_2^2+x_3^2}.$$ The second are angles between vectors - if $\\theta$ is the angle between two vectors $u,v$ , then $$\\cos(\\theta)=\\frac{\\angles {u,v}}{\\norm{u}\\norm{v}}.$$ In particular, two vectors are perpendicular, namely $\\theta = \\frac{\\pi}{2}$ if $$u\\perp v \\iff \\angles {u,v} = 0.$$ This structure stands at the heart of the standard Euclidean geometry that we live in, and can be used to describe many of the interesting properties that our world has. In particular, one of the first theorems that most of us learn about, and is essential to many of the results in geometry: The Pythagorean theorem”: If $u \\perp v$ are perpendicular, then $\\norm{u+v}^2 = \\norm u^2 + \\norm v^2$ The inner product With this geometric intuition, let us recall its generalization as “False[[invalid wikilink: Inner product]]”. In the following, our field will always be one of $\\FF = \\RR,\\CC$ . Definition: Inner Products Let $V$ be a vector space. An inner product on $V$ is a function $$\\angles{\\cdot,\\cdot}:V\\times V \\to \\FF$$ such that Positive: For any $v\\in V$ we have $\\angles {v,v} \\geq 0$ with equality if and only if $v=0.$ Hermitian: For any $u,v\\in V$ we have that $\\angles {u,v} = \\overline{\\angles {v,u}}$ . Linear in first coordinate: For any $u,v,w\\in V$ and $\\alpha \\in \\FF$ we have $$\\angles {u+\\alpha v,w} = \\angles {u,w} + \\alpha \\angles {v,w}.$$ Let’s recall some interesting examples for inner product spaces: Examples: Standard inner product: Over $\\FF^n$ we have the standard inner product defined by: $$\\angles {(x_1,...,x_n),(y_1,...,y_n)} = \\sum_1^n x_i\\overline{y_i}.$$ Integrals: For continuous functions in $[0,1]$ we have the inner product: $$\\angles {f,g} =\\int_0^1 f(x)\\overline{g(x)}\\dx$$ Weighted integral: For a nonnegative function $w:[0,1]\\to[0,\\infty)$ we define: $$\\angles {f,g}_w:=\\int_0^1f(x)\\overline{g(x)}w(x)\\dx$$ The last two examples are for infinite dimensional inner products spaces (where the second example is a specific case of the third one). If we extend our space of continuous functions even a little bit to piecewise continuous functions, we suddenly have two interesting problems that need to be solved: The zero function: Consider the function: This is clearly a piecewise continuous function with one discontinuity at $x=1$ . While it is not the zero function, it does satisfy $\\angles{f,f}=0$ , so that the function $\\angles{\\cdot,\\cdot}$ is no longer an inner product. However, this is the only problem with the definition, which can be fixed easily by “increasing” the zero function to contain such functions. More formally, we will say that two functions are equivalent $f_1\\sim f_2$ if $f_1-f_2$ is almost zero, in the sense that $\\angles{f_1-f_2, f_1-f_2}=0$ . The inner product is well defined modulo this equivalence, namely if $f_1 \\sim f_2$ and $g_1 \\sim g_2$ , then $\\angles{f_1,g_1}=\\angles{f_2,g_2}$ . This should not be two surprising because for piecewise continuous function $f_1 \\sim f_2$ just means that $f_1(x)=f_2(x)$ except for finitely many points. Infinite integrals: Consider the function $f:[0,1]\\to\\RR$ defined by $f(x)=\\cases{\\frac{1}{x} & x>0 \\\\ 0 & x=0}.$ This too is a piecewise continuous function with only one discontinuity at $x=0$ . However this time $\\angles{f,f}=\\int_0^1\\frac{1}{x^2}\\dx = \\infty$ , so our inner product doesn’t necessarily return finite numbers. This problem can be solved by restricting our space of functions to “small” enough functions, namely just $f$ which satisfy $$\\int_0^\\infty |f(x)|^2\\dx If we have two such functions $f,g$ , then $$\\abs{f(x)\\overline{g(x)}} \\leq \\max\\{\\abs{f(x)}^2,\\abs{g(x)}^2\\}\\leq \\abs{f(x)}^2 + \\abs{g(x)}^2.$$ It follows that the integral in the inner product $$\\angles {f,g} =\\int_0^1 f(x)\\overline{g(x)}\\dx$$ converges in absolute value (and is at most $\\int_0^1 \\abs{f(x)}^2 dx + \\int_0^1\\abs{g(x)}^2\\dx$ ), and therefore converges. Definition: The space of piecewise continuous functions. For a segment $I\\subseteq \\RR$ we define: $$E^2(I)=\\{f:I\\to \\CC\\;\\mid\\;f\\text{ piecewise continuous, }\\int_I\\abs{f(x)}^2\\dx We sometimes also denote it by $E(I)$ . [!remark] Remark: $\\mathcal{L}^2$ v-functions. This space can (and should be) extended a little bit, and instead of piecewise continuous function, we can talk about integrable functions, and then it is denoted by $\\mathcal{L}^2$ . However, for simplicity we will continue to work with the piecewise continuous functions. The norm Once we have the inner product (which “corresponds” to angles), we also have the norm (which is the “length” of a vector): Definition: Norms Let $V$ be a vector space. A norm on $V$ is a function $$\\norm {\\cdot} : V \\to \\RR$$ such that: Positive: For any $v \\in V$ we have that $\\norm v\\geq 0$ with equality if and only if $v=0$ . Absolute homogeneous: For any $v\\in V$ and $\\alpha \\in \\FF$ we have $$\\norm {\\alpha v} = |\\alpha|\\cdot \\norm v.$$ Triangle inequality: For any $u,v\\in V$ we have $$\\norm {u+v} \\leq \\norm u + \\norm v.$$ There are many examples of interesting norms, but probably one of the most useful one for us is the induced norm: Theorem: The induced norm. If $V$ is an inner product space, then $\\norm v := \\sqrt {\\angles {v,v}}$ is a norm function. In particular, in the Euclidean geometry, this norm is the standard $\\norm {x_1,...,x_n} = \\sqrt{\\sum_1^n |x_i|^2}$ , and in our new and improved integral inner product on functions, we get: $$ \\norm f = \\sqrt{\\int_0^1 |f(x)|^2\\dx }. $$ The importance of the norm functions, is that we use them to measure the size of vectors, and more over we can talk about distances between vector, by defining $$\\mathrm{dist}(u,v):=\\norm{u-v}.$$ Different norms define different distances, which have all sorts of relations between them. The norms above are usually called the $\\mathcal {L}^2$ norms (since we take the square and then square root), and denoted by $\\norm {\\cdot}_2$ . A couple more interesting norms that we will encounter later on are the $\\mathcal{L}^1$ and $\\mathcal {L}^\\infty$ norms, both in the continues and discrete cases, defined respectively by: $$\\begin{align} \\norm f_1 & := \\int_0^1 |f(x)|\\dx & \\norm {(x_1,...,x_n)}_1 & := \\sum_1^n |x_i| \\\\ \\norm f_\\infty & := \\sup|f(x)| & \\norm {(x_1,...,x_n)}_\\infty & := \\max_i |x_i| \\end{align}.$$ Actually, these are part of a family of norms called $\\mathcal{L}^p$ norms for $1\\leq p\\leq \\infty$ defined by: $$\\begin{align} \\norm f_p & := \\left(\\int_0^1 |f(x)|^p\\dx\\right)^{1/p} & \\norm {(x_1,...,x_n)}_p & := \\left(\\sum_1^n |x_i|^p\\right)^{1/p} \\end{align}.$$ For $p=1,2$ we get the $\\mathcal{L}^1$ and $\\mathcal{L}^2$ norms, and in the limit $p\\to\\infty$ we obtain the $\\mathcal{L}^\\infty$ norm. While there are similarities between the discrete and continuous case, the discrete finite dimensional norms are much more well behaved than their infinite dimensional analogues. We will later discuss more in depth about these infinite dimensional norms, but for now let’s get a little bit of visual intuition about the finite dimensional case, by simply drawing the unit circle $\\norm{(x,y)}_p=1$ in the plane. As you can see, the standard unit circle for $p=2$ is an actual circle, while for $p=1$ we get a diamond, and as $p\\to \\infty$ it approaches (and in the limit is) a square lined up to the X and Y axes. Also, for $p the formula defined above is not a norm, and we can “see” it, since the interior of the unit circle is not convex."
  },"/Fourier_Notes/notes/Fourier/Orthogonal%20sets%20-%20a%20reminder": {
    "title": "Orthogonal sets - a reminder",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Orthogonal%20sets%20-%20a%20reminder",
    "body": "Once we have a vector space, usually the next step is finding a nice basis. In the finite dimensional case, we had the standard basis, denoted by $\\{e_1,...,e_n\\}$ where $$e_i= (0,...,0,\\overbrace{1}^i,0,...,0).$$ This is a simple yet powerful basis which arises naturally and used in many places. One of the main reasons for its usefulness is that it also has nice “geometric” properties, and in a sense it provides a “direction” to the space. Definition and basic properties Trying to understand and generalize this properties, led to the definition of orthonormal bases. Definition: Orthogonal and orthonormal bases Let $V$ be an inner product space and let $\\Omega \\subseteq V$ be a subset. The set is called: Orthogonal: if $0 \\not \\in \\Omega$ and any distinct pair of vectors $v\\neq u$ in $\\Omega$ are perpendicular, namely $\\angles {u,v} =0$ . Orthonormal: if in addition $\\norm v = 1$ for all $v\\in \\Omega.$ We can also write the orthonormality condition it in a more “compact” way. For any $u,v\\in \\Omega$ : $$\\angles {u,v}=\\begin{cases}1 & u = v \\\\0 & u\\neq v\\end{cases}.$$ The main advantage of this orthogonality definition, is that it lets us take our intuition from the standard Euclidean geometry to general spaces. In particular we have the following: Theorem: Orthonormal coefficients Suppose that $\\{v_1,...,v_n\\}\\subseteq V$ is an orthogonal set. Then: Pythagoras: $\\norm {\\sum_1^n v_i}^2 = \\sum_1^n \\norm {v_i}^2$ . Coefficients: If $v\\in span \\{ v_1, ..., v_n \\}$ , namely $v=\\sum_1^n \\alpha_i v_i$ , then $\\alpha_i = \\frac{\\angles {v,v_i}}{\\norm {v_i}^2}$ . In particular for an orthonormal set we get $$v = \\sum_1^n \\angles {v,v_i} v_i \\; , \\text{ and } \\; \\norm{v}^2 = \\sum_1^n |\\angles {v,v_i}|^2.$$ Proof: Under the assumption that $\\angles {v_i,v_j} = 0$ for $i\\neq j$ , we get that $$\\norm{\\sum_1^n v_i}^2 = \\angles {\\sum_{i=1}^n v_i, \\sum_{j=1}^n v_j}=\\sum_{i,j=1}^n \\angles{v_i,v_j}=\\sum_{i=1}^n \\angles{v_i,v_i}=\\sum_1^n \\norm {v_i}^2.$$ This is a straight forward computation: $$\\angles {v,v_j}= \\angles{\\sum_{i=1}^n \\alpha_i v_i,v_j} = \\sum_{i=1}^n \\alpha_i \\angles{v_i,v_j} = \\alpha_i \\norm {v_i}^2$$ As a simple example you should have in mind, in the standard Euclidean space, for a vector $\\bar{x}=(x_1,...,x_n)$ the i’th coordinate, namely the i’th coefficient in the standard basis, is just $x_i = \\angles{\\bar{x}, e_i}$ . The last theorem already suggests that having a basis of orthonormal vectors can be quite helpful, since it is very easy to find the coefficients (both theoretically, and if needed numerically). Moreover, whenever we have an orthogonal set, it is already half of its way to being a basis: Lemma: Any orthogonal set is linearly independent. In particular, in an inner product space of dimension $n$ , an orthogonal set of size $n$ is a basis. Proof: Let $\\Omega$ be an orthogonal set, and suppose that we can write zero as linear combination of its elements (which by definition is a finite combination). This means that $$ 0 = \\sum_1^n \\alpha_i \\omega_i $$ for some elements $\\omega_i \\in \\Omega$ . However, since the set is orthogonal, we know that $\\alpha_i=\\frac{\\angles {0,\\omega_i}}{\\norm {\\omega_i}^2}=0$ , so that this combination is trivial, and we conclude that $\\Omega$ is linearly independent. In an infinite dimensional space, life is much more interesting, and the definition of a spanning set, and therefore of a basis is slightly different, though it has the same intuition (it spans, but with limits). For now, lets see a couple of interesting examples in this infinite dimensional case with the inner product $\\angles{f,g}=\\int_0^1 f(x)g(x)\\dx$ . Example: Orthogonal step functions Consider the following set of functions which contains the constant 1 function, and for each integer $0\\leq p$ , and integer $0\\leq n \\leq 2^p - 1$ the function $$s_{p,i}(x) = \\begin{cases}-1 & \\frac{n}{2^p} In words: each such function chooses a section of length $1/2^p$ and has $-1$ on half of it and $+1$ on the second half. It should not be very hard to convince yourselves that these functions form an orthogonal set (prove it!). What would you change to make it an orthonormal set? The second, and our main, example, is the smooth version of the previous one. Theorem: Sines and Cosines Consider the inner product on functions on $[-\\pi,\\pi]$ defined by $$\\frac {1}{\\pi}\\int_{-\\pi}^\\pi f(x)\\overline{g(x)}\\dx .$$ The set of functions $$\\Omega=\\{\\cos(kx) : k\\geq 0\\} \\cup \\{\\sin(kx) : k\\geq 1\\}$$ is (almost) orthonormal - the only exception is that $cos(0x)\\equiv 1$ has norm $2$ instead of $1$ . This last theorem is quite important that this is going to be our Fourier patterns. The proof of elementary, and you should try to do it by yourself. Once you do have a proof, you are welcome to check on of these three proof methods: Proof: Using **Complex integrations**: If you know some complex integration, this is a very simple exercise using exponentials. For example, writing $\\cos(nx)=\\frac{e^{inx}+e^{-inx}}{2}$ , we get that $$\\align{\\angles {\\cos(mx),\\cos(nx)} & =\\angles {\\frac{e^{imx}+e^{-imx}}{2},\\frac{e^{inx}+e^{-inx}}{2}} \\\\ & =\\frac {1}{4}(\\angles {e^{imx},e^{inx}} + \\angles {e^{imx},e^{-inx}} + \\angles {e^{-imx},e^{inx}} + \\angles {e^{-imx},e^{-inx}})}.$$ These expressions are easy to compute: $$\\angles {e^{inx},e^{imx}} = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi e^{i(m-n)x}dx.$$ If $m=n$ , then the integrand is simply $1$ , so the integral is $\\frac{2\\pi}{\\pi} = 2$ . Otherwise, when $m-n\\neq 0$ we get that $$\\angles {e^{inx},e^{imx}} = \\frac{1}{\\pi} \\frac{1}{i(m-n)}e^{i(m-n)x} \\mid_{-\\pi}^\\pi = 0.$$ Proof: Using **Trigonometric identities**: Ignoring the complex integration, we can use directly the trigonometric equalities. Recall that the product of two cosine satisfies: $$\\cos(\\theta)\\cos(\\varphi) = \\frac {\\cos(\\theta-\\varphi)+\\cos(\\theta+\\varphi)}{2}.$$ Using this equality, we obtain that for $m,n\\geq 0$ we have that $$\\begin{align} \\angles {\\cos(mx),\\cos(nx)} & =\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\cos(mx)\\overline{\\cos(nx)} \\dx \\\\ & =\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\frac{\\cos((m-n)x)+\\overline{\\cos((m+n)x)}}{2} \\dx = \\begin{cases} 2&m=n=0\\\\ 1&m=n\\geq 1 \\\\ 0 & m\\neq n \\end{cases}. \\end{align} $$ A similar computation can be done for the rest of the inner products. Proof: Using **Integration tricks**: Finally, one more way is by using some basic integral operations in an interesting way. The integral above for $m=n=0$ is trivial, and in general it is symmetric, so let’s assume that $m\\neq 0$ . Using integration by parts, with the idea that integrating\\differentiating $\\cos(x)$ twice we return to the same place, we obtain the following: $$\\begin{align}\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\cos(mx)\\overline{\\cos(nx)} \\dx & = \\overbrace{\\frac{1}{m\\pi}\\sin(mx)\\cos(nx)\\mid _{-\\pi}^\\pi}^{=0} + \\frac {n}{m\\pi}\\int_{-\\pi}^\\pi \\sin(mx)\\sin(nx) \\dx\\\\& = -\\overbrace{\\frac {n}{m^2\\pi} \\cos(mx)\\sin(nx)\\mid_{-\\pi}^\\pi} +\\frac {n^2}{m^2\\pi} \\int_{-\\pi}^\\pi \\cos(mx)\\cos(nx) \\dx\\end{align}.$$ It then follows that $$\\left(1-\\frac{n^2}{m^2}\\right)\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\cos(mx)\\overline{\\cos(nx)} \\dx = 0,$$ so unless $m=n$ , the integral must be $0$ . As for the $m=n\\geq 1$ case, namely integrating $\\cos^2(nx)$ , you should first convince yourself that $\\int_{-\\pi}^\\pi \\cos^2(nx) \\dx = \\int_{-\\pi}^\\pi \\sin^2(nx) \\dx$ (why?). Given that, we obtain that $$2\\int_{-\\pi}^\\pi \\cos^2(nx) \\dx = \\int_{-\\pi}^\\pi \\cos^2(nx) \\dx+\\int_{-\\pi}^\\pi \\sin^2(nx) \\dx =\\int_{-\\pi}^\\pi 1 \\dx = 2\\pi,$$ so we conclude that for $n\\geq 1$ we have $$ \\angles {\\cos(nx),\\overline{\\cos(nx)}} = 1.$$ The importance of orthogonal, and in particular orthonormal bases, is that it is very easy to find the coefficients in these bases. Orthogonal projection At this point we already have one very interesting property of orthogonal (and better yet) orthonormal sets - it is very easy to find the coefficients for orthogonal bases. However, these sets have more interesting properties, and one of them is related to a very interesting problem that rises up naturally in many places: Problem: Given a point $v\\in V$ and a set $W\\subseteq V$ , find the distance from $v$ to $W$ , and if possible find the closest point to $v$ . It is not hard to see why this sort of problem appear in many places, since it basically looks for the shortest way from one point to get to some set. Just to make it even more interesting, sometimes this problem is hidden in other problems as well. For example, the famous problem of given a collection of points on the plane, and looking for the best linear approximation, is not only the same problem as above, but with the extra detail that $W$ is a subspace: When $W$ is a subspace, the solution in general is simple, where the main idea is what we expect to see in the standard Euclidean plane: True[[invalid wikilink: Fourier/images/orthogonality/orthogonal projection.png]] As the image suggests, the point $w\\in W$ closest to $v$ is its orthogonal (perpendicular) projection on $W$ . Finally, the most important part, is that this orthogonal projection is very easy to compute: Theorem: Orthogonal projection Let $W\\leq V$ be a finite dimensional subspace, and let $\\{w_1,...,w_k\\}$ be an orthonormal basis for $W$ . The orthogonal projection to $W$ is defined by $$P_W(v):=\\sum_1^k \\angles {v,w_i}w_i.$$ This orthogonal projection of $v$ is the closest point to $v$ in $W$ . Examples Example: Orthogonal step function approximation Let’s try to approximate the function $f(x)=x$ using the orthogonal step functions we defined previously, and we shall use only the first four such functions. (soon to be pictures) These four function form an orthogonal set, and in order to use the theorem we need to normalize them, namely $\\hat{f}_i := \\frac{f_i}{\\norm {f_i}}$ , so that $$ \\hat{f}_1 = f_1, \\;\\; \\hat{f}_2 = f_2, \\;\\; \\hat{f}_3 = \\sqrt{2}f_3, \\;\\; \\hat{f}_4 = \\sqrt{2}f_4.$$ The coefficients are then going to be $$\\angles{x,\\hat{f}_1}=\\frac{1}{2}, \\;\\; \\angles{x, \\hat{f}_2}=\\frac{1}{4}, \\;\\; \\angles{x, \\hat{f}_3} = \\angles{x, \\hat{f}_4} =\\frac{1}{16}. $$ This means that we get the approximation: $$ x \\sim \\frac{1}{2}\\cdot f_1 + \\frac{1}{4}\\cdot f_2 + \\frac{1}{16} \\cdot 2 \\cdot (f_3+f_4).$$ And finally, unsurprisingly, we get the approximation: Exercise Compute $\\angles{x, f_i}$ for any of the step function in the orthogonal basis. Do it one time directly using the integral, and one time “geometrically”. Try to conjecture how the approximation will look like, when taking all the functions with $p\\leq P$ for some positive integer $P$ , and then prove it. Try to do the same for other function (e.g. $f(x)=x^2$ ) - can you say in general how the approximation will look like? Note that in the example above we had $P=2$ since the corresponding segments of our functions were of length $1=1/2^0$ , $1/2$ and $1/2^2$ . Example: Approximation with sine waves. Similar to the previous example, we can try to approximate $f(x)=x$ using sine waves (explain why we don’t need cosines). Add some explanations… There was a very interesting phenomenon with the last approximation: The “distance” between the approximation and our original $f(x)=x$ function goes to zero. In our case this means that the integral $$\\int_{-\\pi}^\\pi (f(x)-2\\sum_1^N (-1)^{k+1} \\frac{\\sin(kx)}{k})^2 \\dx \\to 0.$$ In other words, the area (more or less, up to the square in the integrand) between the function goes to zero. It does not mean that there is a pointwise convergence! For example, in all of our approximations, the values in both $x=\\pm \\pi$ remain zero, while the value of the original function is $\\pm \\pi$ . However, we do have pointwise convergence in ALL the other points. This leads to a very interesting question, which we will later try to solve: Problem: When does the norm convergence implies also pointwise convergence? And if there isn’t pointwise convergence, then what can we say? Exercise: Sine and cosine approximation To get some intuition about the last question, try to approximate some other “nice” function and try to guess the answer for this question for those functions. Try it on a range of families of functions like: constant, linear, polynomial, exponential, unbounded function, noncontinuous, etc."
  },"/Fourier_Notes/notes/Fourier/Infinite%20Orthonormal%20basis": {
    "title": "Infinite Orthonormal basis",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Infinite%20Orthonormal%20basis",
    "body": "Orthonormal basis - from finite to infinite The concept of a basis is fundamental in linear algebra (and generally in mathematics). Its formal definition is just a set of linearly independent vectors which span the given vector space. The other equivalent definition, was that $\\{v_1,...,v_n\\}$ is a basis for $V$ , if and only if any vector $v\\in V$ has a unique presentation as a linear combination $$v = \\sum_1^n \\alpha_i v_i.$$ This allowed us to identify between the abstract vector $v$ and the coordinate vector $(\\alpha_1,...,\\alpha_n)$ (Though remember that different bases produce different coordinate vectors). Moreover, when our vector space can be spanned by finitely many vectors, we have seen that not only there exists a finite basis to the space, but all of the bases have the same size, which we call the dimension of the vector space. This has led to all sorts of interesting results, for example a linearly independent set which is the size of the dimension of the space, is also spanning, and therefore a basis. When dealing with “non finite dimensional” vector spaces, we need to reconsider the definition of a basis. Suddenly finite sums might not be enough to describe the whole space. For simplicity, consider a countable infinite set $S=\\{v_i \\mid i\\in \\NN\\}$ , for which we ask whether it is spanning or linearly independent. There are two main approaches: The algebraic approach: Here we only know how to add finitely many vectors, so the set $S$ is spanning, if every vector is a finite linear combination of finitely many vectors in $S$ , and it is linearly independent if any finite subset of $S$ is linearly independent. The analytic approach: In special and interesting cases, we might be able to define infinite sums, namely $\\sum_1^\\infty \\alpha_i v_i$ , and then the definition is the analogue to what we have for finite sets. As the spaces we deal with are special and interesting, this will be our next step. Convergence in norm As usually, before talking about the limit of infinite sums, we need to define what are limits. Definition: Convergence in norm Let $V$ be a normed vector space. We say that a sequence of vectors $v_n$ converges in norm to $v$ in $V$ if $\\norm{v_n - v}\\to 0$ and we denote it by $v_n \\overset{\\norm{\\cdot}}{\\longrightarrow} v$ , or just $v_n \\to v$ if there is no ambiguity. Remark: Note that we have defined convergence with convergence, namely: $$v_n \\normto v \\iff \\norm{v_n -v}\\to 0.$$ This is not a cyclic definition, since the new norm convergence on the left is defined by the old known convergence of real numbers on the right. Before we go over a few examples to get some intuition, recall our three “main” norms on function spaces (say on $[0,1]$ ): $$\\align{\\norm{f}_1& = \\int_0^1|f(x)|\\dx \\\\ \\norm{f}_2& = \\sqrt{\\int_0^1|f(x)|^2\\dx} \\\\ \\norm{f}_\\infty& = \\sup_{0\\leq x\\leq 1} |f(x)|. } $$ The $\\mathcal{L}^2$ norm is the induced norm from the standard inner product, and comes with all the benefits of such a norm. Unless said otherwise, the “norm” convergences will mean that norm. While the $\\cl^1$ and $\\cl^\\infty$ are not induced norm, they have nice geometric intuition: The norm $\\norm{f}_1$ measures the area between $f$ and the $x$ -axis in absolute value. In particular two functions are close in this norm is the area between them is small. The norm $\\norm{f}_\\infty$ measures the “maximal” (supremum) distance of $f$ from the $x$ -axis. In particular, a function $f$ is close to $g$ in the infinity norm if it is in a small strip around $g$ (Add images) Example 1: The functions $f_n(x) = \\frac{x}{n}$ Looking at the picture, we guess that the functions should converge to the zero function, and this is indeed the case for all of the norms above. Indeed, we have that: $$\\align{\\norm{f_n}_1&=\\frac{1}{2n} \\to 0 \\\\ \\norm{f_n}_2&=\\frac{1}{\\sqrt{3n^2}} \\to 0 \\\\ \\norm{f_n}_\\infty &=\\frac{1}{n} \\to 0 .}$$ Example 2: The functions $f_n(x) = x^n$ This time, the functions look a bit more complicated, but still “seem” to be converging to the zero function. Let’s check if this is true: $$\\align{\\norm{f_n}_1&=\\frac{1}{n+1} \\to 0 \\\\ \\norm{f_n}_2&=\\frac{1}{\\sqrt{2n+1}} \\to 0 \\\\ \\norm{f_n}_\\infty &=1 \\not\\to 0 .}$$ This time, unlike the previous example, the sequence converge to zero both in the $\\mathcal{L}^1$ and $\\mathcal{L}^2$ but not in $\\mathcal{L}^\\infty$ . Excercise: Show that in the space $C[0,1]$ of continuous functions with the $\\cl^\\infty$ norm, the sequence $x^n$ of functions does not converge. Corollary: Different norm can have different convergence of the same sequence. Excercise: Show that for any $f\\in C[0,1]$ we have that $\\norm{f}_1 \\leq \\norm{f}_2 \\leq \\norm{f}_\\infty$ . Conclude that if $f_n$ converges in $\\cl^\\infty$ , then it converges in both $\\cl^1$ and $\\cl^2$ , and similarly if $f_n$ converges in $\\cl^2$ , then it converges in $\\cl^1$ . So intuitively, the infinity norm is the “strongest norm”: convergence there implies convergence “everywhere”. Pointwise convergence One more important type of convergence for functions which we learned about in calculus, which is not a norm convergence (why?) is the pointwise convergence: Definition: Pointwise convergence of functions We say that a sequence of functions $f_n$ converges pointwise in a set $X$ , if $f_n(x)$ is a convergent sequence for any $x\\in X$ . Lemma: If $f_n \\to f$ in $\\cl^\\infty$ (unifrom convergence) then $f_n \\to f$ pointwise. The last lemma is an interesting one, and helped quite a lot in calculus. It meant that whenever we want to look for uniform convergence, the only candidate was the pointwise limit (if exists) which is usually much easier to find. Unfortunately, there is no such connection with the induced norm. Example: Pointwise convergence doesn't imply $\\norm{\\cdot}_2$ convergence Consider the functions $f_n(x)=\\cases{n& x\\in(0,\\frac{1}{n}) \\\\ 0 & else}$ . Then this sequence converges pointwise to the zero function (check!) however $$\\norm {f_n - 0}_2^2 = \\int_0^{1/n} n^2 \\dx =n \\not \\to 0 . $$ Example: $\\norm{\\cdot}_2$ convergence doesn't imply pointwise convergence The idea is to build functions which are (1) have very small area, and (2) this area “shifts” around. Then (1) will imply norm convergence, and (2) will mean that no one point will actually converge. More specifically, our functions will be characteristic (step) functions which are 1 on segment of size $\\frac{1}{2^n}$ and zero otherwise, and we move around this segment. Formally, the first few functions are: $$\\align{f_1(x) \\equiv 1=\\chi_{[0,1]} \\\\ f_2(x)=\\chi_{[0,1/2]} \\; &, \\; f_3(x) = \\chi_{[1/2,1]} \\\\ f_4 = \\chi_{[0,1/4]} &, \\; f_5=\\chi_{[1/4,2/4]}\\;,\\; f_6=\\chi_{[2/4,3/4]}\\;,\\; f_6=\\chi_{[3/4,1]}}$$ Since our step functions always have height 1, then the normed squared equals to the area, which in turn equals to the width $\\frac{1}{2^n}\\to 0$ . On the other hand, this sequence doesn’t converge point wise to any point! Continuous functions Given an inner product space $V$ with the inner product $\\angles{\\cdot,\\cdot}$ , we now know how to generate a norm $\\norm{v}:=\\sqrt{\\angles{v,v}}$ , and from the previous section use this norm to define limits. Of course, once we have “limits” we can ask whether given functions are continuous, and fortunately our standard arithmetic functions are continuous. Theorem: Inner products are continuous Let $V$ be an inner product space and suppose that $v_n \\normto v$ and $u_n \\normto u$ . In addition, let $\\alpha_n \\to \\alpha$ in the field. Then the following functions are continuous: Addition: $v_n + u_n \\normto v + u$ . Negation: $-v_n \\normto -v$ . Scalar multiplication: $\\alpha_n v_n \\to \\alpha v$ . Inner product: $\\angles{v_n , u_n} \\to \\angles{v,u}$ . Norm: $\\norm {v_n} \\to \\norm {v}$ . Proof: All of the proofs are similar to the standard continuity proofs in $\\RR$ . Let’s prove as example only parts (4) and (5). Indeed, for part (4) we have $$\\align{|\\angles{v_n, u_n}-\\angles{v, u}| & \\leq |\\angles{v_n, u_n}-\\angles{v_n, u}| + |\\angles{v_n, u}-\\angles{v, u}| \\\\ &= |\\angles{v_n, u_n - u}| + |\\angles{v_n - v, u}| \\leq \\norm{v_n}\\norm{u_n-u} + \\norm{v_n-v}\\norm{u} = (*) ,} $$ where the last inequality is the Cauchy-Shwartz inequality. By assumption, we have that $\\norm{u_n-u} , \\norm{v_n-v} \\to 0$ . Since $\\norm{u}$ is constant and $\\norm{v_n} \\leq \\norm{v_n-v} + \\norm{v}$ is bounded, we conclude that $(*)$ converges to zero, which is exactly what we wanted to show. Part (5) now follows by taking $u_n=v_n$ , together with the fact that the square root function is continuous. $\\square$ Infinite sums and complete orthonormal bases Now that we have the definition of limits in our normed space, we can define infinite sums $\\sum_1^\\infty v_k$ as the limit (if exists) of the partial sums $\\sum_1^N v_k$ . Remark: Some notations issues Since we work with convergence of functions, where norm convergence and pointwise convergence are different, we will use $\\sum_1^\\infty \\angles{v,e_k}e_k \\sim v$ to indicate norm convergence and $\\sum_1^\\infty \\angles{v,e_k}e_k = v$ for pointwise convergence. Later we will see that they coincide in many cases. With this new infinite sum definition and the continuity we saw above, many of the results from finite dimensional spaces carry on naturally to infinite dimensional spaces. For example: Lemma: Coefficients of orthonormal sets Suppose that $\\{e_k|k\\in\\NN\\}$ is an orthonormal set, and $v=\\sum_1^\\infty \\alpha_k v_k$ . Then $\\alpha_n = \\angles{v, e_n}$ . Proof: We have that $$\\align{ \\angles{v, e_n} & = \\angles{\\sum_1^\\infty \\alpha_k e_k, e_n} = \\angles{\\limfi{N} \\sum_1^N \\alpha_k e_k, e_n} \\\\ &= \\limfi{N} \\angles{ \\sum_1^N \\alpha_k e_k, e_n}},$$ where in the last equality we used the continuity of inner products. Since $\\angles{ \\sum_1^N \\alpha_k e_k, e_n}=\\alpha_n$ for any $N\\geq n$ , we see that the limit is $\\alpha_n$ and we are done. With this in mind, we can define our new infinite orthonormal basis. Definition: An orthonormal basis Let $V$ be an inner product space, and $\\{e_k\\}$ an orthonormal set. We say that it is a orthonormal basis if every $v\\in V$ is the limit $\\sum_1^n\\angles{v, e_k}e_k \\normto v$ . Note that by the definition of a complete orthonormal basis each vector has a presentation $v\\sim\\sum_1^\\infty \\alpha_k e_k$ , and by the lemma above it is unique, namely $\\alpha_k=\\angles{v, e_k}$ , or in other words we again have a unique presentation for every vector, just like for finite dimensional vector spaces and their bases. We can now define the main orthonormal basis that we work with. Theorem: The Fourier system $E[-\\pi, \\pi]$ Let $V=E[-\\pi,\\pi]$ be the vector space of piecewise continuous functions on $[-\\pi,\\pi]$ , with the inner product $\\angles{f,g}:=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)\\overline{g(x)} \\dx$ . Then the functions $$\\{\\frac {1}{2}\\}\\cup\\{\\cos(nx) \\mid n\\geq 1\\} \\cup \\{\\sin(nx) \\mid n\\geq 1\\}$$ form a complete orthonormal basis. Proof: We already know that this system is orthonormal. The proof that it is “spanning” is more complicated and we will not prove in this course. However, for those interested, you should read about the Stone-Weierstrass theorem. We can now extend two of the results we know for finite dimensional inner product spaces to the infinite dimensional case: Claim: Let $V$ be an inner product space with an orthonormal system $\\{e_k \\mid k\\in\\NN\\}$ . Bessel inequality: For every $v\\in V$ we have $\\sum_1^\\infty |\\angles{v,e_k}|^2 \\leq \\norm{v}^2$ . Parseval identity: If the orthonormal system is a basis, then there is an equality $\\sum_1^\\infty |\\angles{v,e_k}|^2 = \\norm{v}^2$ for all $v$ . On the other hand, if there is such an equality for all $v$ , then the system is basis. Generalized Parseval identity: If the orthonormal system is a bsis and $u,v$ are any two vectors, then: $$\\angles{u,v} = \\sum_1^\\infty \\angles{u,e_i}\\overline{\\angles{v,e_i}}.$$ Proof: The claim here is in a sense the limit of the analogue claim for finite dimensional spaces, indeed, the main object here is none other than $$\\sum_1^\\infty |\\angles{v,e_k}|=\\limfi{N} \\sum_1^N |\\angles{v,e_k}| = \\norm{\\limfi{N} \\sum_1^N \\angles{v,e_k}e}.$$ Moreover, recall that these approximations are the orthogonal projections of $v$ to the spaces spanned by $\\{e_1,..., e_N\\}$ , so that we additionally have that $$\\sum_1^N \\angles{v,e_k}e \\perp (v-\\sum_1^N \\angles{v,e_k}e),$$ and therefore $$\\norm{v}^2 = \\norm{\\sum_1^N \\angles{v,e_k}e}^2 + \\norm{v-\\sum_1^N \\angles{v,e_k}e}^2 =(*).$$ The orthogonal projection in $(*)$ implies the standard finite dimensional Bessel inequality $\\sum_1^N |\\angles{v,e_k}|^2 \\leq \\norm{v}^2$ for any $N$ , and taking the limit produces the general Bessel inequality. Using $(*)$ we see that $\\norm{\\sum_1^N \\angles{v,e_k}e} \\to \\norm{v}$ if and only if $\\norm{v-\\sum_1^N \\angles{v,e_k}e} \\to 0$ or equivalently $\\sum_1^N \\angles{v,e_k}e \\normto v$ . Hence, Parsevel identity holds for all $v\\in V$ if and only if the system is complete. Finally, for an orthonormal basis, using the continuity of the inner product, we obtain that : $$\\align{\\angles{u,v}&=\\angles{\\sum_{i=1}^\\infty\\angles{u,e_i}e_i,\\sum_{j=1}^\\infty\\angles{v,e_j}e_j}\\\\&=\\sum_{i,j=1}^\\infty\\angles{u,e_i}\\overline{\\angles{v,e_j}}\\angles{e_i,e_j}=\\sum_{i=1}^\\infty\\angles{u,e_i}\\overline{\\angles{v,e_i}}}$$ Example: The function $f(x)=x$ Recall that we have already seen that $\\angles{x, \\sin(nx)}=2\\frac{(-1)^{n+1}}{n}$ . On the other hand, $$\\angles{x, \\cos(nx)} = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} x\\cos(x)\\dx 0$$ for all $n$ , since $x\\cos(nx)$ is an odd function. Thus we conclude that $x \\sim 2\\sum_1^\\infty \\frac{(-1)^{n+1}}{n} \\sin(nx)$ . Computing the Parseval identity, we obtain that $$\\frac{2\\pi^2}{3}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} x^2\\dx = \\sum_1^\\infty \\frac{4}{n^2}.$$ So while we already knew from calculus that $\\sum_1^\\infty \\frac{1}{n^2}$ converge, now we can also find out its limit $\\frac{\\pi^2}{6}$ . Lemma: The Riemann Lebesgue Lemma For an orthonormal basis $\\{e_k \\mid k\\in \\NN \\}$ and any vector $v$ we have that $\\angles{v,e_k}\\to 0$ . -Add intuition- Proof: We know that $\\norm v = \\sum_1^\\infty |\\angles{v,e_k}|^2$ so we must have that $\\limfi{n} |\\angles{v,e_k}| = 0$ ."
  },"/Fourier_Notes/notes/Fourier/Fourier%20Series": {
    "title": "Fourier Series",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Fourier%20Series",
    "body": "The Fourier series After reminding ourselves the notations and results from inner product spaces, and combining it with the norm convergence in our new infinite dimensional vector spaces, we are ready to study the Fourier transforms on periodic function. In the following sections we would like to study the inner product space $E[-\\pi,\\pi]$ of piecewise continuous function on $[-\\pi,\\pi]$ (or equivalently $2\\pi$ periodic functions on $\\RR$ ) with the inner product $$\\angles{f,g}=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x)\\overline{g(x)}\\dx.$$ Our orthonormal basis will consist of $$\\{\\frac{1}{\\sqrt{2}}, \\sin(x), \\cos(x), \\sin(2x), \\cos(2x),...\\}.$$ Remark: Equality between functions Recall that in our space $E[-\\pi,\\pi]$ we identify between two function $f,g$ if $f-g$ is zero almost everywhere (namely $f\\equiv g$ for all but finitely many points). In particular, if both $f$ and $g$ have the same Fourier series, then $f\\sim g$ are the same in $E[-\\pi, \\pi]$ (both equivalent to the same Fourier series) and therefore are equal almost everywhere. Definition: Fourier coefficients The Fourier coefficients of $f\\in E[-\\pi,\\pi]$ are $\\angles{f,\\frac{1}{\\sqrt{2}}}$ , $\\angles{f,\\sin(nx)}$ , and $\\angles{f,\\cos(nx)}$ for all $n\\geq 1$ . In particular we have the classic Fourier series expansion: $$f \\sim \\sum \\angles{f,\\frac{1}{\\sqrt{2}}}\\frac{1}{\\sqrt{2}} + \\sum_{n=1}^\\infty \\left( \\angles{f,\\cos(nx)}\\cos(nx) + \\angles{f,\\sin(nx)}\\sin(nx)\\right).$$ When there is no ambiguity, we will use the notation: $$\\align{a_0&:=\\angles{f,\\cos(0x)}=\\sqrt{2}\\angles{f,\\frac{1}{\\sqrt{2}}} \\\\a_n&:=\\angles{f,\\cos(nx)} \\; , \\text{for } n\\geq 1\\\\b_n&:=\\angles{f,\\sin(nx)} \\; , \\text{for } n\\geq 1,}$$ for which the Fourier series looks like: $$f \\sim \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(nx) + b_n\\sin(nx)\\right).$$ Before you look at the examples below (which are $f(x)=x,\\;|x|,\\;\\chi_{[-\\pi,0]}-\\chi_{[0,\\pi]}$ ), try to compute some Fourier series for simple and nice functions (and in particular for these examples). To visualize these result, you can use the following Desmos program: Enter the $a_0$ in the simple $a_0$ cell. The syntax for the $a_n$ (and similarly $b_n$ ) should be a = a_n for n=[1,...,k] To plot steps functions like $f(x)=\\chi_{[-\\pi,0]}-\\chi_{[0,\\pi]}$ use the notation f={-pi&lt;x&lt;0:-1 , 0&lt;=x&lt;pi:1} or more generally {condition1: value1, condition2: value2, ...} Once you finish putting in the coefficients and choosing your function $f(x)$ , you can close the menu and choose the approximation level using the $k=?$ slider below the graph. You can also go directly to the Desmos site here. Examples: Fourier series $f(x)=x$ : In this case, we already saw that $b_n=2\\frac{(-1)^{n+1}}{n}$ and $a_n=0$ for all $n$ (since $x\\cos(nx)$ is an odd function). $f(x)=\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ : This is also an odd function, so that $a_n=0$ for all $n$ . As for the $b_n$ , the product $f(x)\\sin(nx)$ is even, so we have that $$\\align{b_n & = \\angles{\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]},\\sin(nx)} = \\frac{1}{\\pi} \\int_0^\\pi \\sin(nx) \\dx - \\frac{1}{\\pi} \\int_{-\\pi}^0 \\sin(nx) \\dx \\\\ & = \\frac{2}{\\pi} \\int_0^\\pi \\sin(nx) \\dx = \\frac{2}{n\\pi} \\cos(nx)\\mid_0^\\pi=\\frac{2}{n\\pi}\\left((-1)^n-1\\right).}$$ It follows that $b_{2n}=0$ while $b_{2n+1}=\\frac{4}{(2n+1)\\pi}$ . All in all, we get that $$\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}\\sim\\sum_0^\\infty \\frac{4}{(2n+1)\\pi} \\sin((2n+1)x).$$ $f(x)=|x|$ : Now we have an even function, so that $b_n=0$ for all $n$ . Similarly, since $f(x)\\cos(nx)$ is even we get: $$\\align{a_n&=\\angles{|x|,\\cos(nx)}=\\frac{2}{\\pi} \\int_0^\\pi x\\cos(nx)\\dx \\\\ &= \\frac{2}{n\\pi} \\left( x\\sin(nx)\\mid_0^\\pi+ \\int_0^\\pi \\sin(nx)\\dx\\right)=\\frac{2}{n^2\\pi}\\left((-1)^n-1\\right)}.$$ It follows that $a_{2n}=0$ while $a_{2n+1}=\\frac{-4}{(2n+1)^2\\pi}$ . However, if $n=0$ then the computation above is not well defined as we divided by zero! So for this case we need a separate computation, we produces $$\\angles{|x|,1}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}|x|\\dx = \\pi.$$ Finally, we get that $$|x|\\sim \\frac{\\pi}{2} - \\sum_1^\\infty \\frac{4}{(2n+1)^2\\pi} \\cos((2n+1)x).$$ $f(x)=\\sin^2(x)$ : While we can compute all the coefficients using the integrals, here we can use another method. Recall that $\\sin(x)=Im(e^{ix})=\\frac{e^{ix}-e^{-ix}}{2i}$ . Using this identity, we see that $$\\align{\\sin^2(x) &= \\left(\\frac{e^{ix}-e^{-ix}}{2i}\\right)^2 = \\frac{e^{2ix}-2+e^{-2ix}}{-4} \\\\ & = \\frac{\\cos(2x)-1}{-2}= \\frac{1}{2}-\\frac{\\cos(2x)}{2}.}$$ This is already Fourier series expression of $f(x)$ , and since there is always a unique such combination, this is the Fourier series of $f(x)$ . Pointwise convergence In our Fourier series $f \\sim \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(nx) + b_n\\sin(nx)\\right)$ , the limit on the right is the norm convergence, and we have already seen that it doesn’t imply pointwise convergence. However, as can be seen in the Fourier series in the step function $\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ above, in most of the points we do have pointwise convergence. Moreover, the only points where it doesn’t seem to have pointwise convergence are the two edge points at $\\pm \\pi$ and in $0$ . These are exactly the “problematic” points where the function is not continuous. Leaving the edges aside for now, where does the Fourier series converges at the point $x=0$ ? This is actually quite simple to find, since $$\\sum_0^\\infty \\frac{4}{(2n+1)\\pi} \\overbrace{\\sin((2n+1)0)}^{=0}=0.$$ More over, the intuition is that the approximation seems to converge to $-1$ just to the left of $x=0$ and to $+1$ just to the right of $x=0$ , and more or less go straight from $-1$ to $+1$ in the middle. In other words, the value at $x=0$ is exactly the average of the two values to its left and to its right! We actually see something similar at the edge points $\\pm \\pi$ . If we think of $f$ as a periodic function, then as it passes through the point $x=\\pi$ it jumps from $+1$ to $-1$ , which again has average $0$ which is where our blue approximation function is at. These observation are true in a more generalized setting where the functions are “smooth enough”. However, since we don’t even assume that our functions are continuous, we need a slightly different definition than just saying differentiable functions. Definition: Half derivative Let $f\\in E[-\\pi,\\pi]$ and denote the right limit at a point by $f(a^+)=\\displaystyle{\\lim_{x\\to a^+}}f(x)$ We define the right half derivative as the limit, if it exists, of $$\\displaystyle{\\lim_{x\\to a^+}}\\frac{f(x)-f(a^+)}{x-a}.$$ Similarly we define the left half derivative. For simplicity of notation, we identify between the edge points $\\pm \\pi$ and consider its left and right half derivatives as the left half derivative at $\\pi$ and right half derivative at $-\\pi$ . We denote by $E'[-\\pi,\\pi]\\subseteq E[-\\pi,\\pi]$ all the piecewise continuous function on $[-\\pi,\\pi]$ that have left and right half derivative at every point in $[-\\pi,\\pi]$ We this new definition of “differentiable” function, we have the following: Theorem: Dirichlet's theorem for pointwise convergence Let $f\\in E'[-\\pi,\\pi]$ . Then at a given point $\\lambda \\in(-\\pi,\\pi)$ the Fourier series converges pointwise to the average of the left and right limits, namely $$\\frac{f(\\lambda^+)+f(\\lambda^-)}{2} = \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(n\\lambda) + b_n\\sin(n\\lambda)\\right).$$ At the edge points $\\lambda =\\pm \\pi$ , it converges to $\\frac{f(-\\pi^+)+f(\\pi^-)}{2}$ . In particular, the series converges pointwise at each point where $f$ is continuous. Remark: Gluing the edges. As the last theorem suggest, we should actually think about the edge points $\\pm \\pi$ as a single point. Example: We already saw that this theorem holds for the discontinuities of $f=\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ , namely the pointwise convergence there are to the average of the left and right limits. Let’s consider the pointwise convergence at $x=\\frac{\\pi}{2}$ . Since $f(x)$ is continuous there and $f(\\frac{\\pi}{2})=1$ , this is going to be the limit. Writing the pointwise convergence of the Fourier series, we get: $$1=\\sum_0^\\infty \\frac{4}{(2n+1)\\pi} \\sin((2n+1)\\frac{\\pi}{2})=\\frac{4}{\\pi}\\sum_0^\\infty \\frac{(-1)^n}{2n+1} .$$ Try to prove this using other methods (clue: $\\arctan(1)=\\frac{\\pi}{4}$ ). As another example, recall that the approximation for $f(x)=x$ looks like: True[[invalid wikilink: Pasted image 20240131120746.png]] Here the function is “continuous” except for the edges, where it jumps from $f(\\pi^-)=\\pi$ to $f(-\\pi^+)=-\\pi$ . The average is of course $\\frac{f(-\\pi^+)+f(\\pi^-)}{2}=0$ , and the pointwise convergence at these points are $$2\\sum_{k=1}^{\\infty}\\left(-1\\right)^{k+1}\\overbrace{\\frac{\\sin\\left(\\pm k\\pi\\right)}{k}}^{=0}=0.$$ Excercise: Compute the Fourier series for $f(x)=\\chi_{[0,\\pi]}$ . Where are its discontinuity points, and what should be the pointwise convergence there for the Fourier series? Uniform convergence We have now seen that for a “smooth” enough function $f(x)$ , our Fourier series not only converges in norm but also pointwise (more or less). Can we strengthen this to a uniform convergence? In general, the answer is no. Our Fourier approximations are continuous (combination of sine and cosine functions), and the uniform limit of continuous functions is again continuous. So unless $f(x)$ by itself is continuous, this is false. However, if $f(x)$ is continuous and “smooth” enough, then we do actually have uniform convergence. Before stating the theorem and proving it, recall one of the main tools to prove uniform convergence: Theorem: Weierstrass theorem for uniform convergence Let $f_n:I\\to \\CC$ be a sequence of functions. If $\\sum_1^\\infty M_n for some sequence of numbers satisfying $\\sup_x |f_n(x)|\\leq M_n$ , then $\\sum_1^\\infty f_n(x)$ converges uniformly and in absolute value. In our case, we are dealing with sums of the form $$\\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(nx) + b_n\\sin(nx)\\right).$$ Since the sine and cosine functions are bounded by $1$ , using Weierstrass’ theorem, if $$\\sum_1^\\infty \\left(|a_n|+|b_n|\\right) then the Fourier series will converge uniformly. We already know that this sum is “not too large”, since Bessel inequality implies that $$\\sum_1^\\infty \\left(|a_n|^2+|b_n|^2\\right)\\leq \\norm{f}^2,$$ however, this is not enough in general to show that the sum without the square is finite (for example, if $a_n=b_n=\\frac{1}{n}$ ). It turns our that the “smoother” $f$ is, the faster both $a_n$ and $b_n$ converge to zero, and the better the chance for the series above to converge. In particular, we have the following result for “smooth” functions. Theorem: Term by term derivatives Suppose that $f\\in C[-\\pi, \\pi]$ is continuous and $f(\\pi)=f(-\\pi)$ and that $f'(x)\\in E[-\\pi,\\pi]$ (and in particular $f'(x)$ exists except for finitely many points). Then writing the Fourier series of $f$ as: $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)),$$ we have that the Fourier series of $f'$ is the element wise derivative: $$f'(x)\\sim \\sum_1^\\infty (-na_n \\sin(nx)+ nb_n\\cos(nx)).$$ Note that going the other way around, given the $n$ -th Fourier coefficients of $f'$ , we can obtain the $n$ -th Fourier coefficients of $f$ by dividing by $n$ , so they converge to zero very fast. In particular we have the following: Corollary: Uniform convergence Under the conditions of the Theorem above, the Fourier series of $f$ converges uniformly to $f$ . Proof: Denote by $a_n,b_n$ , and $\\tilde{a}_n,\\tilde{b}_n$ the Fourier coefficients of $f(x)$ and $f'(x)$ respectively. By the theorem we know that $|a_n|=\\frac{|\\tilde{n}_n|}{n}, |b_n|=\\frac{|\\tilde{a}_n|}{n}$ for $n\\geq 1$ . Using this fact and the Cauchy Shwarz inequality, we get $$\\sum_1^N (|a_n|+|b_n|)=\\sum_1^N \\frac{1}{n}(|\\tilde{a}_n|+|\\tilde{b}_n|)\\leq \\sqrt{\\sum_1^N \\frac{2}{n^2}}\\sqrt{\\sum_1^N\\left( |\\tilde{a}_n|^2+|\\tilde{b}_n|^2\\right)}.$$ If we can show that the last expression is bounded (and therefore converge) we could use Weierstrass theorem to prove uniform convergence. We already know that $\\sum_1^\\infty \\frac{1}{n^2}$ is finite (and even know how to compute it), and for the other term we can use Bessel’s inequality to obtain: $$\\sum_1^N \\left(|\\tilde{a}_n^2| + |\\tilde{b}_n^2|\\right) \\leq \\norm{f'},$$ which completes the proof. Example: $f(x)=|x|$ This function is periodic continuous, and its derivative is $f'=\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ which is in $E[-\\pi,\\pi]$ so we can apply the last lemma. We already computed its Fourier series: $$|x|\\sim \\frac{\\pi}{2} - \\sum_1^\\infty \\frac{4}{(2n+1)^2\\pi} \\cos((2n+1)x).$$ Since $$\\sum_1^\\infty\\sup_x|\\frac{4}{(2n+1)^2\\pi} \\cos((2n+1)x)|\\leq \\sum_1^\\infty \\frac{4}{(2n+1)^2\\pi} we can use Weierstrass’ theorem to conclude that its convergence is indeed uniform. Furthermore, taking the term by term derivatives, we obtain another Fourier series that we already saw, namely: $$\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]} \\sim \\sum_1^\\infty \\frac{4}{(2n+1)\\pi} \\sin((2n+1)x).$$ Note that the Fourier coefficients of $\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ (which is not continuous) behave like $\\frac{1}{n}$ while the coefficients of the continuous $|x|$ behave like $\\frac{1}{n^2}$ . Proof of term by term derivatives: The main idea of the proof is to move between the Fourier coefficients of $f$ and $f'$ via integration by parts. More specifically, write the Fourier series of both $f$ and $f'$ : $$\\align{f(x)& \\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx))\\\\ f'(x) &\\sim \\frac{\\tilde{a}_0}{2} + \\sum_1^\\infty (\\tilde{a}_n \\cos(nx)+ \\tilde{b}_n\\sin(nx))}$$ Using integration by parts we have that $$\\align{\\tilde{a}_n &= \\angles{f',\\cos(nx)}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f'(x)\\cos(x)\\dx\\\\ &=\\frac{1}{\\pi}\\left(f(x)\\cos(nx)\\mid_{-\\pi}^{\\pi} - \\int_{-\\pi}^{\\pi}nf(x)\\sin(nx)\\dx \\right)\\\\&=\\overbrace{(f(\\pi)-f(-\\pi))}^{=0}(-1)^n+n\\angles{f,\\sin(nx)} = nb_n.}$$ A similar computation shows that $\\tilde{b}_n = -na_n$ . This shows that we can take the derivative element wise. Excercise: Suppose that we are given $f$ as in the theorem, however, we don’t know if $f(\\pi)=f(-\\pi)$ . Find the Fourier coefficients of $f'(x)$ . Try to use the last theorem, instead of reproving it. We saw that we can take the derivative term by term. Similarly, we can also take the integral element wise in the Fourier series. Remark: Integrals vs Derivative We usually study derivatives before integrals, and we sometimes get the feeling that they are easier. While this is usually true when trying to actually compute these functions, when speaking about the results, the integral, namely the antiderivative of a function is usually much “better” than the derivative of a function. Indeed, if we measure a function by how smooth it is, namely how many derivatives it has, then we always get that the anti derivative has at least (!) one derivative. Theorem: Term by term integrals Suppose that $f\\in E[-\\pi, \\pi]$ and denote its Fourier series by $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ Then for any $[c,d]\\subseteq [-\\pi,\\pi]$ we have the term by term integral: $$\\int_c^d f(t)\\dt = \\frac{a_0(d-c)}{2} + \\sum_1^\\infty (\\frac{a_n}{n} (\\sin(nd)-\\sin(nc)) - \\frac{b_n}{n}(\\cos(nd)-\\cos(nc)).$$ Proof: The main step here is to note that we can write $$\\align{\\int_c^d f(t) \\dt &= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(t)\\pi\\chi_{[c,d]}\\dt = \\angles{f,\\pi\\chi_{[c,d]}} \\\\ & =\\angles{\\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)),\\pi\\chi_{[c,d]}}.}$$ Using the fact that the inner product is continuous (with respect to the $\\norm{\\cdot}_2$ norm), we can take the summation outside the integral to obtain: $$\\align{\\int_c^d f(t) \\dt &=\\angles{\\frac{a_0}{2}, \\pi\\chi_{[c,d]}} + \\sum_1^\\infty\\angles{a_n \\cos(nx)+ b_n\\sin(nx),\\pi\\chi_{[c,d]}} \\\\ & = \\int_c^d\\frac{a_0}{2} \\dt + \\sum_1^\\infty\\int_c^d(a_n \\cos(nx)+ b_n\\sin(nx))\\dt \\\\ & = \\frac{a_0(d-c)}{2} \\dt + \\sum_1^\\infty(\\frac{a_n}{n} (\\sin(nd)-\\sin(bc))- \\frac{b_n}{n}(\\cos(nd)-\\cos(nc))\\dt.}$$ Is the indefinite integral differentiable? Let $f(x)=|x|$ which is continuous, and define $$F(x)=\\int_{-\\pi}^{x} f(t)\\dt-\\frac{\\pi^2}{2},$$ so that $F'(x)=f(x)$ . What can you say about the convergence of the Fourier series of $F(x)$ ? What is it $\\cl_2$ , $\\cl_\\infty$ and pointwise limit? Check to see if your guess is correct. Proof: It is not hard to show that $$F(x)=\\cases{-\\frac{x^2}{2}&x\\leq 0\\\\ \\frac{x^2}{2} & x\\geq 0}.$$ Write it Fourier series as $$F(x) \\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ Since $F(x)$ is an odd function, we immediately get that $a_n=0$ for all $n$ . As for $b_n$ , using symmetry and integration by parts we get $$\\align{b_n&=\\angles{F(x),\\sin(nx)}=\\frac{2}{\\pi}\\int_0^\\pi \\frac{x^2}{2} \\sin(nx) \\dx \\\\ & = -\\frac{2}{\\pi n} \\left[ \\frac{x^2\\cos(nx)}{2}\\mid_0^\\pi -\\int_0^\\pi x\\cos(nx)\\dx \\right] = -\\frac{2}{\\pi n} \\left[ \\frac{\\pi^2(-1)^n}{2} - \\int_0^\\pi x\\cos(nx)\\dx \\right].}$$ A second integration by parts gives: $$\\int_0^\\pi x\\cos(nx)\\dx = x\\frac{\\sin(nx)}{n}\\mid_0^\\pi - \\int_0^\\pi \\frac{\\sin(nx)}{n}\\dx=\\frac{\\cos(nx)}{n^2}\\mid_0^\\pi=\\frac{(-1)^n-1}{n^2}.$$ All together we have that $b_n=-\\frac{2}{\\pi n} \\left[ \\frac{\\pi^2(-1)^n}{2} + \\frac{(-1)^n-1}{n^2} \\right]$ . Considering the converge of the Fourier series, since $F\\in E[-\\pi, \\pi]$ we automatically get the $\\cl_2$ -norm convergence. Also, the function is continuous “everywhere” (except the edge points) and has half derivatives, so Dirichlet’s theorem applies and we have pointwise convergence in $(-\\pi,\\pi)$ . On the edges, the pointwise convergence is to the average: $$\\frac{\\displaystyle{\\lim_{x\\to-\\pi+}F(x)+\\lim_{x\\to\\pi-}F(x)}}{2}=\\frac{\\pi^2/2-\\pi^2/2}{2}=0.$$ The function $F(x)$ is not continuous at the edges, since $F(-\\pi)\\neq F(\\pi)$ so we can’t use the theorem about uniform convergence. In general the fact that the conditions of the theorem don’t hold doesn’t mean that its result doesn’t hold. However, in this case it is true, since if there is uniform convergence in $[-\\pi,\\pi]$ , then considering the functions as $2\\pi$ periodic there will be uniform convergence everywhere, so the limit (of continuous function) is going to be continuous. As this is not true for $F$ , the convergence is not uniform. The last exercise shows that taking the integral of a continuous function doesn’t necessarily gives us a smooth function, since there can be issues with the edge points. However, we can fix these problems in a way. Excercise: Let $f_0(x)=sign(x)$ which is not continuous. Show that there are $f_1(x),f_2(x)$ such that $f_2'(x)=f_1(x)$ and $f_1'(x)=f_0(x)$ in almost every $x$ and both $f_1,f_2$ are continuous (including edge points). Proof: The main idea is to start with $|x|$ as before, and to change it by a scalar to $f_1(x)=|x|+c$ . This keeps $f_1(x)$ as a continuous function with $f_1'(x)=f_0(x)$ . Let $F$ be as in the previous exercise, and define $$F_C(x)=\\int_{-\\pi}^{x} (f_1(t)+C)\\dt-\\frac{\\pi^2}{2} = F(x)+Cx.$$ The function $F_C(x)$ is still continuous in $(-\\pi,\\pi)$ and $F_C'(x)=f_1(x)$ . Also, it limit at the edge points are $$\\lim_{x\\to-\\pi^+}F_C(x)=-\\frac{\\pi^2}{2}-C\\pi\\;\\;;\\;\\;\\lim_{x\\to\\pi^-}F_C(x)=\\frac{\\pi^2}{2}+C\\pi.$$ Choosing $C=-\\frac{\\pi}{2}$ will cause them to be equal and therefore make $F_C(x)$ continuous. Example: Fourier in differential \\\\ integral equations Let’s try to find continuous $f$ solving the equation $$ \\int_0^x f(t)\\dt = x - f'(x) .$$ If the equation above holds, then in particular $f'(x)$ is by itself defined and continuous. Let’s write the Fourier expansion of $f$ as $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ We would like to take the term by term derivative, but not all the conditions of the theorem hold - we don’t know that $f(-\\pi)=f(\\pi)$ . However, we can assume that it is true and continue on, but we need to check that our final solution does satisfy this condition (or alternatively, it satisfies the equation we started with). Under this assumption, we can write: $$f'(x)\\sim \\sum_1^\\infty (-n a_n \\sin(nx)+ n b_n\\cos(nx)).$$ For any function in $E[-\\pi,\\pi]$ we can do term by term integration, so putting everything together we obtain the equation: $$\\frac{a_0x}{2} + \\sum_1^\\infty (\\frac{a_n}{n} \\sin(nx)- \\frac{b_n}{n}(\\cos(nx)-1)) = x - \\sum_1^\\infty (-n a_n \\sin(nx)+ n b_n\\cos(nx)).$$ Moving everything to the same side, we get $$\\left(\\frac{a_0}{2}-1\\right)x + \\sum_1^\\infty \\frac{b_n}{n} + \\sum_2^\\infty \\left( a_n(\\frac{1}{n}-n) \\sin(nx) - b_n(\\frac{1}{n}-n\\right)\\cos(nx))=0.$$ If all the “coefficients” are zero, then of course it solves the equation. However, note that the last equation is not a Fourier expansion (namely, it is not a linear combination of a basis), so we don’t know automatically that this is the only solution. In any case, one way to make all the coefficient zero is by taking: $$a_0=2,\\; a_n =0\\; \\forall n\\geq 2,\\; b_n =0 \\; \\forall n\\geq 1.$$ We are now left with $f(x)=1+a_1\\cdot\\cos(x)$ , and note that the functions in this family do satisfy $f(-\\pi)=f(\\pi)$ . Finally, just to be on the safe side, let’s see that they do satisfy our original equation: $$\\align{\\int_0^x f(t)\\dx & = \\int_0^x (1+a_1\\cos(t))\\dx = x + a_1 \\sin(t)\\mid_0^x = x+a_1\\sin(x)\\\\ x-f'(x) & = x-(1+a_1\\cos(x))' = x + a_1\\sin(x)}.$$ Convergence summarization Let’s summarize our convergence results in the space $E[-\\pi,\\pi]$ : Norm and pointwise convergence implications for sequence $f_n$ of functions: $$\\align{\\norm{\\cdot}_\\infty & \\Rightarrow \\norm{\\cdot}_2 \\Rightarrow \\norm{\\cdot}_1 \\\\ \\norm{\\cdot}_\\infty&\\Rightarrow \\text{pointwise}.}$$ Neither the pointwise of $\\norm\\cdot_2$ convergence imply one another. For any $f\\in E[-\\pi,\\pi]$ there is $\\norm\\cdot_2$ convergence of the Fourier series: $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ The coefficients are determined uniquely by $f$ , and if two functions $f,g$ have the same Fourier coefficients, then they are the same in $E[-\\pi,\\pi]$ , namely $f(x)=g(x)$ except for finitely many points. If $f$ has half derivatives at every point, then the Fourier series converges point wise to the averages: $$\\frac{f(x^+)+f(x^-)}{2}.$$ In particular, if $f$ is continuous at $x$ , then the Fourier series converges pointwise to $f(x)$ . For any $f\\in E[-\\pi,\\pi]$ we can do term by term integration of the Fourier series. If $f$ is continuous (including edge points) and $f'\\in E[-\\pi,\\pi]$ , then we can also take derivatives term by term."
  },"/Fourier_Notes/notes/Fourier/Fourier%20Transform": {
    "title": "Fourier Transform",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Fourier%20Transform",
    "body": "Up until now we talked about functions on $[-\\pi,\\pi]$ , or equivalently $2\\pi$ periodic functions. We now shift our focus to general function on $\\RR$ . Inner products The first issue we encounter with these general functions, is that the corresponding inner products is on an infinite segment : $$\\angles{f,g}:=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}f(x)\\overline{g(x)}\\dx.$$ This integral can diverge even for very nice functions, and in particular $\\angles{1,1}=\\infty$ . To solve this problem we consider a subfamily of function, which is large enough to contain most of the functions that we work on. Definition: Norms on functions on $\\RR$ For a function $f:\\RR \\to \\CC$ we define: $$\\begin{align} \\norm f_1 & = \\int_{-\\infty}^{\\infty} \\abs{f(x)} \\dx \\\\ \\norm f_2 & = \\frac{1}{2\\pi} \\left(\\int_{-\\infty}^{\\infty} \\abs{f(x)}^2 dx\\right)^{1/2} \\\\ \\norm f_\\infty & = \\sup_x \\abs{f(x)}. \\end{align}$$ For $p=1,2,\\infty$ we denote by $$E^p(\\RR) = \\{f:\\RR \\to \\CC \\;\\mid\\; f \\text{ piecewise continuous}, \\norm f_p In the $[-\\pi,\\pi]$ finite case, we have that $\\norm f_1 \\leq \\norm f_2 \\leq \\norm f_\\infty$ (maybe up to a scalar normalization). This is no longer true in the infinite case, for example $f\\equiv 1$ has $\\norm f_2 = \\norm f_1 =\\infty$ while $\\norm f_\\infty =1$ . Actually, you can show that: Excercise: For each $p\\in\\{1,2,\\infty\\}$ find a function $f$ such that $\\norm f_p , while the other two norms are infinity. We already saw that the inner product is not well defined for just bounded functions in $E^\\infty (\\RR)$ , but it is well defined for function in $E^2(\\RR)$ . Lemma: Inner product on $E^2(\\RR)$ The map $\\angles{f,g}$ defined above is an inner product on $E^2(\\RR)$ . Proof: Once we know that $\\angles{f,g}$ is well defined, the rest of properties of inner products are easy to prove. We already saw in Inner product spaces - a reminder that the integral converges, but as a reminder, because $\\abs{f(x)\\overline{g(x)}}\\leq \\abs{f(x)}^2+\\abs{g(x)}^2$ , we get that the integral even converges in absolute value since $$\\int_{-\\infty}^\\infty\\abs{f(x)g(x)}\\dx \\leq \\norm{f(x)}_2^2+\\norm{g(x)}_2^2 We can now look for an orthonormal basis and do a similar process as with the Fourier series. However, our main goal is to study functions through their periodicity (namely translation to the left and right), and the only periodic function which is in $E^2(\\RR)$ is the zero function. Instead, we will just “extend” what we saw in the finite segment case to all of $\\RR$ . The Fourier Transform in $E^1(\\RR)$ Definition: The Fourier transform For a function $f:\\RR \\to \\CC$ and $\\omega \\in \\RR$ we write: $$\\hat f(\\omega) := \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx = \\angles{f,e^{i\\omega x}}.$$ If this limit exists for every $\\omega \\in \\RR$ , we call $\\hat{f}:\\RR \\to \\CC$ the Fourier transform of $f$ . We sometimes denote this transform by $\\hat{f} = \\mathcal{F}[f]$ . We will soon see that this transform is well defined for our functions in $E^1(\\RR)$ , but first, let’s see some examples. Example: The function $\\chi_{[a,b]}$ . For $a define the characteristic function: $$f(x)=\\chi_{[a,b]}(x)=\\cases{1&x\\in[a,b]\\\\0&else}.$$ Computing its Fourier transform, we have: $$2\\pi \\hat{f}(w)= \\int_{-\\infty}^{\\infty} \\chi_{[a,b]}(x)e^{-i\\omega x}dx = \\int_{a}^{b} e^{-i\\omega x}dx =\\frac{1}{\\omega} ie^{-i\\omega x}\\mid_a^b = \\frac{e^{-i\\omega a}-e^{-i\\omega b}}{\\omega i}. $$ Note that the computation above is invalid when $\\omega=0$ for which we have: $$\\hat{f}(0)=\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\chi_{[a,b]}(x)dx=\\frac {b-a}{2\\pi}. $$ The function $\\hat f(\\omega)$ is easily seen to be continuous for $\\omega \\neq 0$ , but you can also check that it is continuous at $\\omega = 0$ as well. Also, since $e^{i\\omega x}$ is bounded, as $\\omega \\to \\infty$ we see that $\\hat{f}(\\omega)\\to 0$ . We will see later on that this is a much more general phenomena. More over, when $a=-b$ is symmetric, the expression above is the real function: $$\\hat{f}(\\omega)=\\cases{\\frac{\\sin(\\omega b)}{\\omega \\pi} & \\omega \\neq 0 \\\\ \\frac{b}{\\pi} & \\omega = 0.}$$ Example: The function $e^{-a|x|}$ , for $a>0$ . First we note that $f(x):=e^{-a|x|}\\in E^1(\\RR)$ . Next, we have that $$\\begin{align} 2\\pi \\cdot \\hat f(\\omega) & = \\int_{-\\infty}^{\\infty} e^{-a|x|} e^{-i\\omega x}\\dx = \\int_{-\\infty}^{0} e^{(a-i\\omega) x}\\dx + \\int_{0}^{\\infty} e^{(-a-i\\omega) x}\\dx \\\\ & = \\int_{0}^{\\infty}\\left(e^{-(a-i\\omega)x}+e^{(-a-i\\omega)x}\\right)\\dx=\\left[\\frac{e^{-ax}e^{i\\omega x}}{i\\omega-a}-\\frac{e^{-ax}e^{-i\\omega x}}{i\\omega+a}\\right]_{0}^{\\infty}. \\end{align}$$ Since $e^{i\\omega x}$ is bounded by 1, and $e^{-ax}\\to 0$ as $x\\to\\infty$ , we conclude that $$2\\pi \\cdot \\hat f(\\omega) = \\frac{1}{i\\omega+a}-\\frac{1}{i\\omega-a} = \\frac{2a}{\\omega^2+a^2}. $$ Basic properties Before proving some basic results about the Fourier transform, we need to actually show that it is well defined for our functions, namely that $\\angles{f,e^{i\\omega x}}$ is finite when $f\\in E^1(\\RR)$ . Lemma: If $f\\in E^1(\\RR)$ , then $\\hat{f}(\\omega)$ is well defined for all $\\omega$ and $\\norm {\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi} \\norm f_1$ . Proof: This is a simple upper bound computation: $$\\abs{\\hat{f}(\\omega)}=\\abs{\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx} \\leq \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\abs{f(x)}\\abs{e^{-i\\omega x}}dx=\\frac{1}{2\\pi} \\norm f_1.$$ Remark: Generalized inner products As we mentioned before, the “inner product” $\\angles{f,e^{i\\omega x}}$ is no longer a standard an inner product, however there is some method to this madness. This is is a product between a function in $L^1(\\RR)$ and $L^\\infty(\\RR)$ , where it is usually between two functions in $L^2(\\RR)$ . These two pairs of numbers satisfy the simple “equation” $\\frac{1}{1}+\\frac{1}{\\infty} = \\frac{1}{2} + \\frac{1}{2} = 1$ , and as it turns out, this is enough for the “inner product” to be defined. For more details, you should read about Holder inequality. In the Fourier series section, we used $\\sin(nx), \\cos(nx)$ as our basis. Using the complex exponents is much easier (once we know how to work with them) and we almost get automatically the following results. Claim: Arithmetic operations Let $f\\in E^1(\\RR)$ . Then: The Fourier transform is linear, namely $\\mathcal{F}(\\alpha f + g) = \\alpha \\mathcal{F}(f)+\\mathcal{F}(g)$ . Translation-&gt;Rotation: Setting $f_\\alpha(x) := f(x+\\alpha)$ , we have $$\\hat{f}_\\alpha (\\omega) = e^{i\\alpha\\omega}\\hat{f}(\\omega).$$ Multiplication: If $\\lambda \\neq 0$ , then setting $g(x)=f(\\lambda x)$ we have $$\\hat g (\\omega) = \\frac {\\hat{f}(\\omega/\\lambda)}{\\abs{\\lambda}}.$$ Conjugation: $\\overline{\\cf[f](\\omega)}=\\cf[\\bar{f}](-\\omega)$ . Proof: Follows from the fact that integrals are linear. (and 3) Both claims follow from change of parameters: $$\\hat{f_\\alpha}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x+\\alpha)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega (x-\\alpha)}dx = e^{i\\omega\\alpha}\\hat f(\\omega).$$ For $\\lambda>0$ we have: $$\\hat{g}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(\\lambda x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega/\\lambda) x}\\frac{1}{\\lambda}dx = \\frac{1}{\\lambda}\\hat f(\\frac{\\omega}{\\lambda}).$$ For $\\lambda , the proof is similar. $$2\\pi \\overline{\\hat{f}(\\omega)} = \\overline{\\int_{-\\infty}^{\\infty} f(x) e^{-i\\omega x}\\dx} = \\int_{-\\infty}^{\\infty} \\overline{f(x)} e^{i\\omega x}\\dx = 2\\pi \\hat{\\overline{f}}(-\\omega)$$ If $f\\in L^1(\\RR)$ and we multiply it by some bounded function $g\\in L^\\infty(\\RR)$ then it is easy to check that $f(x)\\cdot g(x)\\in L^1(\\RR)$ , and actually $\\norm {f\\cdot g}_1 \\leq \\norm {f}_1 \\norm{g}_\\infty$ . In this case, we can ask what is the Fourier transform of $f\\cdot g$ . Probably the most important bounded function we work with is $e^{ix}$ which not only appear in the computation of the Fourier transform, but we also saw it in the claim above where a translation became a rotation. As it turns out the other direction works as well. Claim: Rotation $\\Rightarrow$ translation. Let $f\\in E^1(\\RR)$ and for $c\\in \\RR$ let $h_c(x):=e^{icx}f(x)$ . Then $$\\hat{h_c}(\\omega)=\\hat{f}(\\omega-c).$$ In the sine and cosine notations, we have: $$\\align{ \\cf[f(x)\\sin(x)]&=\\frac{\\cf[f](\\omega-c)-\\cf[f](\\omega+c)}{2i} \\\\ \\cf[f(x)\\cos(x)]&=\\frac{\\cf[f](\\omega-c)+\\cf[f](\\omega+c)}{2} }$$ Proof: This is again a simple computation: $$\\hat{h_c}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{icx}f(x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega-c)x}dx = \\hat f(\\omega-c).$$ There is already an interesting phenomenon suggested by these results. When we translated $f$ it transformed into a rotation of $\\hat{f}$ and when we rotated $f$ it transformed into a translation in $\\hat{f}$ . Something similar happened with the multiplication (and conjugation), but more “stable” - multiplication transformed into multiplication (same with conjugation). This duality is very important and we will see that more as we progress. Lets see a couple more examples. Corollary: If $f$ is even (resp. odd) then $\\cf[f]$ is even (resp. odd). If $f$ is real (resp. pure imaginary), then $\\overline{\\cf[f](\\omega)}=\\cf[f](-\\omega)$ (resp. $=-\\cf[f](-\\omega)$ ). In particular, if $f$ is real and even (e.g. cosine functions), then $\\hat{f}$ is real and even, and if $f$ is real and odd, then $\\hat{f}$ is pure imaginary and odd. Proof: Take the multiplication identity with $\\lambda = -1$ , so that $g(x)=f(-x)$ . In this case we get that $\\hat{g}(\\omega)=\\hat{f}(-\\omega)$ . If $f$ is symmetric, then $g=f$ , so that $\\hat{f}(\\omega)=\\hat{f}(-\\omega)$ is symmetric, and if $f$ is odd, then $g(x)=-f(x)$ in which case $$\\hat{f}(-\\omega)=\\widehat{g}(\\omega)=\\widehat{-f}(\\omega)=-\\widehat{f}(\\omega)$$ is odd. The second part is proved similarly from the conjugation identity. Continuity and derivatives Our next step is to show some properties on the function $\\hat {f}$ itself, and we start by showing that it is continuous. Claim: The Fourier transform is continuous If $f\\in E^1(\\RR)$ , then: The function $\\hat{f}$ is continuous. (The Riemann-Lebesgue lemma): $\\limfi{\\abs{\\omega}} \\hat{f}(\\omega) = 0$ . Proof: For the first claim, let’s consider the following example with $f(x)=e^{-\\abs{x}/10}$ We would like to show that the integrals over $f(x)e^{-i\\omega x}$ and $f(x) e^{-i\\omega_0x}$ are close if $\\omega, \\omega_0$ are close. However, since it is hard to draw complex function, we will look at their real analogues. In the picture above, the function itself is drawn in green, and we also draw $f(x)\\cdot \\sin(10x)$ and $f(x)\\cdot \\sin(10.2 x)$ in blue and red. The transform should be thought of as the integrals over these function. In this example we want to show that since $10$ and $10.2$ are close, their integrals over $f(x)\\cdot \\sin(10x)$ and $f(x)\\cdot \\sin(10.2 x)$ are also close. There are two ideas that can be seen in the image that we can use in general: If we are “near the center” the blue and red functions themselves are very close so their integrals are close and If we are “far away from the center”, while this no longer holds, the total area beneath $f(x)$ is very small, so even when multiplying by some sine functions it remains small. Thus, even if we look at the difference between two such functions, the area will be small. More formally, fix some $M>0$ which “measures” how far we are from the center, then: $$\\align{ 2\\pi\\abs{\\hat{f}(\\omega+h)-\\hat{f}(\\omega)} & \\leq \\int_{-\\infty}^\\infty\\abs{e^{-i(\\omega+h)x}-e^{-i\\omega x}}\\abs{f(x)}\\dx = \\int_{-\\infty}^\\infty\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx \\\\ & = \\int_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx + \\int_{\\abs{x}> M}\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx \\\\ & \\leq \\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\int_{\\abs{x}\\leq M}\\abs{f(x)}\\dx + 2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx \\\\ & \\leq \\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\norm f_1 + 2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx.}$$ Choose your favorite $\\varepsilon>0$ . Since $\\norm f_1 , we can choose $M$ big enough so that $$2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx Next, for $\\abs{x}\\leq M$ , for all $\\abs{h}$ small enough, using the continuity of $e^{-ihx}$ we can make sure that $$\\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\norm f_1 Together we see that as $h\\to 0$ we have that $\\hat{f}(\\omega+h)\\to \\hat{f}(\\omega)$ , or in other words $\\hat{f}$ is continuous. The Riemann Lebesgue lemma is proved in a similar fashion. When computing the integral in $f(\\omega)$ , when we are far away from the center the integral will be very small, regardless of $\\omega$ . When we are close to the center, we can approximate our function by a step function. For each such step, when $\\omega$ is very large, we should expect very high frequency fluctuations so every “positive area” will more or less cancel out with a “negative area”. More formally, for any choice of $M>0$ we get a simple upper bound: $$\\align{ 2\\pi\\abs{\\hat{f}(\\omega)} & = \\abs{\\int_{-\\infty}^\\infty e^{-i\\omega x}f(x)\\dx} \\leq \\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}f(x)\\dx} + \\int_{\\abs{x}>M} \\abs{f(x)}\\dx.}$$ As before, for any $\\varepsilon>0$ , we can choose $M$ big enough so that the second summand is as small as we want : $$\\int_{\\abs{x}>M} \\abs{f(x)}\\dx Since our function $f$ is Riemann integrable, we can approximate it by a step function $h$ , namely $h(x)=\\sum_1^n \\lambda_i \\chi_{[a_i,b_i]}$ is a finite combination of steps, and $\\int_{\\abs{x}\\leq M}\\abs{h(x)-f(x)}\\dx . We can use it to upper bound the first summand: $$\\align{ \\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}f(x)\\dx}&=\\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}\\left(f(x)-h(x)\\right)\\dx + \\int_{\\abs{x}\\leq M} e^{-i\\omega x}h(x)\\dx} \\\\ & \\leq \\int_{\\abs{x}\\leq M} \\abs{f(x)-h(x)}\\dx + \\abs{\\sum_{i=1}^n\\lambda_i \\int_{\\abs{x}\\leq M} e^{-i\\omega x}\\chi_{[a_i,b_i]}(x)\\dx} \\\\ & \\leq \\frac{\\varepsilon}{4}+\\sum_{i=1}^n\\abs{\\lambda_i}\\hat{\\chi}_{[a_i,b_i]}(\\omega). }$$ We already saw that $\\hat{\\chi}_{[a_i,b_i]}(\\omega)=\\frac{e^{-i\\omega a}-e^{-i\\omega b}}{\\omega i}\\to0$ as $\\omega \\to \\infty$ , and since there are only finitely many such summands ( $n$ now is fixed), for all $\\omega$ large enough this sum is at most $\\frac{\\varepsilon}{2}$ . Putting everything together, we get that for all $\\varepsilon>0$ and for all $\\omega$ large enough, we have that $\\abs{\\hat{f}(\\omega)} , or in other words $\\hat{f}(\\omega)\\to 0$ . Next we turn to derivatives, which also contain an interesting duality. Claim: Fourier transform and derivatives Suppose that both $f\\in E^1(\\RR)$ , and $f'\\in E^1 (\\RR)$ . Then $$\\cf[f'](\\omega)=i\\omega \\cf[f](\\omega).$$ Similarly, if $xf(x)\\in E^1 (\\RR)$ , then $\\hat{f}$ is differentiable and $$\\cf[-ixf(x)](\\omega) = \\cf[f]'(\\omega).$$ Remark: Before we give a proper proof, note that if we are allowed to change the order of derivation and integration, then the second part is almost automatic. However, we don’t know that we can do this, and we basically prove it here. Proof: For the first part, we use integration by parts $$\\align{ 2\\pi \\widehat{f'}(\\omega)&=\\int_{-\\infty}^{\\infty} f'(x)e^{-i\\omega x}\\dx=f(x)e^{-i\\omega x}\\mid_{-\\infty}^{\\infty}+i\\omega\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}\\dx. }$$ Using the fact that $f(x)=f(0)+\\int_0^x f'(t)\\dt$ and $\\norm {f'}_1 is finite, we see that $\\limfi{x} f(x)$ exists. Since we also know that $\\norm{f}_1 is finite, this limit must be zero. Similarly, we get that $\\displaystyle{\\lim_{x\\to -\\infty}}f(x)=0$ , and since $e^{-i\\omega x}$ is bounded, we conclude that $$f(x)e^{-i\\omega x}\\mid_{-\\infty}^{\\infty} = 0. $$ The other summand is simply $2\\pi i \\omega \\hat{f}(\\omega)$ which completes the first part of claim. For the second part, by definition we have that $$\\align{ 2\\pi\\left(\\cf[f]'(\\omega) -\\cf[-ixf(x)] \\right) & =\\lim_{h\\to 0}\\int_{-\\infty}^{\\infty}f(x)\\frac{e^{-i(\\omega+h)x}-e^{-i\\omega x}}{h}\\dx + i\\int_{-\\infty}^{\\infty}xf(x) e^{-i\\omega x}\\dx \\\\ & = \\lim_{h\\to 0}\\int_{-\\infty}^{\\infty}f(x)e^{-i\\omega x}\\left( \\frac{e^{-ihx}-(1-ihx)}{hx}x \\right)\\dx. }$$ As $e^z$ is analytic, we can find some constant $C$ such that $\\abs{\\frac{e^{-ihx}-(1-ihx)}{hx}} for $\\abs{hx} . If $\\abs{hx}>1$ , then $\\abs{\\frac{e^{-ihx}-(1-ihx)}{hx}} . Using the fact that $\\norm {xf(x)}_1 , the same ideas from before show that the limit is zero. $f(x)=e^{-|x|} x^n$ Note first that the function $f(x)$ is in $E^1(\\RR)$ , so it has a Fourier transform. For the $n=0$ case we know that $\\cf\\left[e^{-|x|}\\right](\\omega) = \\frac{1}{\\omega^2 + 1}$ . Using the last claim and a bit of induction, we get that : $$\\cf\\left[x^ne^{-|x|}\\right](\\omega) = (-i)^n \\cf\\left[(-ix)^ne^{-|x|}\\right](\\omega) = (-i)^n \\cf\\left[e^{-|x|}\\right]^{(n)}(\\omega) = (-i)^n \\frac{\\partial^n}{\\partial\\omega^n}\\left(\\frac{1}{\\omega^2+1}\\right).$$ For example, for $n=1$ we simply get $\\cf\\left[xe^{-|x|}\\right](\\omega)=\\frac{2\\omega}{(\\omega^2+1)^2}i$ . Example: Fourier transform of the Gaussian Letting $f(x)=e^{-x^2}$ , its Fourier transform is $$\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2}e^{-i\\omega x}\\dx,$$ which doesn’t seem to simple to compute. Instead, let us use the fact that we can take the derivative beneath the sign of the integral to get that : $$ 2\\pi \\hat{f}'(\\omega)= \\int_{-\\infty}^{\\infty} e^{-x^2}(-ix)e^{-i\\omega x}\\dx = \\frac{i}{2} \\int_{-\\infty}^{\\infty} (-2x)e^{-x^2}e^{-i\\omega x}\\dx.$$ Taking $u(x)=e^{-x^2}$ , and $v(x)=e^{-i\\omega x}$ , our integral is $u'(x)v(x)$ so we can do integration by parts. It follows that $$ 2\\pi \\widehat{f}'(\\omega)= \\frac{i}{2} e^{-x^2}e^{-i\\omega x}\\mid_{-\\infty}^{\\infty} - \\frac{1}{2}\\omega \\int_{-\\infty}^{\\infty} e^{-x^2}e^{-i\\omega x}=\\pi\\omega \\hat{f}(\\omega).$$ The solution to the differential equation $\\hat{f}'(\\omega)=\\frac{\\omega}{2}\\hat{f}(\\omega)$ is $ae^{-\\omega^2/4}$ . Finally, since $$a = \\hat{f}(0)=\\frac {1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2}\\cdot 1 \\dx = \\frac{1}{2\\sqrt{\\pi}},$$ we conclude that $$\\hat{f}(\\omega) = \\frac{1}{2\\sqrt{\\pi}} e^{-\\omega^2/4}.$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_motivation": {
    "title": "מוטיבציה - התמרת פורייה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_motivation",
    "body": "מה אנחנו חוקרים? “התמרת פורייה” זה אחד הכלים הכי שימושיים כאשר רוצים להבין פונקציות ממשיות (ומרוכבות). אלו פונקציות שכמובן מופיעות בצורה טבעית בשלל מקומות בעולם, למשל: פיזיקה: פונקציות מקום, מהירות, כח, טמפרטורה, לחץ וכו’, פונקציות דיגיטליות: צבעי פיקסלים בתמונות, קבצי אודיו, או תנועה באנימציות, פונקציות “אבסטרקטיות”: כמו מחירי מניות בבורסה, או מדדי פופלריות באתרי מדיה חברתית. ה”פונקציות הממשיות” האלו מגיעות בכל מני צורות, כתלות בתחום הגדרה שלהן, כאשר המקרים הכי נפוצים הם: פונקציות על הישר הממשי: $f:\\RR\\to\\RR$ , פונקציות מחזוריות, כלומר $f:\\RR\\to\\RR$ עם זמן מחזור $ T>0 $ כך ש $f(x+T)=f(x)$ . פונקציות דיסקרטיות על קבוצה סופית $f:\\{1,2,...,n\\}\\to\\RR$ , פונקציות דיסקרטיות על קבוצה אינסופית $f:\\ZZ\\to\\RR$ , שאנחנו סדרך קוראים להן פשוט סדרות. בגלל שיש כל כך הרבה סוגים של פונקציות ממשיות, ומאחר והן כל כך שימושיות, זה מאוד חשוב למצוא מבנים וכלים מתמטיים על מנת לחקור אותם. כבר למדנו על כמה כאלו בקורסים הראשונים באלגברה וחדו”א, כמו: קודם כל, צריך לבחור את הסוג פונקציות : חסומות, רציפות, גזירות, אולי רק רציפות למקוטעין, אינטגרביליות וכו’. אלגברה לינארית: בדרך כלל המשפחת פונקציות האלו תהווה מרחב ווקטורי, מה שמאפשר לנו להשתמש בכל הכלים מאלגברה לינארית (פונקציות לינאריות, בסיס, מימד וכו’). חדו”א: עבור פונקציות על הישר הממשי, אפשר גם לדבר על הרציפות שלהן, נגזרות, אינטגרלים וכו’. גאומטריה: אנחנו יכולים גם לבחור “גודל”, או בצורה פורמלית את הנורמה, של פונקציה בכל מני דרכים, כל אחת עם היתרונות והחסרונות שלה. למשל, אפשר למדוד את השטח (בערך מוחלט) מתחת לגרף של הפונקציה: אחת התכונות החשובות של נורמות כאלו היא שהן מקיימות את אי שוויון המשולש $\\norm{f+g} \\leq \\norm f + \\norm g$ , מה שמאפשר לנו להגדיר מרחקים במרחב , כלומר $dist(f,g):=\\norm{f-g}$ ובכך בעצם להגדיר “גאומטריה” על המרחב. זוויות: בצורה יותר כללית, אנחנו יכולים לחשוב על “זוויות” בין הפונקציות, ואז לנסות למשוך אינטואיציה מהמרחבים האוקלידים הרגילים (למשל, רעיונות כמו משפט פיתגורס). בצורה יותר פורמלית, אנחנו מוסיפים מכפלה פנימית למרחבים שלנו (שנזכיר את הרעיונות שם בהמשך). הרעיון המרכזי בהתמרות פורייה, הוא שיש לנו מבנה נוסף מאוד מעניין על המרחבים האלו: סימטריה: לכל המשפחות שהזכרנו יש “סימטריות” מאוד נחמדות. אנחנו בדרך כלל חושבים על סימטריה כשיקופים - יש לנו את האובייקט ה”מקורי”, וכאשר אנחנו משקפים אותו דרך המראה, אנחנו מקבלים את “אותו האובייקט” (או לכל הפחות הוא נראה כמו המקורי). דבר דומה קורה עם הפונקציות הממשיות שלנו - למשל, פונקציות על הישר הממשי ניתן להזיז ימינה ושמאלה ושוב לקבל פונקציות באותה משפחה: $$ \\text{Translation left and right: }f(x) \\mapsto f(x+c)$$ על הסימטריה הזאת של תזוזה ימינה/שמאלה ניתן לחשוב כתזוזה במרחב או בזמן (כתלות במה הפונקציות שלנו מתארות). המבנה ה”סימטרי” הזה נראה די פשוט תחילה, אבל הוא מוביל לשאלות מאוד מעניינות. למשל, האם יש “תבניות” מעניינות שחוזרות על עצמן יחסית לסימטריות שלנו? האם יש “תבניות יסודיות” כאלו שכדאי לחפש בכל הפונקציות? החיפוש הזה של תבניות כאלו הוא אחד המטרות העיקריות של הקורס, ובמובן מסוים “התמרות פורייה” נועדו בדיוק כדי לחקור את התביות האלו."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_Fourier_transform": {
    "title": "התמרת פורייה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_Fourier_transform",
    "body": "עד כה עבדנו עם פונקציות על הקטע $[-\\pi, \\pi]$ , או בצורה שקולה פונקציות שהן $2\\pi$ מחזוריות. עכשיו נרחיב את העולם שלנו ונסתכל על פונקציות כלליות על הישר הממשי $\\RR$ . מכפלות פנימיות עוד לפני שננסה להבין מה אומרת תורת פורייה על הישר הממשי, יש לנו כבר בעיה בהגדרה של המכפלה הפנימית: $$.\\angles{f,g}:=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}f(x)\\overline{g(x)}\\dx$$ האינטגרל למעלה יכול להתבדר אפילו עבור פונקציות מאוד יפות, ובפרט $\\angles{1,1}=\\infty$ . כדי לפתור את הבעיה הזאת, כמו עבור פונקציות על קטעים סופיים, נסתכל על תתי משפחות של פונקציות, שמצד אחד המכפלה הפנימית “תעבוד” עבורם, ומצד שני הן יהיו מספיק גדולות כדי שרב הפונקציות בנרצה לעבוד איתן יהיו שם. הגדרה: נורמות על פונקציות על $\\RR$ עבור פונקציה $f:\\RR\\to\\CC$ נגדיר את הנורמות הבאות: $$\\begin{align} \\norm f_1 & = \\int_{-\\infty}^{\\infty} \\abs{f(x)} \\dx \\\\ \\norm f_2 & = \\frac{1}{2\\pi} \\left(\\int_{-\\infty}^{\\infty} \\abs{f(x)}^2 dx\\right)^{1/2} \\\\ \\norm f_\\infty & = \\sup_x \\abs{f(x)} \\end{align}$$ בנוסף, עבור $p=1,2,\\infty$ נסמן: $$.E^p(\\RR) = \\{f:\\RR \\to \\CC \\;\\mid\\; f \\text{ piecewise continuous}, \\norm f_p עבור הנורמות המקבילות בקטע הסופי $[-\\pi,\\pi]$ ראינו שמתקיים ש $\\norm f_1 \\leq \\norm f_2 \\leq \\norm f_\\infty$ (עד כדי סקלר נרמול), כלומר נורמת אינסוף היא “הכי חזקה” ואחת השאלות ששאלנו היא מתי התכנסות בנורמה 2 (של טור פורייה) גורר התכנסות בנורמת אינסוף. הטענה הזאת כבר לא נכונה על הישר $\\RR$ כולו, למשל עבור $f\\equiv 1$ מתקיים ש $\\norm f_2 = \\norm f_1 =\\infty$ ואילו $\\norm f_\\infty =1$ . כתרגיל, הראו ש: תרגיל: לכל $p\\in\\{1,2,\\infty\\}$ מצאו פונקציה $f$ כך ש $\\norm{f}_p בעוד ששתי הנורמות האחרות הן אינסוף. בעוד שכפי שראינו המכפלה הפנימית שלנו מלמעלה לא מוגדרת עבור פונקציות חסומות ב $E^\\infty(\\RR)$ , הן כן מוגדרת עבור $E^2(\\RR)$ עם הוכחה דומה למה שהיה במקרה של הקטע החסום $[-\\pi,\\pi]$ . המכפלה הפנימית על $E^2(\\RR)$ הפונקציה $\\angles{f,g}$ מלמעלה מוגדרת היטב ומגדירה מכפלה פנימית על $E^2(\\RR)$ . הוכחה: ברגע שנראה שהפונקציה מוגדרת היטב (כלומר האינטגל מתכנס), שאר התכונות של המכפלה הפנימית מוכחות בקלות (כתרגיל). כבר ראינו את ההוכחה במקרה של הקטע החסום, אך כתזכורת, בגלל ש $\\abs{f(x)\\overline{g(x)}}\\leq \\abs{f(x)}^2+\\abs{g(x)}^2$ , אנחנו מקבלים שהאינטגרל מתכנס בהחלט וחסום ע”י: $$.\\int_{-\\infty}^\\infty\\abs{f(x)g(x)}\\dx \\leq \\norm{f(x)}_2^2+\\norm{g(x)}_2^2 בשלב הזה, נוכל להתחיל לחפש בסיס אורתונורמלי ולעשות תהליך דומה למרחב של הפונקציות על $[-\\pi,\\pi]$ . אבל, המטרה העיקרית שלנו היא להבין פונקציות כלליות ע”י פונקציות מחזוריות, ופרט לפונקציית האפס, אף פונקציה מחזורית היא לא ב $E^2(\\RR)$ כי האינטגרל של הריבוע שלה בערך מוחלט תמיד יהיה אינסוף. לכן, במקום לעבוד מעל $E^2(\\RR)$ דווקא נעבוד מעל “מרחב” אחר שם יש לנו את הפונקציות המחזוריות, ובהמשך נראה את הקשר חזרה לפונקציות ב $E^2(\\RR)$ . התמרת פורייה על $E^1(\\RR)$ הגדרה: התמרת פורייה עבור פונקציה $f:\\RR\\to \\CC$ ו $\\omega\\in \\RR$ נכתוב $$.\\hat f(\\omega) := \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx = \\angles{f,e^{i\\omega x}}$$ אם הגבול הזה קיים לכל $\\omega \\in \\RR$ נקרא לפונקציה $\\hat{f}:\\RR\\to\\CC$ בשם ההתמרת פורייה של $f$ . לעיתים נכתוב גם $\\cf[f]=\\hat{f}$ . נשים לב שלמרות שאנחנו משתמשים בסימון של המכפלה הפנימית, הפונקציות עצמן הן לא ב $E^2(\\RR)$ (למשל, הפונקציה $e^{i\\omega x}$ היא מחזורית ולכן לא במרחב הזה) ולכן באופן כללי האינטגרל לא אמור להתכנס. למזלנו, יש לנו את התוצאה הבאה: למה: אם $f\\in\\ E^1(\\RR)$ , אז $\\hat{f}(\\omega)$ מוגדרת היטב לכל $\\omega$ ומתקיים ש $\\norm{\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi} \\norm{f}_1$ . הוכחה: זהו חישוב פשוט, ע”י שנראה שהאינטגרל מתכנס בהחלט: $$.\\abs{\\hat{f}(\\omega)}=\\abs{\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx} \\leq \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\abs{f(x)}\\abs{e^{-i\\omega x}}dx=\\frac{1}{2\\pi} \\norm f_1$$ הערה: הכללה של מכפלות פנימיות כפי שאמרנו, ה”מכפלה הפנימית” $\\angles{f,e^{i\\omega x}}$ היא כבר לא מכפלה פנימית סטנדרטית לפי ההגדרה, כי אנחנו לא לוקחים פונקציות מתוך אותו מרחב $E^2(\\RR)$ , אך כן יש משמעות לביטוי הזה. למעבר למכפלה הפנימית בין פונקציות ב $E^2(\\RR)$ זוהי עכשיו גם מכפלה בין פונקציה ב $E^1(\\RR)$ ו $E^\\infty(\\RR)$ . שני הזוגות מספרים האלו מקיימים את ה”משוואה” המעניינת: $$.\\frac{1}{1}+\\frac{1}{\\infty}=\\frac{1}{2}+\\frac{1}{2}=1$$ מסתבר שזה תכונה יותר כללית והפונקציה הזאת מגדירה מכפלה בין מרחבים נוספים. לפרטים נוספים, כדאי לקרוא על אי שוויון הלדר. דוגמאות נראה שתי דוגמאות חשובות להתמרת פורייה: דוגמא: הפונקציה $\\chi_{[a,b]}$ עבור $a נגדיר את הפונקציה המציינת $$.f(x)=\\chi_{[a,b]}(x)=\\cases{1&x\\in[a,b]\\\\0&else}$$ חישוב של התמרת פורייה של הפונקציה הזאת יתן: $$.2\\pi \\hat{f}(w)= \\int_{-\\infty}^{\\infty} \\chi_{[a,b]}(x)e^{-i\\omega x}dx = \\int_{a}^{b} e^{-i\\omega x}dx =\\frac{1}{\\omega} ie^{-i\\omega x}\\mid_a^b = \\frac{e^{-i\\omega a}-e^{-i\\omega b}}{\\omega i} $$ הביטוי למעלה אינו מוגדר כאשר $\\omega=0$ ולכן שם צריך לחשב בנפרד ולקבל ש $$. \\hat{f}(0)=\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\chi_{[a,b]}(x)dx=\\frac {b-a}{2\\pi}$$ קל לראות שהפונקציה $\\hat{f}(\\omega)$ היא רציפה עבור $\\omega \\neq 0$ אבל גם לא קשה לבדוק שהיא רציפה ב $\\omega =0$ . בנוסף, מאחר ו $e^{-i\\omega a}$ חסומה בערך מוחלט, אז כאשר $\\omega \\to \\infty$ נקבל ש $\\hat{f}(\\omega)\\to 0$ . נראה בהמשך שזו תופעה הרבה יותר כללית. בנוסף, עבור $a=-b$ , כלומר כאשר הפונקציה היא זוגית, ההתמרה תהיה: $$\\hat{f}(\\omega)=\\cases{\\frac{\\sin(\\omega b)}{\\omega \\pi} & \\omega \\neq 0 \\\\ \\frac{b}{\\pi} & \\omega = 0.}$$ דוגמא: הפונקציה $e^{-a|x|}$ עבור $a>0$ . קודם כל, נשים לב ש $f(x):=e^{-a|x|}\\in E^1(\\RR)$ ולכן ההתמרה שלה מוגדרת היטב. החישוב עצמו של ההתמרה הוא: $$\\begin{align} 2\\pi \\cdot \\hat f(\\omega) & = \\int_{-\\infty}^{\\infty} e^{-a|x|} e^{-i\\omega x}\\dx = \\int_{-\\infty}^{0} e^{(a-i\\omega) x}\\dx + \\int_{0}^{\\infty} e^{(-a-i\\omega) x}\\dx \\\\ & = \\int_{0}^{\\infty}\\left(e^{-(a-i\\omega)x}+e^{(-a-i\\omega)x}\\right)\\dx=\\left[\\frac{e^{-ax}e^{i\\omega x}}{i\\omega-a}-\\frac{e^{-ax}e^{-i\\omega x}}{i\\omega+a}\\right]_{0}^{\\infty}. \\end{align}$$ מאחר ו $|e^{i\\omega x}|$ חסומה ע”י 1, ובנוסף $e^{-ax}\\to 0$ כאשר $x\\to\\infty$ , נוכל להסיק ש $$. 2\\pi \\cdot \\hat f(\\omega) = \\frac{1}{i\\omega+a}-\\frac{1}{i\\omega-a} = \\frac{2a}{\\omega^2+a^2}$$"
  },"/Fourier_Notes/notes/FourierHebrew/basic%20properties": {
    "title": "תכונות בסיסיות",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/basic%20properties",
    "body": "התמרת פורייה היא פעולה מאוד מעניינת והיא משמרת כל מני תכונות ופעולות שאנחנו רגילים לעבוד איתן. כפי שנראה מייד, רובן נובעות כמעט מיידית עקב העובדה שההתמרה מוגדרת באמצעות פונקציית האקספוננט $e^{iwx}$ המרוכבת, כאשר הכח הגדול שיש לפונקציה הזאת הוא שהיא לוקחת סכומים (הזזות) לכפל (סיבובים). לפני שנראה ונוכיח את התכונות האלו, נזכיר שההתמרה שלנו עובדת לכל $f\\in E^1(\\RR)$ ואף מתקיים ש $\\norm {\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi}\\norm f_1$ . לא קשה לבדוק שאם $g\\in E^\\infty (\\RR)$ היא חסומה, אז $\\norm{f\\cdot g}_1 \\leq \\norm{f}_1\\norm{g}_\\infty$ ולכן $f\\cdot g\\in E^1(\\RR)$ וגם לה אפשר לבצע התמרה, ובפרט זה נכון לפונקציות סיבוב מהצורה $e^{icx}$ . טענה: פעולות אריתמטיות תהא $f\\in E^1(\\RR)$ . אז : התמרת פורייה היא לינארית: כלומר $\\cf(\\alpha f + g) = \\cf(f)+\\cf(g)$ . כפל בסקלר: אם $\\lambda\\neq0$ , אז עבור $g_\\lambda(x)=f(\\lambda x)$ נקבל ש $$\\hat g_\\lambda (\\omega) = \\frac {\\hat{f}(\\omega/\\lambda)}{\\abs{\\lambda}}.$$ הזזה $\\Leftarrow$ סיבוב: עבור ההזזה $f_\\alpha(x):=f(x+\\alpha)$ נקבל את ההתמרה: $$.\\hat{f}_\\alpha(\\omega) = e^{i\\alpha\\omega}\\hat{f}(\\omega)$$ סיבוב $\\Leftarrow$ הזזה: עבור $c\\in \\RR$ נסמן $h_c(x)= e^{icx} f(x)$ . אז $$\\hat{h}_c(\\omega) = \\hat{f}(\\omega - c)$$ הצמדה: מתקיים ש $$.\\overline{\\cf[f](\\omega)}=\\cf[\\overline{f}](-\\omega)$$ הוכחה: נובע מכך שהאינטגרל זו פונקציה לינארית. עבור $\\lambda>0$ עושים החלפת משתנים (ל $\\lambda x$ ): $$\\hat{g}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(\\lambda x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega/\\lambda) x}\\frac{1}{\\lambda}dx = \\frac{1}{\\lambda}\\hat f(\\frac{\\omega}{\\lambda}).$$ עבור $\\lambda ההוכחה דומה. נובע מהחלפת משתנים (ל $x+\\alpha$ ): $$\\hat{f_\\alpha}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x+\\alpha)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega (x-\\alpha)}dx = e^{i\\omega\\alpha}\\hat f(\\omega).$$ פה אפילו לא צריך לעשות החלפת משתנים, רק החלפת פרמטר: $$\\hat{h_c}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{icx}f(x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega-c)x}dx = \\hat f(\\omega-c).$$ נובע מכך שהצמדה של אינטגרל זה אינטגרל של הצמדה: $$2\\pi \\overline{\\hat{f}(\\omega)} = \\overline{\\int_{-\\infty}^{\\infty} f(x) e^{-i\\omega x}\\dx} = \\int_{-\\infty}^{\\infty} \\overline{f(x)} e^{i\\omega x}\\dx = 2\\pi \\hat{\\overline{f}}(-\\omega)$$ דוגמא: הפונקציות $\\chi_{[a,b]}(x)$ ו $e^{-|ax|}$ . כבר חישבנו את התמרה של $\\chi_{[a,b]}(x)$ : $$.\\hat{ \\chi}_{[a,b]}(\\omega)=\\cases{\\frac{e^{-i\\omega a}-e^{-i\\omega b}}{2\\pi\\omega i} & \\omega\\neq 0 \\\\ \\frac{b-a}{2\\pi} & \\omega=0}$$ במקרה המאוד פשוט של $[a,b]=[-1,1]$ נקבל את ההתמרה $$.\\hat{\\chi}_{[-1,1]}(\\omega)=\\cases{\\frac{\\sin(\\omega)}{\\pi\\omega} & \\omega\\neq 0 \\\\ \\frac{1}{\\pi} & \\omega=0}$$ עד כדי הזזות מתיחה, פונקציה מציינת של קטע סופי תמיד ניתן להעביר לפונקציה מציינת של $[-1,1]$ , למשל: $$.\\chi_{[a,b]}(x) =\\chi_{[-\\frac{b-a}{2},\\frac{b-a}{2}]}(x-\\frac{a+b}{2}) =\\chi_{[-1,1]}(\\frac{2x}{b-a}-\\frac{a+b}{b-a})$$ זה מקשר לנו בין ההתמרות של שתי הפונקציות ע”י מתיחות וסיבובים, ובפרט: $$\\hat{\\chi}_{[a,b]}(\\omega)=e^{i\\frac{a+b}{a-b}\\omega}\\cdot \\frac{b-a}{2}\\cdot \\hat{\\chi}_{[-1,1]}\\left(\\frac{b-a}{2}\\omega\\right)$$ במילים אחרות, כל הפונקציות האלו “נובעות” מאותה פונקציה, ולכן כל ההתמרות ניתן לחשב מאותה התמרה. בצורה דומה הפונקציה $e^{-|ax|}$ היא מתיחה של הפונקציה $e^{-x}$ ולכן ניתן לקשר בין ההתמרות שלהן. הפונקציה $f(x)=e^{-x}\\cdot \\chi_{[0,\\infty)}(x)$ החישוב היחיד של ההתמרה של הפונקציה הזאת הוא לא קשה: $$.\\hat{f}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-x}\\chi_{[0,\\infty)}(x)e^{-i\\omega x}\\dx = \\frac{1}{2\\pi}\\int_{0}^\\infty e^{-(1+i\\omega) x}\\dx = \\frac {1}{2\\pi(1+i\\omega)}$$ אם נשים לב ש $e^{-|x|}$ מורכבת משני עותקים של $f(x)$ כלומר $e^{-|x|}=f(x)+f(-x)$ אז נוכל לחשב את ההתמרה של $e^{-|x|}$ גם דרך הפירוק הזה: $$.\\widehat{e^{-|x|}}(\\omega) =\\hat{f}(\\omega)+\\frac{1}{|-1|}\\hat{f}(\\frac{\\omega}{-1}) =\\hat{f}(\\omega)+\\hat{f}(-\\omega)= \\frac{1}{2\\pi}\\left(\\frac{1}{1+i\\omega}+\\frac{1}{1-i\\omega}\\right)=\\frac{1}{\\pi(1+\\omega^2)}$$ מהטענה למעלה קיבלנו שההתמרה לינארית, כלומר היא מעבירה חיבור של פונקציות וכפל בסקלר לחיבור וכפל בסקלר. בנוסף, השילוב של כפל בסקלר ממשי והכפל $\\lambda x$ שהם בעצם מתיחות/כיווצים עוברים גם למתיחות וכיווצים ובפרט $$\\sqrt{|\\lambda|} \\cdot f(\\lambda x) \\Rightarrow \\sqrt{|\\lambda|^{-1}} \\hat{f}(\\lambda^{-1} \\omega)$$ לבסוף, גם ההצמדה של פונקציה מותמרת (פחות או יותר) להצמדה של ההתמרה. הפעולות של סיבוב והזזה לא מותמרות ל”עצמן”, אבל הן כן עוברות אחת לשניים: הזזה מותמרת לסיבוב וסיבוב מותמר חזרה להזזה. הדואליות הזאת היא משהו מיוחד שמרמז שאולי אם נבצע את ההתמרה פעמיים, אז נחזור לפונקציה שממנה יצאנו, ובאמת נראה משהו כזה בהמשך. לפני זה, בעוד שנוח מאוד לעשות חישובים עם האקספוננטים, הרבה פעמים נרצה לעבוד ממש עם פונקציות ממשיות, ומסקנה מיידית של סיבוב-&gt; הזזה נותן לנו את התוצאה הבאה: טענה: סיבוב $\\Leftarrow$ הזזה, בעולם הממשי עבור כפל בסינוסים וקוסינוסים נקבל: $$\\align{ \\cf[f(x)\\sin(cx)]&=\\frac{\\cf[f](\\omega-c)-\\cf[f](\\omega+c)}{2i} \\\\ \\cf[f(x)\\cos(cx)]&=\\frac{\\cf[f](\\omega-c)+\\cf[f](\\omega+c)}{2} }$$ ברגע שיש לנו את התכונות האלו והפעולות ש”נשמרות” תחת ההתמרה, שאלה מעניינת שאפשר לשאול זה מה אפשר להגיד על פונקציות “מיוחדות” יחסית לפעולות האלו. למשל, מה קורה לפונקציה שמראש צמודה לעצמה, כלומר $f(x)=\\overline{f(x)}$ לכל $x$ או במילים אחרות פונקציה ממשית. טענה: פונקציות שמורות אם פונקציה $f$ היא זוגית (בהתאם אי זוגית), אז $\\cf[f]$ היא זוגית (בהתאם אי זוגית). אם פונקציה $f$ היא ממשית (בהתאם מדומה טהורה), אז $\\overline{\\cf[f](\\omega)}=\\cf[f](-\\omega)$ (בהתאם שווה ל $-\\cf[f](-\\omega)$ ). בפרט, אם $f$ היא ממשית וזוגית, אז $\\hat{f}$ היא ממשית וזוגית, ואם $f$ היא ממשית ואי זוגית, אז $\\hat{f}$ היא מדומה טהורה ואי זוגית. הוכחה: נסתכל על זהות הכפל בסקלר עם $\\lambda = -1$ , ונסמן $g(x)=f(-x)$ . במקרה הזה נקבל ש $\\hat{g}(\\omega)=\\hat{f}(-\\omega)$ . אם $f$ זוגית, כלומר $g=f$ אז כמובן שגם $$\\hat{f}(\\omega)=\\hat{g}(\\omega)=\\hat {f}(-\\omega)$$ או במילים אחרות $\\hat{f}$ היא זוגית. באותה צורה אם $f$ היא אי זוגית אז גם $\\hat{f}$ היא אי זוגית. שאר הטענה נובעת עם הוכחה דומה יחד עם העובדה ש $f$ ממשית (בהתאם מדומה טהורה) אם $\\bar{f}=f$ (בהתאם $\\bar{f}=-f$ ). דוגמא: ראינו כבר את ההתמרות של הפונקציות הממשיות וזוגיות: $$\\align{e^{-a|x|},a>0 & \\longrightarrow \\frac{a}{\\pi(\\omega^2+a^2)} \\\\ \\chi_{[-a,a]} & \\longrightarrow \\frac{\\sin(\\omega a)}{\\omega \\pi}}$$ עבור דוגמאות לפונקציות ממשיות אי זוגיות, אפשר למשל לכפול את הפונקציות למעלה ב $x$ . בהמשך נראה ש $\\cf[xf(x)] = i\\cf[f]'$ , ולכן בגלל ש $\\cf[f]$ היא פונקציה זוגית ממשית אז הנגזרת שלה אי זוגית ממשית, ולכן $i\\cf[f]'$ היא אי זוגית ומדומה טהורה. דוגמא יותר פשוטה, בואו נחשב את ההתמרה סינוס על קטע סופי (כדי שהפונקציה תהיה אינטגרבילית), למשל $\\chi_{[-\\pi,\\pi]}(x)\\cdot \\sin(x)$ . מהתכונות שראינו למעלה על התמרות של סיבובים (כפל בסינוס), נקבל ש $$\\cf\\left[\\chi_{[-\\pi,\\pi]}(x)\\cdot \\sin(x) \\right](\\omega) = \\frac{\\cf\\left[\\chi_{[-\\pi,\\pi]}\\right](\\omega-1) - \\cf\\left[\\chi_{[-\\pi,\\pi]}\\right](\\omega+1)}{2i} = \\frac{\\frac{\\sin((\\omega-1)\\pi)}{(\\omega-1)\\pi} - \\frac{\\sin((\\omega+1)\\pi)}{(\\omega+1)\\pi}}{2i}$$ הפונקציה שקיבלנו היא מדומה טהורה. בנוסף, הפונקציה היא אי זוגית כי הפונקציה במכנה היא אי זוגית, בדיוק כפי שציפינו."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_transform_derivatives": {
    "title": "רציפות וגזירות",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_transform_derivatives",
    "body": "עד עכשיו ראינו תכונות בסיסיות של התמרות. עכשיו נעבור לתכונות היותר מתקדמות של רציפות וגזירות, ובפרט נראה איך התמרת פורייה הופכת את פעולת הנגזרת לכפל ב $\\omega$ . היתרון הגדול של זה, כפי שנראה בהמשך, הוא שנוכל להשתמש בהתמרה כדי להפוך משוואה דיפרנציאלית עם נגזרות למשוואה אלגברית שבדרך כלל אותה הרבה יותר קל לפתור. רציפות נתחיל עם רציפות של ההתמרה. משפט: התמרה של פונקציה היא רציפה אם $f\\in E^1(\\RR)$ אז $\\hat{f}$ היא רציפה. הוכחה: כדי לקבל אינטואיציה עבור ההוכחה, נסתכל תחילה על הפונקציה $f(x)=e^{-|x|/10}$ . היינו רוצים להראות שהאינטגרל על $f(x)e^{-i\\omega x}$ ו $f(x)e^{-i\\omega_0x}$ הם מאוד קרובים אם $\\omega,\\omega_0$ מאוד קרובים. בגלל שקשה לצייר פונקציות מרוכבות, במקום נסתכל על המקביל הממשי שלהן, ובפרט בתמונה למעלה הפונקציה המקורית היא בירוק, ובנוסף יש את הפונקציות $f(x)\\sin(10x)$ ו $f(x)\\sin(10.2x)$ בכחול ואדום. מאחר ו $10, 10.2$ הם קרובים, היינו רוצים להראות שהאינטגרלים על הפונקציות המתאימות הם קרובים. יש שני רעיונות שנשתמש בהם, שאפשר לראות בתמונה למעלה: אם מסתכלים על האזור “ליד המרכז” אז הפונקציה האדומה והפונקציה הכחולה מאוד קרובות אחת לשנייה, ולכן נצפה שהאינטגרלים עליהן יהיו קרובים, כאשר נמצאים “רחוק מהמרכז”, זה אמנם כבר לא נכון, אבל מראש הפונקציה הירוקה היא מאוד קטנה, ולכן גם שנכפול אותה בסינוסים היא תשאר מאוד קטנה. לכן, אפילו שנסתכל על ההפרש בין שתי פונקציות, האינטגרל יהיה מאוד קטן. בצורה יותר פורמלית, נבחר איזשהו $M>0$ שיציין מה נחשב “קרוב למרכז” ונפרק את האינטגרל שלנו בהתאם: $$\\align{ 2\\pi\\abs{\\hat{f}(\\omega+h)-\\hat{f}(\\omega)} & \\leq \\int_{-\\infty}^\\infty\\abs{e^{-i(\\omega+h)x}-e^{-i\\omega x}}\\abs{f(x)}\\dx = \\int_{-\\infty}^\\infty\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx \\\\ & = \\int_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx + \\int_{\\abs{x}> M}\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx \\\\ & \\leq \\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\int_{\\abs{x}\\leq M}\\abs{f(x)}\\dx + 2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx \\\\ & \\leq \\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\norm f_1 + 2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx.}$$ נבחר את ה $\\varepsilon>0$ האהוב עלינו. בגלל ש $\\norm{f}_1 אז הזנב של האינטגרל מאוד קטן (צעד 2 שתיארנו למעלה) ולכן נוכל לבחור את $M$ להיות מספיק גדול כך ש $$.2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx עכשיו, כאשר $M$ נקבע, נקבל שעבור $|x|\\leq M$ ולכל $|h|$ מספיק קטן, הרציפות של $e^{-ihx}$ (צעד 1 שתיארנו למעלה) תגרור ש $$.\\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\norm f_1 סה”כ קיבלנו שכאשר $h\\to 0$ מתקיים ש $\\hat{f}(\\omega+h)\\to \\hat{f}(\\omega)$ , או במילים אחרות $\\hat{f}$ רציפה. הלמה של רימן לבג תהא $f\\in E^1(\\RR)$ אז $\\limfi{|\\omega|} \\hat{f}(\\omega) = 0$ . הוכחה: רעיון ההוכחה של הלמה של רימן לבג דומה להוכחת הרציפות שראינו למעלה. נתחיל שוב עם דוגמא ויזואלית שבה נכפול פונקציה ב $\\sin(\\omega x)$ עבור $\\omega$ מאוד גדול, וננסה להבין למה האינטגרל אמור להיות קרוב מאוד לאפס. כמו שהוכחה הקודמת, כאשר נרצה לחשב את האינטגרל של $f(x)\\sin(\\omega x)$ , אז כאשר נמצאים רחוק מהמרכז, אז מראש הפונקציה $f(x)$ היא מאוד “קטנה” במובן שהאינטגרל על הזנב שלה הוא מאוד קטן, ולכן גם כאשר נכפול ב $\\sin(\\omega x)$ לא יהיה שם הרבה שטח. מצד שני, כאשר אנחנו “קרובים” למרכז, אפשר לקרב את הפונקציה באמצעות פונקציה קבועה למקוטעין (הקטעים השחורים בתמונה), וכאשר כופלים פונקציה קבועה כזאת בסינוס עם תדירות מאוד גבוהה אז השטחים החיוביים והשליליים כמעט יבטלו אחד את השני, וככל שהתדירות יותר גבוהה כך יהיה יותר ביטול. פה חשוב מאוד שעברנו לקטע סופי עבור הפרוצדורה הזאת, כי אם היו לנו אינסוף קטעים שחורים כאלו, אז גם אם כל אחד מהם תורם מעט מאוד, יכול להיות שביחד שם תורמים הרבה שטח לאינטגרל, ופרט הוא לא יהיה קרוב לאפס. בואו ננסה לפרמל את הרעיונות האלו, וכמו מקודם נתחיל עם $M>0$ שיפריד את הישר לאזור הקרוב והרחוק מהמרכז: $$.\\align{ 2\\pi\\abs{\\hat{f}(\\omega)} & = \\abs{\\int_{-\\infty}^\\infty e^{-i\\omega x}f(x)\\dx} \\leq \\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}f(x)\\dx} + \\int_{\\abs{x}>M} \\abs{f(x)}\\dx}$$ גם פה, נבחר $\\varepsilon>0$ ובגלל ש $\\norm{f}_1 אז נוכל לבחור את $M$ להיות מספיק גדול כך ש: $$.\\int_{\\abs{x}>M} \\abs{f(x)}\\dx נעבור עתה לקטע החסום $[-M,M]$ ובגלל שהפונקציה אינטגרבילית רימן שם, ניתן לקרב אותה ע”י קומבינציה של פונקציות מציינות (הקטעים השחורים בתמונה), כלומר פונקציה מהצורה $h(x)=\\sum_1^n \\lambda_i \\chi_{[a_i,b_i]}$ המקיימת $\\int_{\\abs{x}\\leq M}\\abs{h(x)-f(x)}\\dx . באמצעות הפונקציה הזאת נוכל לחסום את הנסכם הראון מלמעלה: $$\\align{ \\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}f(x)\\dx}&=\\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}\\left(f(x)-h(x)\\right)\\dx + \\int_{\\abs{x}\\leq M} e^{-i\\omega x}h(x)\\dx} \\\\ & \\leq \\int_{\\abs{x}\\leq M} \\abs{f(x)-h(x)}\\dx + \\abs{\\sum_{i=1}^n\\lambda_i \\int_{\\abs{x}\\leq M} e^{-i\\omega x}\\chi_{[a_i,b_i]}(x)\\dx} \\\\ & \\leq \\frac{\\varepsilon}{4}+\\sum_{i=1}^n\\abs{\\lambda_i}\\hat{\\chi}_{[a_i,b_i]}(\\omega). }$$ כבר ראינו שעבור פונקציה מציינת מתקיים ש $\\hat{\\chi}_{[a_i,b_i]}(\\omega)=\\frac{e^{-i\\omega a}-e^{-i\\omega b}}{\\omega i}\\to0$ כאשר $|\\omega| \\to \\infty$ , ומאחר ואנחנו סוכמים על מספר סופי של פונקציות כאלו (בשלב הזה $n$ הוא קבוע), אז עבור כל $|\\omega|$ מספיק גדול, כל הביטוי למעלה יהיה לכל היותר $\\frac{\\varepsilon}{2}$ . אם נשים עכשיו את הכל יחד, נקבל שעבור $|\\omega|$ מספיק גדול מתקיים ש $|\\hat{f}(\\omega)| או במילים אחרות $\\limfi{|\\omega|}\\hat{f}(\\omega) = 0$ . גזירות נעבור לנגזרת של ההתמרה וההתמרה של הנגזרת, שכפי שראינו בתכונות קודמות, גם פה יש דואליות בין הפעולות האלו. טענה: אם $f$ רציפה, אינטגרבילית בהחלט (כלומר $\\norm{f}_1 ), גזירה למקוטעין ו $f'\\in E^1(\\RR)$ , אז $$.\\cf[f'](\\omega)=i\\omega \\cf[f](\\omega)$$ אם $xf(x)\\in E^1(\\RR)$ , אז $\\cf[f]$ גזירה ומתקיים ש $$.\\cf[f]'(\\omega)=\\cf[-ixf(x)](\\omega)$$ הוכחה: התמרה של הנגזרת: עבור ההתמרה של הנגזרת נרצה לחשב את $$,2\\pi \\widehat{f'}(\\omega) =\\int_{-\\infty}^{\\infty} f'(x)e^{-i\\omega x}\\dx$$ מה שיוביל אותנו להשתמש בנגזרת בחלקים. נזכיר שאינטגרציה בחלקים נובעת מכלל גזירה של מכפלה, כלומר $$,(uv)'=u'v+uv'$$ אך מאחר והפונקציה שלנו היא רק גזירה למקוטעין, אז השוויון הזה נכון למקוטעין. למזלנו, לאינטגרלים לא אכפת אם משנים את האינטגרנד במספר סופי של נקודות, ולכן השוויון השמאלי למטה נכון תמיד: $$.\\int_{-\\infty}^{\\infty} f'(x)e^{-i\\omega x}\\dx = \\int_{-\\infty}^{\\infty} (f(x)e^{-i\\omega x})' \\dx+i\\omega\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}\\dx \\overset{(*)}{=} f(x)e^{-i\\omega x}\\mid_{-\\infty}^{\\infty}+i\\omega\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}\\dx$$ עבור השוויון ב $(*)$ בעצם השתמשנו במשפט היסודי שם דרוש שהפונקציה $g(x)=f(x)e^{-\\omega x}$ תהיה גזירה ברציפות. מסתבר שאפשר להכליל את המשפט הזה קצת יותר ורק להניח ש $g$ רציפה וגזירה ברציפות למקוטעין, ונוכיח את זה למטה. תחת ההנחה שהשוויון למעלה נכון, עדיין נותר להבין מה זה $f(x)e^{-i\\omega x}\\mid_{-\\infty}^{\\infty}$ ובעצם נרצה להראות שזה שווה לאפס. הביטוי $e^{-i\\omega x}$ הוא חסום ולכן מספיק להראות ש $\\displaystyle{\\lim_{x\\to -\\infty}}f(x)=\\limfi{x}f(x)=0$ . מאחר ו $f(x)=f(0)+\\int_0^x f'(t)\\dt$ ונתון ש $f'\\in E^1(\\RR)$ אז קיימים הגבולות $\\limfi{x} f(x)$ ו $\\displaystyle{\\lim_{x\\to -\\infty}}f(x)$ . מאחר ואנחנו גם יודעים ש $\\int_{-\\infty}^\\infty f(x)\\dx$ קיים וסופי, אז הגבולות האלו חייבים להיות אפס, וזה מסיים את ההוכחה. נגזרת של התמרה: עבור הנגזרת של ההתמרה, כרגיל נקבל שאם מותר להחליף סדר אינטגרציה וגזירה, אז ההוכחה היא טריוויאלית: $$.2\\pi\\frac{\\partial}{\\partial \\omega}\\left(\\cf[f]\\right)(\\omega) = \\frac{\\partial}{\\partial \\omega}\\int_{-\\infty}^\\infty f(x)e^{-i\\omega x}\\dx = \\int_{-\\infty}^\\infty f(x)\\frac{\\partial}{\\partial \\omega}e^{-i\\omega x}\\dx = -i\\int_{-\\infty}^\\infty xf(x)e^{-i\\omega x}\\dx =-2\\pi i\\cf[xf(x)](\\omega)$$ כדי להוכיח פורמלית שאפשר לעשות את המעבר הזה, נעשה כמה צעדים לפשט את הבעיה: נחליף את הנגזרת בהגדרה שלה עם הגבול, נעביר אגפים ונראה שההפרש שווה לאפס, נשים ערך מוחלט, ואז רק צריך לחסום מלמעלה, נכניס את הערך המוחלט לאינטגרל ע”י אי שוויון המשולש. זה יוביל אותנו לפיתוח הבא: $$\\align{ 2\\pi\\abs{\\cf[f]'(\\omega) -\\cf[-ixf(x)] } & = \\abs{\\lim_{h\\to 0}\\int_{-\\infty}^{\\infty}f(x)\\frac{e^{-i(\\omega+h)x}-e^{-i\\omega x}}{h}\\dx + i\\int_{-\\infty}^{\\infty}xf(x) e^{-i\\omega x}\\dx} \\\\ & \\leq \\limsup_{h\\to 0}\\int_{-\\infty}^{\\infty}\\abs{xf(x)e^{-i\\omega x}\\left( \\frac{e^{-ihx}-(1-ihx)}{hx} \\right)}\\dx \\\\ & = \\limsup_{h\\to 0}\\int_{-\\infty}^{\\infty}\\abs{xf(x)}\\cdot \\abs{ \\frac{e^{-ihx}-(1-ihx)}{ihx} }\\dx. }$$ אנחנו עובדים תחת ההנחה ש $\\norm{xf(x)}_1 ולכן רק צריך להראות שהנכפל השני באינטגרנד הוא מאוד קטן כאשר $|h|$ מאוד קטן. קרוב לראשית: נשים לב תחילה שאם נציב $u=-ihx$ אז הגורם הזה שווה ל: $$.\\abs{\\frac{1}{u}(e^u-(1+u))} = \\abs{\\sum_2^\\infty \\frac{u^{n-1}}{n!}} = \\abs{u\\sum_0^\\infty \\frac{u^n}{(n+2)!}}\\leq \\abs{u}e^\\abs{u} = \\abs{hx}e^{\\abs{hx}}\\leq |hx|\\cdot e$$ לכן, אם נקבע $1>\\varepsilon>0$ , אז עבור $x$ קטן מספיק, נניח $|x| אז הביטוי הזה יהיה קטן מ $\\varepsilon$ . רחוק מהראשית: לעומת זאת, עבור $|x|>\\frac{\\varepsilon}{e|h|}$ גדולים החסם הזה כבר לא יהיה טוב, אבל נוכל לחסום את הביטוי בצורה שונה $$.\\abs{ \\frac{e^{-ihx}-1}{ihx} +1}\\leq \\frac{|e^{-ihx}|+1}{|hx|}+1\\leq \\frac{1+1}{\\varepsilon/e}+1 חיבור שני המקרים: אם נפריד את האינטגרל ל $x$ -ים הקטנים וה $x$ -ים הגדולים, נקבל ש $$\\align{\\int_{-\\infty}^{\\infty}\\abs{xf(x)}\\cdot \\abs{ \\frac{e^{-ihx}-(1-ihx)}{ihx} }\\dx & \\leq \\int_{|x|\\varepsilon/e|h|}\\abs{xf(x)}\\cdot \\frac{10}{\\varepsilon}\\dx \\\\ & \\leq \\norm{xf(x)}_1\\cdot \\varepsilon +\\frac{10}{\\varepsilon}\\cdot \\int_{|x| >\\varepsilon/e|h|}\\abs{xf(x)} \\dx}$$ עבור ה $\\varepsilon>0$ הקבוע שלנו, נקבל ש $\\frac{\\varepsilon}{e|h|}\\to \\infty$ כאשר $|h|\\to 0$ ולכן האינטגרל בנסכם השני שואף לאפס. במילים אחרות, קיבלנו ש $$.\\limsup_{h\\to 0}\\int_{-\\infty}^{\\infty}\\abs{xf(x)}\\cdot \\abs{ \\frac{e^{-ihx}-(1-ihx)}{ihx} }\\dx\\leq \\norm{xf(x)}_1\\varepsilon$$ אבל ה $\\varepsilon>0$ גם יכול להיות קטן כרצוננו, ולכן נקבל שהגבול חייב להיות אפס וסיימנו. כדי לקבל קצת אינטואיציה ויזואלית לצעד האחרון בהוכחה, נסתכל למשל על הדוגמא הבאה: הפונקציה $\\abs{xf(x)}$ היא הפונקציה האדומה, והגורם $\\abs{\\frac{e^{-ihx}-(1-ihx)}{ihx}}$ זו הפונקציה השחורה. הפונקציה הזאת מתחילה בראשית, עולה בצורה לינארית בהתחלה, ואז פחות או יותר מתייצבת באזור $y=1$ . ככל שמקטינים את $h$ , הפונקציה יותר ויותר נמתחת ולכן לוקח לה יותר זמן להגיע לחלק בו היא מתייצבת. כאשר כופלים אותה ב $|xf(x)|$ ומקבלים את הפונקציה הכחולה, אז כרגיל כאשר רחוקים מהראשית מראש לפונקציה האדומה אין הרבה שטח ולכן גם כפל בפונקציה החסומה השחורה לא יהיה עם הרבה שטח. לעומת זאת באזור הראשית, איפה שיש לפונקציה האדומה הרבה שטח, הפונקציה השחורה נעשית יותר ויותר קרובה לאפס ולכן הורגת אותו. אלו בדיוק שני החלקים של ההוכחה, כאשר כל מה שהיינו צריכים לעשות זה להפריד בין החלק ה”קרוב” ו”רחוק” מהראשית. לפני שנעבור לדוגמאות, אנחנו עדיין חייבים להכליל את המשפט היסודי: המשפט היסודי של חדו\"א לפונקציות גזירות למקוטעין תהא $f:\\RR \\to \\RR$ רציפה, וגזירה ברציפות למקוטעין כך ש $f'$ . אז לכל $a מתקיים ש $$.\\int_a^bf'(x)\\dx=f(b)-f(a)$$ הוכחה: נסמן את הנקודות אי גזירות של $f$ בקטע להיות $a=t_0 . בכל אחר מהקטעים $[t_k,t_{k+1}]$ הפונקציה גזירה ברציפות, ולכן ניתן להשתמש במשפט היסודי הרגיל, ולכן נקבל ש $$.\\int_a^b f'(x)\\dx = \\sum_{k=1}^n\\int_{t_{k-1}}^{t_k}f'(x)\\dx=\\sum_{k=1}^n(f(t_k)-f(t_{k-1}))=f(t_n)-f(t_0)=f(b)-f(a)$$ במילים אחרות, העובדה ש $f$ היא בעצמה רציפה, הוביל לכך שיש לנו טור טלסקופי שכל הנקודות ה”בעייתיות” מצטמצמות, ולכן המשפט היסודי עדיין תקף. דוגמאות דוגמא: הפונקציה $f(x)=e^{-|x|} x^n$ נשים לב תחילה ש $f(x)$ נמצאת ב $E^1(\\RR)$ ולכן יש לה התמרת פורייה. עבור המקרה של $n=0$ כבר חישבנו את ההתמרה להיות $\\cf\\left[e^{-|x|}\\right](\\omega) = \\frac{1}{\\omega^2 + 1}$ . שימוש בתכונה על הנגזרת של ההתמרה, עם קצת אינדוקציה, נותן לנו ש $$\\cf\\left[x^ne^{-|x|}\\right](\\omega) = (-i)^n \\cf\\left[(-ix)^ne^{-|x|}\\right](\\omega) = (-i)^n \\cf\\left[e^{-|x|}\\right]^{(n)}(\\omega) = (-i)^n \\frac{\\partial^n}{\\partial\\omega^n}\\left(\\frac{1}{\\omega^2+1}\\right).$$ למשל, במקרה של $n=1$ נקבל פשוט את $\\cf\\left[xe^{-|x|}\\right](\\omega)=\\frac{2\\omega}{(\\omega^2+1)^2}i$ . דוגמא: התמרת פורייה של גאוסיין $f(x)=e^{-x^2}$ . לפי ההגדרה, ההתמרה של $f$ היא $$,\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2}e^{-i\\omega x}\\dx$$ שלא נראית כל כך פשוטה לחישוב. במקום, נשתמש בכך שעבור התמרות מותר להחליף סדר אינטגרציה ונגזרת כדי לקבל $$ .2\\pi \\hat{f}'(\\omega)= \\int_{-\\infty}^{\\infty} e^{-x^2}(-ix)e^{-i\\omega x}\\dx = \\frac{i}{2} \\int_{-\\infty}^{\\infty} (-2x)e^{-x^2}e^{-i\\omega x}\\dx$$ עכשיו נוכל לקחת $u(x)=e^{-x^2}$ ו $v(x)=e^{-i\\omega x}$ ולשים לב שהאינטגרנד שלנו הוא $u'(x)v(x)$ , ולכן נוכל לעשות אינטגרציה בחלקים: $$ .2\\pi \\widehat{f}'(\\omega)= \\frac{i}{2} e^{-x^2}e^{-i\\omega x}\\mid_{-\\infty}^{\\infty} - \\frac{1}{2}\\omega \\int_{-\\infty}^{\\infty} e^{-x^2}e^{-i\\omega x}=\\pi\\omega \\hat{f}(\\omega)$$ קיבלנו משוואה דיפרנציאלית $\\hat{f}'(\\omega)=\\frac{\\omega}{2}\\hat{f}(\\omega)$ שהפתרון שלה הוא $\\hat{f}(\\omega)=ae^{-\\omega^2/4}$ . כדי למצוא את המקדם $a$ נציב תנאי התחלה $\\omega=0$ ונקבל $$,a = \\hat{f}(0)=\\frac {1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2}\\cdot 1 \\dx = \\frac{1}{2\\sqrt{\\pi}}$$ ולכן סה”כ קיבלנו ש $$.\\cf[e^{-x^2}](\\omega)=\\hat{f}(\\omega) = \\frac{1}{2\\sqrt{\\pi}} e^{-\\omega^2/4}$$ משפט ההתכנסות הנשלטת של לבג ראינו פה שלוש הוכחות על התכנסות מהצורה $\\limfi{y}\\int_{-\\infty}^\\infty f(x,y)\\dx$ . בשלושת המקרים רצינו להראות שאפשר להחליף סדר גבול ואינטגרציה והיו לנו הנחות שאומרת שהפונקציה באינטגרנד לא יותר מדיי גדולה, (או בצורה יותר ספציפית, היה לנו פונקציה חסומה כפול אינטגרבילית בהחלט). הסגנון הוכחות הזה חוזר על עצמו הרבה, ולשם השלמות נזכיר משפט (ללא הוכחה) שבדיוק מטפל במקרים כאלו. משפט ההתכנסות הנשלטת של לבג: (Lebesgue's dominated convergence theorem) יהיו $f_n:\\RR \\to \\CC$ סדרת פונקציות מדידות כך ש הפונקציות מתכנסות נקודתית לפונקציה $f$ , קיימת פונקציה אינטגרבילית בהחלט $g$ כך ש $|f_n(x)|\\leq g(x)$ לכל $n$ ולכל $x$ . במקרה זה נקבל ש $$\\limfi n\\int_{-\\infty}^\\infty\\abs{f_n(x)-f(x)}\\dx=0$$ ובפרט מתקיים ש $$.\\limfi n \\int_{-\\infty}^\\infty f_n(x)\\dx = \\int_{-\\infty}^\\infty f(x)\\dx $$ דוגמא: רציפות של ההתמרה בהוכחה של הרציפות של ההתמרה, רצינו להראות שהגבול הבא הוא אפס: $$\\lim_{h\\to 0}\\int_{-\\infty}^\\infty\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx $$ כאשר נתון לנו ש $\\norm{f}_1 . אם נסמן $f_h(x)=\\abs{(e^{-ihx}-1)f(x)}$ , כאשר הפעם השאיפה היא כש $h\\to 0$ במקום $n\\to \\infty$ כמו במשפט ההתכנסות הנשלטת, אז התכנסות נקודתית: לכל $x=x_0$ קבוע נקבל את ההתכנסות הנקודתית $\\displaystyle{\\lim_{h\\to 0}} f_h(x_0)=|0|\\cdot|f(x_0)|=0$ , כלומר הפונקציות מתכנסות נקודתית לפונקצית האפס. שליטה: אי שוויון המשולש נותן לנו ש $|f_h(x)|\\leq 2|f(x)|$ , כלומר הפונקציות נשלטות ע”י $2f(x)$ שהיא אינטגרבילית בהחלט. משפט ההתכנסות הנשלטת יתן לנו ש $$\\lim_{h\\to 0}\\int_{-\\infty}^\\infty f_h(x)\\dx = \\int_{-\\infty}^\\infty \\limfi _{h\\to 0}f_h(x)\\dx = \\int_{-\\infty}^\\infty 0 \\dx = 0$$ וזה יסיים את ההוכחה. תרגיל: השתמשו במשפט ההתכנסות הנשלטת כדי לקצר קצת את ההוכחה של הנגזרת של ההתמרה."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_delta_function": {
    "title": "התמרת פורייה ההפוכה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_delta_function",
    "body": "איך מוצאים את ההתמרה ההפוכה כאשר למדנו על טורי פורייה, התחלנו עם פונקציה מחזורית על $f:[-\\pi,\\pi]\\to \\RR$ , חישבנו ממנה את מקדמי פורייה $a_n,b_n$ והדרך העיקרית להבין בעזרתם את הפונקציה הקבועה הייתה להרכיב חזרה אתה פונקציה , כלומר $$.f(x) \\sim \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n \\cos(nx)+b_n \\sin(nx)\\right)$$ זה כמובן נבע מתכונות של בסיס אורתונורמלי, כי אם נסמן $$,\\{v_1,v_2,v_3,...\\}=\\{ \\frac{1}{\\sqrt{2}}\\}\\cup \\{\\cos(nx),\\sin(nx)\\;\\mid\\; n\\in \\NN\\}$$ אז מה שרשום למעלה זה בעצם $f\\sim \\sum _1^\\infty \\angles{f,v_k}\\cdot v_k$ . היינו רוצים לעשות דבר דומה גם בהתמרת פורייה. הפעם ה”מקדמים” שלנו הם $\\hat{f}(\\omega)=\\angles{f,e^{i\\omega x}}$ והם מוגדרים לכל $\\omega$ . אם זה היה דומה לפונקציות המחזוריות, אז היינו מחפשים משהו בסגנון $\\sum_\\omega \\hat{f}(\\omega) e^{i\\omega x}$ , אבל בגלל שהמקדמים הם לא דיסקרטים, אז נצטרך להסתכל על אינטגרל. תחת ההנחה שמותר לנו לעשות את כל המעברים הבאים, נקבל ש $$\\align{f(x_0)&\\overset{?}{=}\\int_{-\\infty}^\\infty \\hat{f}(\\omega)e^{i\\omega x_0}\\dom=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(x)e^{-i \\omega x}e^{i\\omega x_0}\\dx \\dom \\\\& =\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x) \\left(\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom\\right) \\dx }$$ יש שני דברים מוזרים בחישוב הזה: הפונקציה $\\omega\\mapsto e^{i\\omega(x_0-x)}$ היא לא אינטגרבילית בהחלט על כל הישר, כי בערך מוחלט היא שווה ל 1. יותר מכך, אפילו אם היה לה אינטגרל, שנסמן ב $g(x_0,x)$ , איזה פונקציה זו אמורה להיות כדי ש $$?\\;\\;\\;f(x_0)\\overset{?}{=}\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty f(x)g(x_0,x)\\dx$$ נזכיר שאם נשנה פונקציה בנקודה אחת, האינטגרל שלה לא ישתנה, כלומר אם נשנה את הערך $f(x_0)$ בנקודה אחת $x_0$ אז אגף ימין לא ישתנה, בעוד שאגף שמאל ישתנה. למרות שתי הבעיות האלו, עדיין אפשר “להגדיר” את ההתמרה ההפוכה, ולמרות שלא נכנס לכל ההוכחה המתמטית, בואו ננסה להבין קצת מאיפה היא מגיעה. נתחיל קודם עם הבעיה השנייה. למרות שאין אינטגרל שמקיים את המעבר הזה, יש פונקציה מאוד פשוטה שמתחילה עם $f$ ומסיימת עם $f(x_0)$ שהיא פשוט ההצבה בנקודה $x_0$ ונסמן אותה ב $$. \\;\\; \\delta_{x_0}(f):=f(x_0)$$ נשים לב תחילה שזו פונקציה מהצורה $\\delta_{x_0} : E^1(\\RR)\\to \\RR$ בדיוק כמו פונקציית האינטגרל (כלומר היא לוקחת פונקציה ומחזירה סקלר) ובנוסף כמו האינטגרל זוהי פונקציה לינארית, כי לפי ההגדרה: $$.\\;\\;\\delta_{x_0}(\\alpha f_1 + f_2) = \\alpha f_1(x_0) + f_2(x_0) = \\alpha \\delta_{x_0}(f_1)+\\delta_{x_0}(f_2)$$ אמנם אי אפשר להגדיר אותה ע”י רק אינטגרל, אבל האם ניתן לקרב אותה ע”י אינטגרל? אם הפונקציה $f$ היא רציפה, אז כן! למשל, נוכל לקחת את הפונקציות מדרגה הבאות שהשטח שלהן הוא תמיד 1 ושהוא נעשה יותר ויותר מרוכז סביב $x_0$ : $$\\sigma_{h,x_0}(x):=\\frac{2}{h}\\cdot \\chi_{[x_0-h,x_0+h]}(x)=\\cases{\\frac{2}{h} & |x-x_0|\\leq h\\\\ 0 & else}$$ אם נכפול אותן בפונקציה רציפה $f$ ונבצע אינטגרציה, אז נקבל ש $$\\int_{-\\infty}^{\\infty}f(x)\\sigma_{h,x_0}(x)\\dx = \\frac{2}{h} \\int_{x_0-h}^{x_0+h}f(x)\\dx\\overset{h\\to 0}{\\longrightarrow} f(x_0)$$ במילים אחרות, פונקצית ההצבה שלנו היא גבול של פונקציות שמבוטאות באמצעות אינטגרלים. זה גם מסביר קצת את הבעיה הראשונה. במקום לחשוב על $$x\\mapsto\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom$$ כפונקציה במשתנה $x$ שלא מוגדרת היטב כי האינטגרל לא מוגדר היטב, צריך לחשוב עליה כגבול של פונקציות (שיקבע לפי גבולות האינטגרציה כמו שנראה למטה) ואז פתאום נקבל את ההתמרה ההפוכה: $$.\\align{f(x_0)=\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x) \\left(\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom\\right) \\dx }$$ פונקצית דלתא של דירק עם האינטואיציה מלמעלה, “נגדיר” את פונקציית דלתא של דירק, עם אותו סימון כמו פונקציית דלתא למעלה: סימון: פונקציית דלתא של דירק נסמן $\\delta_a:\\RR \\to \\RR$ להיות ה”פונקציה” שמקיימת $$f(a)=\\int_I f(t)\\delta_a(t)\\dt $$ עבור כל פונקציה רציפה ב $a$ וקבוצה $I$ שמכילה את $a$ כנקודה פנימית. כאשר $a=0$ נסמן פשוט $\\delta(t):=\\delta_0(t)$ . הערה: בדרך כלל הקבוצה $I$ הולכת להיות כל הישר, אך כמובן שהפונקציה תלויה רק האיך ש $f$ מתנהגת באזור הנקודה $a$ . שימו לב שכמו שאמרנו למעלה, זהו סימון ואנחנו לא באמת מבצעים אינטגרציה במובן הרגיל. מה שכן, בגלל שפונקציית ההשמה $f\\mapsto f(a)$ מקיימת תכונות דומות לפונקצית האינטגרל, אז זהו סימון נוח. באופן רשמי פונקציה כזאת נקראת פונקציונלי. בפרט “נניח” שאפשר לבצע החלפת משתנים, ואז נקבל ש הזזה של פונקציית דיראק מתקיים: $$f(a+b)=\\int_{-\\infty}^{\\infty} f(t)\\delta_a(t-b)\\dt $$ ובפרט: $$f(b)=\\int_{-\\infty}^{\\infty} f(t)\\delta(t-b)\\dt $$ הוכחה: אם נרשה החלפת משתנים $s=t-b$ אז נקבל ש $$.\\int_{-\\infty}^{\\infty} f(t)\\delta_a(t-b)\\dt=\\int_{-\\infty}^{\\infty} f(s+b)\\delta_a(s)\\dt=f(a+b)$$ סימון: התכנסות לפונקציית דלתא כאשר נסמן התכנסות של פונקציות $g_n(x)\\to\\delta_a(x)$ לפונקציית דירק, הכוונה היא ש $$\\int_{-\\infty}^{\\infty}f(x)g_n(x)\\dx \\to f(a)$$ לכל פונקציה $f\\in E^1(\\RR)$ רציפה בנקודה $a$ . תחת הסימון הזה, בעצם בחלק הקודם הראנו ש $\\sigma_{h,a}\\to\\delta_a$ כאשר $h\\to 0$ . נשים לב שגם אם נחשוב על פונקציית דירק כגבול של אינטגרלים, אז לא מפתיע שהיא מקיימת תכונות כמו אינטגרציה ובפרט השימוש בהחלפת משתנים כמו שראינו למעלה. כדי להראות שתוצאה דומה נכונה עבור פונקציות האקספוננטים שלנו נציין תחילה טענה ללא הוכחה. עוד מעט נוכיח משהו יותר חזק, כרגע זה רק לשם האינטואיציה. טענה: מתקיים ש $\\displaystyle{\\lim_{M\\to\\infty}} \\frac{\\sin(Mx)}{\\pi x} = \\delta(x)$ . באמצעות הטענה הזאת נוכל עכשיו להראות ש: גבול של אקספוננטים הוא דירק: לכל $a\\in\\RR$ מתקיים ש $$\\limfi{N} \\frac{1}{2\\pi} \\int_{N}^{N} e^{i\\omega(t-a)}\\dom \\to \\delta_a(t)$$ הוכחה: בגלל שהראנו שאפשר להזיז את פונקציית דירק, כלומר $\\delta_a(t)=\\delta(t-a)$ אז נוכל לסמן $T=t-a$ ולהראות שהגבול הוא $\\delta(T)$ . עתה נקבל: $$\\frac{1}{2\\pi} \\int_{-N}^{N} e^{i\\omega T}\\dom = \\frac{1}{2\\pi}\\left( \\int_{-N}^{N} \\cos(\\omega T)\\dom + \\int_{-N}^{N} i\\sin(\\omega T)\\dom\\right)$$ פונקציה הסינוס היא אי זוגית, ולכן האינטגרל שלה מתאפס, וסה”כ נשאר עם $$\\frac{1}{2\\pi} \\int_{-N}^{N} e^{i\\omega T}\\dom = \\frac{1}{2\\pi} \\frac{\\sin(\\omega T)}{T}\\mid_{-N}^N = \\frac{\\sin(N T)}{\\pi T}$$ והתוצאה נובעת מהטענה הקודמת. עכשיו נוכל לחזור לטיעון שלנו בראש הדף בשביל לקבל שקיימת לנו התמרה הפוכה שמקיימת $$\\align{f(x_0) & =\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x) \\left(\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom\\right) \\dx }$$ בנקודות רציפות של $f$ . פונקצית דלתא באי רציפות מאחר והפונקציות שלנו באופן כללי הן לא רציפות, אז הטענה מלמעלה (שלא הוכחה לחלוטין) לא תעזור ועדיין נרצה לדעת מה קורה בנקודות אי רציפות. כדי לקבל קצת אינטואיציה בואו נסתכל על דוגמאות פשוטות וננסה להבין מה קורה שם, כאשר נשים דגש על הנקודה בראשית, כלומר אנחנו רוצים להבין את $$.\\int_{-\\infty}^\\infty f(x)\\frac{\\sin(Mx)}{\\pi x} \\dx$$ פונקציות אי זוגיות הפונקציות שהכי קל לחשב עבורן את האינטגרל למעלה הן הפונקציות האי זוגיות, למשל $$.f(x)=\\cases{\\frac{1}{1+x^2}&x\\geq 0 \\\\ \\frac{-1}{1+x^2} & x זוהי כמובן פונקציה רציפה פרט לראשית, ובחרנו אותה כך שהאינטגרל שלה יתכנס, כלומר $f\\in E^1(\\RR)$ . מאחר ו $f$ אי זוגיות ואינטגרבילית בהחלט ו $\\frac{\\sin(Mx)}{\\pi x}$ היא זוגית וחסומה, אז המכפלה שלהן היא אינטגרבילית בהחלט ואי זוגית ולכן $$.\\int_{-\\infty}^\\infty f(x) \\frac{\\sin(Mx)}{\\pi x}\\dx = 0$$ אנחנו מצפים שהערך הזה יהיה קשור לערך של הפונקציה בראשית, אך כמובן הוא לא הכי מוגדר טוב שם, כי הפונקציה לא רציפה בראשית. לעומת זאת, אם נצייר אותה אז פשוט נקבל את ואז המספר 0 הוא פשוט ממוצע הגבולות החלקיים: $$.\\frac{f(0^+)+f(0^-)}{2}$$ פונקציות מציינות מקרה ראשון “לא טריוויאלי” בו אנחנו ממש יכולים לחשב את האינטגרל הזה הוא כאשר נסתכל על הפונקצית מדרגה $f(x)=\\chi_{[a,b]}(x)$ ואז נקבל את האינטגרל $$\\int_{a}^b \\frac{\\sin(Mx)}{\\pi x} \\dx = \\int_{Ma}^{Mb} \\frac{\\sin(x)}{\\pi x} \\dx$$ כתרגיל הראו שהאינטגרל $\\int_0^\\infty \\frac{\\sin(x)}{x}\\dx$ מתכנס ומסתבר שגם אפשר לחשב את האינטגרל הזה (למשל עם כלים מפונקציות מרוכבות) ולהראות שהוא שווה ל $\\frac{\\pi}{2}$ . אם אנחנו מאמינים לזה, אז הגבול כאשר $M\\to \\infty$ למעלה תלוי באיפה נמצאת הראשית יחסית לקטע $(a,b)$ : $$\\limfi{M} \\int_{a}^b \\frac{\\sin(Mx)}{\\pi x} \\dx = \\cases{1 & a נשים לב שאם הפונקציה היא רציפה בראשית, כלומר $a או $0\\notin [a,b]$ אז באמת קיבלנו שהגבול של האינטגרל הוא הערך הראשית. אחרת, קיבלנו $\\frac{1}{2}$ שהוא “ממוצע” הערכים, כלומר $$.\\frac{f(0^-)+f(0^+)}{2}$$ מאחר והאינטגרל שלנו הוא פונקציה לינארית, אז נוכל לחשב אותו לכל קומבינציה לינארית (סופית) של פונקציות מדרגה. יותר מכך, אם $f$ פונקציה כללית שהיא אינטגרבילית בהחלט וקיימים לה הגבולות החד צדדיים בראשית, אז נוכל להגדיר $$.f_1(x)=f(x)-f(0^+)\\chi_{[-0,1]}-f(0^-)\\chi_{[-1,0]}$$ לפונקציה החדשה $f_1(x)$ מתקיים ששני הגבולות החלקיים $f_1(0^+)=f_1(0^-)=0$ הם אפס, כדי הנקודת האי רציפות הסליקה, נוכל להניח שהפונקציה רציפה בראשית ו $f_1(0)=0$ . בנוסף ממה שראינו למעלה: $$.\\int_{-\\infty}^\\infty f(x)\\frac{\\sin(Mx)}{\\pi x} \\dx = \\int_{-\\infty}^\\infty f_1(x)\\frac{\\sin(Mx)}{\\pi x} \\dx + \\frac{f(0^+)+f(0^-)}{2}$$ במילים אחרות, מספיק שנבין את האינטגרל לפונקציות מהצורה $f_1$ שרציפות בראשית ו $f(0)=0$ . בדוגמא למעלה “תיקנו” את הפונקציה הסגולה $f(x)$ ע”י ההזזה שלה עם הפונקציות המציינות הכתומות, ונשארנו עם הפונקציה השחורה $f_1(x)$ . שינוי רחוק מהראשית מה יקרה אם נשנה את הפונקציה שלנו רחוק מהראשית, נניח בתחום $[100,200]$ ? אם נסמן את הפונקציה החדשה ב $\\tilde{f}(x)$ כלומר $f(x)=\\tilde{f}(x)$ בכל $x\\notin [100,200]$ אז נקבל ש $$\\align{\\int_{-\\infty}^\\infty \\tilde{f}(x) \\frac{\\sin(Mx)}{\\pi x}\\dx & = \\int_{-\\infty}^\\infty f(x) \\frac{\\sin(Mx)}{\\pi x}\\dx + \\int_{100}^{200} (\\tilde{f}(x)-f(x)) \\frac{\\sin(Mx)}{\\pi x}\\dx \\\\ & = \\int_{-\\infty}^\\infty f(x) \\frac{\\sin(Mx)}{\\pi x}\\dx + \\int_{100}^{200} \\frac{\\tilde{f}(x)-f(x)}{\\pi x} \\sin(Mx)\\dx }$$ בתחום $[100,200]$ הפונקציה $g(x)=\\frac{\\tilde{f}(x)-f(x)}{\\pi x}$ היא תחילה הפרש של שתי פונקציה ב $E^1(\\RR)$ ולכן היא גם שם, וחילקנו ב $x$ , שלא יוצר בעיות כי אנחנו לא ליד האפס. ההבדל בין השני אינטגרלים על $f(x)$ ו $\\tilde{f}(x)$ הוא הגבול של האינטגרל $$,\\limfi{M}\\int_{100}^{200} g(x) \\sin(Mx) \\dx$$ אבל ראינו שאינטגרל כזה שואף לאפס, כי זו בדיוק הלמה של רימן לבג. אפילו לא היינו צריכים לעצור ב 200, ויכולנו לעשות אינטגרל עד אינסוף, ואותו הדבר בחלק השלילי. במילים אחרות, הגבול של האינטגרל כולו תלוי רק באיך ש $f$ נראית מאוד קרוב לראשית! אם נמשיך “לתקן” את הפונקציה כמו שעשינו בחלק הקודם, אז למשל כל שינוי (אינטגרבילי בהחלט) בפונקציה בתחומים $[1,\\infty)$ ו $(-\\infty,-1]$ לא ישנה את הגבול של האינטגרל, ולכן נוכל להניח שהפונקציה נתמכת ב $[-1,1]$ , (כלומר שווה לאפס מחוץ לקטע הזה), כלומר היא נראית כמו מה קורה בראשית? אזור הראשית הוא יותר בעייתי, כי אי אפשר פשוט לעשות את ה”מעבר”: $$ f(x)\\cdot \\frac{\\sin(Mx)}{\\pi x} = \\frac{f(x)}{\\pi x} \\cdot \\sin(Mx)$$ ולהשתמש ברימן לבג, כי באופן כללי הפונקציה $\\frac{f(x)}{x}$ היא כבר לא צריכה להיות עם אינטגרל סופי כי היא יכולה להתבדר בראשית. אבל, מאחר וקצת “תיקנו” את $f(x)$ כדי שתהיה רציפה בראשית ו $f(0)=0$ אז אולי בכל זאת יש סיכוי שזה יתכנס. יותר מכך, מה שנקבל הוא $$,\\frac{f(x)}{x}=\\frac{f(x)-f(0)}{x-0}$$ כלומר יהיה שם גבול, אם לפונקציה שלנו יש נגזרת! בצורה יותר ישירה, בלי התיקון ממוקדם, נוכל לכתוב שהאינטגרל על $[0,1]$ הוא $$\\align{\\int_0^1 f(x)\\cdot \\frac{\\sin(Mx)}{\\pi x} \\dx & = \\int_0^1 \\left(f(x)-f(0^+)+f(0^+)\\right)\\cdot \\frac{\\sin(Mx)}{\\pi x} \\dx \\\\ & \\int_0^1 \\left[\\frac{f(x)-f(0^+)}{x-0}\\cdot\\frac{\\sin(Mx)}{\\pi} +f(0^+)\\cdot \\frac{\\sin(Mx)}{\\pi x}\\right] \\dx} $$ כמו שכבר אמרנו, למחובר השני אנחנו ממש יכולים לחשב את האינטגרל. במחובר הראשון הפונקציה $\\frac{f(x)-f(0^+)}{x}$ תשאר אינטגרבילית בהחלט אם יש לה גבול בראשית, ואז שוב נוכל להשתמש בלמה של רימן לבג. אבל בעצם כבר ראינו את הביטויים האלו - להגיד שיש גבול בראשית בדיוק שקול לכך שיש נגזרת מתואמת מצד ימין, וזה הרכיב האחרון שנצטרך עבור ההוכחה הכללית. התמרת פורייה ההפוכה עכשיו אפשר לחבר את כל החלקים כדי לקבל את התוצאה הבאה: משפט: התמרת פורייה ההפוכה תהא $f\\in E^1(\\RR)$ . בכל נקודה $x_0\\in \\RR$ בה יש לפונקציה נגזרות מתואמות, מתקיים ש $$\\limfi{M} \\int_{-M}^M \\hat{f}(\\omega)e^{i\\omega x_0}\\dom = \\frac{f(x_0^-)+f(x_0^+)}{2}$$ הוכחה: כרגיל, כדאי להתחיל עם פישוט הבעיה, למשל נרצה לעבור מ $x_0$ כללי לראשית. לשם כך נגדיר את $g(x)=f(x_0+x)$ ואז נקבל ש $\\hat{g}(\\omega)=\\hat{f}(\\omega)e^{i\\omega x_0}$ בסימון החדש הזה מספיק להראות ש $$\\align{\\frac{g(0^-)+g(0^+)}{2}& \\overset{?}{=}\\limfi{M} \\int_{-M}^M \\hat{g}(\\omega)\\dom = \\frac{1}{2\\pi}\\limfi{M} \\int_{-\\infty}^{\\infty} g(x)\\int_{-M}^M e^{-i\\omega x} \\dom \\dx \\\\ & = \\limfi{M} \\int_{-\\infty}^{\\infty} g(x) \\frac{\\sin(Mx)}{\\pi x} \\dx}$$ נחשב את האינטגרל על החלק החיובי של ציר ה $x$ . נחלק אותו לאינטגרל בקטע $[0,1]$ ובקטע $[1,\\infty)$ וכמו מקודם ליד הראשית נכתוב $g(x)=g(x)-g(0^+)-g(0^+)$ ואז האינטגרל למעלה (ללא הגבול) יהיה: $$\\int_{0}^{\\infty} g(x) \\frac{\\sin(Mx)}{\\pi x} \\dx = \\int_{0}^{1} \\frac{g(x)-g(0^+)}{\\pi x} \\sin(Mx) \\dx+g(0^+)\\cdot \\int_{0}^{1} \\frac{\\sin(Mx)}{\\pi x} \\dx + \\int_{1}^{\\infty} \\frac{g(x)}{\\pi x} \\sin(Mx) \\dx$$ כפי שראינו, הנסכם השלישי שואף לאפס ע”י שימוש בלמה של רימן לבג, והנסכם השני שואף ל $\\frac{g(0^+)}{2}$ . הפונקציה $\\frac{g(x)-g(0^+)}{\\pi x}$ בנסכם הראשון נמצאת ב $E^1(\\RR)$ כי $g(x)\\in E^1(\\RR)$ ולא הוספנו בעיה בראשית בחלוקה ב $x$ בגלל הנגזרת המתואמת. גם שם נוכל להשתמש בלמה של רימן לבג ולהגיע למסקנה שהגבול כולו ביחד הוא $\\frac{g(0^+)}{2}$ . חישוב דומה נעשה בחלק השלילי של ציר האיקס ויחד איתו מקבלים את התוצאה שרצינו. מסקנה: התמרה כפולה היא (כמעט) הזהות אם $f,\\hat{f}\\in E^1(\\RR)$ ול $f$ יש נגזרות מתואמות ב $x_0$ אז מתקיים ש $$\\hat{\\hat{f}}(x_0) = \\frac{1}{2\\pi} \\frac{f(-x_0^+)+f(-x_0^-)}{2}$$ ובפרט אם $f$ רציפה ב $x_0$ אז מתקיים ש $$\\hat{\\hat{f}}(x_0) = \\frac{1}{2\\pi} f(-x_0)$$ הוכחה: נשים לב תחילה שתחת ההנחה ש $f,\\hat{f} \\in E^1(\\RR)$ הביטוי $\\hat{\\hat{f}}(x)$ מוגדר היטב. בפרט, בגלל ש $\\hat{f}\\in E^1(\\RR)$ הקצב שאיפה ל $\\pm \\infty$ באינטגרל לא משנה, ולכן $$\\align{\\hat{\\hat{f}}(x_0) & = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\hat{f}(\\omega) e^{-i\\omega x_0} \\dom = \\frac{1}{2\\pi}\\limfi{M}\\int_{-M}^M \\hat{f}(\\omega) e^{i\\omega (-x_0)} \\dom \\\\ &= \\frac{1}{2\\pi} \\frac{f(-x_0^+)+f(-x_0^-)}{2}}$$ נזכיר שהראנו שאם $f\\in E^1(\\RR)$ ולכן יש לה התמרת פורייה, אז $\\norm {\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi} \\norm f_1$ . עכשיו שיש לנו את ההתמרה ההפוכה בעצם גם נקבל ש מסקנה: אם $f,\\hat{f} \\in E^1(\\RR)$ ו $f$ רציפה עם נגזרות מתואמות, אז $f$ חסומה ומתקיים ש $$.\\norm f_\\infty \\leq \\norm {\\hat{f}}_1$$ דוגמאות דוגמא: הפונקציה $e^{-\\abs{x}}$ כבר ראינו ש $\\cf\\left[e^{\\abs{x}}\\right] = \\frac{1}{\\pi (\\omega^2+1)}$ . מאחר ושתי הפונקציות $e^{-\\abs{x}}, \\frac{1}{\\pi (\\omega^2+1)}\\in E^1(\\RR)$ , הן גזירות ברציפות בכל מקום פרט לראשית, אבל שם יש גזירות מתואמות, אז ממשפט ההתמרה ההפוכה נקבל ש $$.e^{-\\abs{x}}=\\int_{-\\infty}^\\infty \\frac{1}{\\pi (\\omega^2+1)} e^{i \\omega x} \\dom$$ שימו לב שביטלנו את החלוקה ב $2\\pi$ בשני האגפים, ולא צריך לקחת $-x$ באגף שמאל, כי הוא גם ככה מופיע בערך מוחלט. נוכל עכשיו להשתמש בזהות אוילר $e^{i \\omega x} = \\cos(\\omega x)+i\\sin(\\omega x)$ יחד עם העובדה שהפונקציה $\\frac{1}{\\pi (\\omega^2+1)}$ היא ממשית וזוגית, כדי להסיק ש: $$.e^{-\\abs{x}}=\\int_{-\\infty}^\\infty \\frac{\\cos(\\omega x)}{\\pi (\\omega^2+1)} \\dom$$ דוגמא: הפונקציה המציינת $\\chi_{[-a,a]}$ ראינו ש $\\cf[\\chi_{[-a,a]}](\\omega) = \\frac{\\sin(\\omega a)}{\\omega \\pi}$ . הפונקציה הזאת היא בעצם הפונקציה שהשתמשנו כדי להוכיח את קיום ההתמרה ההפוכה! כלומר, כאשר $a\\to \\infty$ ההתמרה של הפונקציה שואפת לפונקציית דלתא, בעוד שהפונקציה המקורית עצמה “שואפת” לפונקציה הקבועה $1$ . אם נחשוב עליה עכשיו בתור ההתמרה, ונבצע לה התמרה הפוכה, אז בגלל שזו פונקציה זוגית, אז כמו בדוגמא הקודמת נקבל ש: $$\\limfi{M}\\frac{1}{2\\pi}\\int_{-M}^M \\frac{\\sin(\\omega a)}{\\omega \\pi} \\cos(\\omega x) \\dom = \\frac{\\chi_{[-a,a]}(x^+)+\\chi_{[-a,a]}(x^-)}{2}$$ שוב נשתמש בכך שהפונקציה באינטגרל היא זוגית, כדי לקבל ש $$\\frac{1}{\\pi}\\int_{0}^\\infty \\frac{\\sin(\\omega a)}{\\omega \\pi} \\cos(\\omega x) \\dom = \\cases{1&\\abs{x}a}$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_plancharel": {
    "title": "נוסחת פלנשרל",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_plancharel",
    "body": "אם נחזור שוב לעולם של טורי פורייה, אז יכולנו להשתמש שם בשוויון פרסיבל ולקבל שעבור פונקציה $f:[-\\pi,\\pi]\\to\\CC$ רציפה למקוטעין עם מקדמי פורייה $a_n,b_n$ מתקיים ש $$.\\norm{f}_2^2=\\frac{\\abs{a_0}^2}{2}+ \\sum_1^\\infty \\left(\\abs{a_n}^2+\\abs{b_n}^2\\right)$$ כאשר המקדמים שלנו הם ההתמרה $\\hat{f}(\\omega)$ שמוגדרים לכל מספר ממשי, לכן היינו מצפים לקבל שוויון (עד כדי קבוע) מהצורה: $$.\\norm{f}_2^2=C\\int_{-\\infty}^\\infty \\abs{\\hat{f}(\\omega)}^2\\dom=C\\norm{\\hat{f}}_2^2$$ אבל, מייד יש לנו בעיות עם ההגדרה הזאת - עד עכשיו ההתמרה הוגדרה עבור פונקציות שעבורן $\\norm f_1 והן לא בהכרח מקיימות ש $\\norm {f}_2 וגם הכיוון ההפוך לא נכון. למזלנו יש הרבה פונקציות שעבורן שתי הנורמות האלו סופיות, ואז הטענה הזאת תהיה נכונה. משפט פלנשרל: יהיו $f,g\\in E^1(\\RR)\\cap E^2 (\\RR)$ אז: ההתמרות $\\hat{f}, \\hat{g}$ גם נמצאות ב $E^2(\\RR)$ , מתקיים ש: $$\\int_{-\\infty}^\\infty \\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)} \\dom = \\frac{1}{2\\pi} \\int _{-\\infty}^\\infty f(x)\\overline{g(x)} \\dx$$ או בכתיב של מכפלות פנימיות $2\\pi\\angles{\\hat{f}, \\hat{g}}=\\angles{f,g}$ , ובפרט מתקיים ש $\\norm{f}_2=\\norm{\\hat{f}}_2$ . הוכחה: הרעיון באופן כללי הוא החלפת סדרי אינטגרציה, ואם מותר לנו לעשות אותם אז “קל” להראות שהטענה נכונה. מבחינה פורמלית, נראה תחילה שכאשר ל $f,g$ יש תומכים סופיים, כלומר קיים $M>0$ מספיק גדול כך ש $f(x)=g(x)=0$ עבור $|x|>M$ , אז ההחלפות מותרות. באופן כללי, יש צורך להשתמש בקירובים של פונקציות כלליות ע”י צמצום שלהם לקטעים סופיים, כלומר $f_M(x):=f(x)\\cdot \\chi_{[-M,M]}$ אבל לא נכנס לפרטים האלו כאן. בנוסף, נתחיל גם עם ההנחה שגבולות אינטגרציה ששואפים ל $\\pm \\infty$ באותו קצב, ואז נראה איך נפתרים מזה. תחת ההנחות האלו נקבל ש $$\\align{\\int_{-N}^N \\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)} \\dom & = \\frac{1}{4\\pi^2} \\int_{-N}^N \\left(\\int_{-\\infty}^\\infty f(x)e^{-i \\omega x} \\dx \\overline{\\int_{-\\infty}^\\infty g(y)e^{-i \\omega y} \\dy} \\right)\\dom \\\\ & = \\frac{1}{4\\pi^2} \\int_{-N}^N \\left(\\int_{-M}^M f(x)e^{-i \\omega x} \\dx \\overline{\\int_{-M}^M g(y)e^{-i \\omega y} \\dy} \\right)\\dom \\\\ & = \\frac{1}{2\\pi} \\int_{-M}^M \\int_{-M}^M f(x)\\overline{g(y)} \\left( \\int_{-N}^N\\frac{e^{-i \\omega (x-y)}}{2\\pi} \\dom \\right)\\dx \\dy} $$ אם נשאיף את $N\\to\\infty$ אז נקבל את פונקצית דירק בנקודה $x-y$ באינטגרל הפנימי, לכן סה”כ האינטגרל למעלה יהיה שווה ל $$.(*)=\\frac{1}{2\\pi} \\int_{-M}^M f(x)\\overline{g(x)} \\dx=\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x)\\overline{g(x)} \\dx$$ אם נבחר $f=g$ אז נקבל ש $$.\\limfi{N}\\int_{-N}^N \\abs{\\hat{f}(\\omega)}^2 \\dom = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\abs{f(x)}^2 \\dx$$ מאחר ופה האינטגרלים הם על פונקציות אי שליליות, קצב השאיפה ל $\\pm \\infty$ בגבולות האינטגרציה לא משנה את ההתכנסות, ולכן נקבל ש $\\norm {\\hat{f}}_2^2= \\frac{1}{2\\pi}\\norm{f}_2^2 , וזה כבר מראה ש $\\hat{f}\\in E^2(\\RR)$ , וכמובן גם $\\hat{g}\\in E^2(\\RR)$ . אם נחזור עכשיו לחישוב הראשון למעלה, אז נוכל להשתמש באי שוויון קושי שוורץ (או בכך שהמכפלה הפנימית מוגדרת היטב על $E^2(\\RR)$ ), כדי לקבל ש $$.\\int_{-N}^N \\abs{\\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)}} \\dom =\\angles{|\\hat{f}|,|\\hat{g}|} \\leq \\norm{\\hat{f}}_2 \\norm{\\hat{g}}_2 = \\frac{1}{4\\pi^2}\\norm{f}_2 \\norm{g}_2 זה אומר שהאינטגרל מתכנס בהחלט, ולכן גם בלי הערך המוחלט הוא מתכנס, ללא קשר לקצב שאיפת הגבולות אינטגרציה ל $\\pm \\infty$ . זה מסיים את ההוכחה עבור פונקציות עם תומך סופי. דוגמאות דוגמא: הפונקציה $e^{-a|x|}$ עבור $a>0$ ראינו כבר שההתמרה של הפונקציה הזאת היא $\\cf[e^{-a|x|}](\\omega)=\\frac{a}{\\pi(\\omega^2+a^2)}$ . אם נחשב את הנורמה של הפונקציה נקבל ש $$.\\norm{e^{-a|x|}}_2^2 = \\int_{-\\infty}^\\infty e^{-2a|x|}\\dx=2 \\int_{0}^\\infty e^{-2ax}\\dx=\\frac{e^{-2ax}}{-a}\\mid_0^\\infty = \\frac{1}{a}$$ מצד שני, שימוש בפלנשרל נותן ש $$.\\int_{-\\infty}^\\infty \\left(\\frac{a}{\\pi(\\omega^2+a^2)}\\right)^2 \\dom = \\frac{1}{2\\pi} \\frac{1}{a}$$ אם נעביר אגפים ונשתמש בזוגיות של הפונקציה, נקבל ש $$.\\dboxed{\\int_{0}^\\infty \\frac{1}{(\\omega^2+a^2)^2} \\dom = \\frac{\\pi}{4a^3}} $$ דוגמא: פונקציות מדרגה סימטריות עבור $a>0$ נסתכל על הפונקציה $$.f_a(x)=\\chi_{[-a,a]}(x)=\\cases{1 & |x|\\leq a \\\\ 0 & |x| > a}$$ עבור הפונקציות האלו ראינו ש $\\hat{f}_a(\\omega)=\\frac{\\sin(\\omega a)}{\\omega \\pi}$ . שימוש בפלנשרל עבור המכפלה הפנימית יתן לנו ש: $$\\align{\\int_{-\\infty}^\\infty \\frac{\\sin(\\omega a)}{\\omega \\pi} \\cdot \\frac{\\sin(\\omega b)}{\\omega \\pi} \\dom &= \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f_a(x) f_b(x) \\dx = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f_{\\min(a,b)}(x) \\dx \\\\&= \\frac{\\min(a,b)}{\\pi}}$$ התמרות פורייה ב $E^2 (\\RR)$ אם נתונה פונקציה $f\\in E^2(\\RR)$ שלא ידוע מראש שהיא גם ב $E^1(\\RR)$ , אז באופן כללי אין סיבה שהאינטגרל בהתמרת פורייה יתכנס. למרות זאת, יש דרך להרחיב את ההגדרה של התמרת פורייה גם לפונקציות כאלו. התמרת פורייה ב $E^2(\\RR)$ תהא $f\\in E^2(\\RR)$ ועבור $M>0$ נסמן $$.f_M(x):=f(x)\\cdot \\chi_{[-M,M]}(x)=\\cases{f(x) & |x|\\leq M \\\\ 0 & |x|>M}$$ הפונקציות $f_M(x)$ נמצאות ב $E^1(\\RR) \\cap E^2(\\RR)$ ונגדיר את התמרת פורייה של $f$ להיות $$\\hat{f}(\\omega) = \\limfi{M} \\hat{f}_M(\\omega)= \\limfi{M} \\frac{1}{2\\pi} \\int_{-M}^M f(x)e^{-i \\omega x}\\dx$$ כאשר הגבול הוא בנורמת $\\norm{\\cdot}_2$ . הערה: דבר ראשון כדאי לשים לב שקל להראות ש $f_M$ אינטגרביליות בהחלט, כי $$\\norm{f_M}_1=\\int_{-\\infty}^\\infty \\abs{f_M} \\dx=\\int_{-M}^M \\abs{f}\\cdot 1 \\dx\\overset{(*)}{\\leq} \\sqrt{\\int_{-M}^M |f|^2(x)\\dx \\int_{-M}^M |1|^2(x)\\dx}\\leq \\norm{f}_2 \\sqrt{2M} $$ כאשר האי שוויון ב $(*)$ הוא אי שוויון קושי שוורץ. החלק היותר מסובך בהגדרה, הוא שאם $f$ הייתה מראש ב $E^1(\\RR)$ אז בעצם עכשיו יש לנו שתי הגדרות להתמרה שלה, וצריך להראות ששתיהן מחזירות את אותה פונקציה. הטענה הזאת נכונה, אבל לא נוכיח אותה פה. לבסוף, ההתמרה של פונקציה ב $E^2(\\RR)$ כמו שמוגדרת למעלה, מחזירה גבול של פונקציות אינטגרבילית בריבוע, שהוא בפרט פונקציה אינטגרבילית בריבוע שהן לא מוגדרות נקודתית. כלומר כאשר אנחנו אומרים ששתי פונקציות כאלו הן שוות, אז הן שוות כמעט לכל $x$ . תחת ההגדרה הזאת, מקבלים שהתמרת פורייה לוקחת פונקציות מהמרחב הפונקציות האינטגרביליות בריבוע לעצמו והתכונות שתיארנו עד עכשיו להתמרה עדיין נכונות, ובפרט ההתמרה ההפוכה ומשפט פלנשרל. דוגמא: הפונקציה $f(x)=\\frac{\\sin(x)}{x}$ הפונקציה $f(x)$ היא לא אינטגרבילית בהחלט אבל היא כן אינטגרבילית בריבוע כי $|f(x)|^2\\leq \\frac{1}{x^2}$ והאינטגרל של הפונקציות הזאת מתכנס באינסוף. לפי ההגדרה החדשה שלנו מתקיים ש $$.\\hat{f}(\\omega)=\\limfi{M} \\frac{1}{2\\pi} \\int_{-M}^M \\frac{\\sin(x)}{x}e^{-i \\omega x}\\dx$$ החישוב עצמו הוא די מסובך, אבל למזלנו כבר נתקלנו בפונקציה הזאת ואנחנו יודעים ש $\\cf[\\chi_{[-T,T]}](\\omega)=\\frac{\\sin(\\omega T)}{\\omega \\pi}$ , ולכן בפרט $\\pi\\cf[\\chi_{[-1,1]}](\\omega)=\\frac{\\sin(\\omega)}{\\omega}$ . אם נשתמש במשפט ההתמרה ההפוכה נקבל ש $$.\\cf\\left[\\frac{\\sin(\\omega)}{\\omega}\\right](x) = \\pi \\cf\\left[\\cf\\left[\\chi_{[-1,1]}\\right]\\right](x) = \\frac{1}{2} \\chi_{[-1,1]}(-x) = \\frac{1}{2} \\chi_{[-1,1]}(x)$$ ובאופן כללי יותר נקבל ש $$.\\cf\\left[\\frac{\\sin(\\omega T)}{\\omega}\\right](x) = \\frac{1}{2} \\chi_{[-T,T]}(x)$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_convolution": {
    "title": "קונבולוציה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_convolution",
    "body": "ראינו כבר הרבה תכונות מעניינות של התמרת פורייה, ובפרט ראינו שהיא לוקחת פעולות שאנו רגילים לעבור איתם כמו חיבור, כפל, סיבוב, הצמדה וכו’ פחות או יותר לעצמן. פעולה נוספת מאוד חשובה היא כפל של פונקציות, ונרצה להראות איך לבטא את ההתמרה $\\widehat{f\\cdot g}$ באמצעות ההתמרות $\\hat{f},\\hat{g}$ . פה מגיעה פעולת הקונבולוציה, אבל לפני שנגדיר אותה ונראה אותה מופיעה בהתמרות האלו, בואו נראה שבעצם אנחנו מכירים אותה כבר. מאיפה מגיעה הקונבולוציה אחת ממשפחות הפונקציות הכי פשוטה שאנחנו מכירים היא משפחת הפולינומים, כלומר פונקציות מהצורה: $$.f(x)=a_0+a_1 x + a_2 x^2 +\\cdots +a_d x^d = \\sum_0^d a_k x^k$$ גם פה במובן מסויים יש לנו “התמרה”. אם מתחילים עם הפונקציה $f$ אז המקביל למקדמי פורייה אלו פשוט המקדמים $a_k$ ו”ההתמרה ההפוכה” זו הקומבינציה שלהם שבאגף ימין בביטוי למעלה. $$\\align{f&\\overset{'\\cf'}{\\longrightarrow}(a_0,a_1,...,a_d)\\\\. \\sum_0^da_kx^k & \\overset{'\\cf^{-1}\\;'}{\\longleftarrow}(a_0,a_1,...,a_d)}$$ הערה: במקרה של פולינומים (מעל $\\CC$ ) קיבלנו ממש את השוויון $f(x)=\\sum_0^d a_kx^k$ לכל $x$ . כפי שכבר ראינו, במקרה של טורי והתמרות פורייה, צריך לעבוד קצת יותר בשביל השוויון הזה. אם נתון לנו פולינום נוסף $$,g(x)=b_0+b_1 x + b_2 x^2 +\\cdots +b_d x^d = \\sum_0^d b_k x^k$$ אז הפונקציה $f(x)g(x)$ היא גם פולינום: $$,f(x)g(x) = \\sum_0^{2d} c_kx^k$$ ולכן נוכל לשאול איך מבטאים את המקדמים שלה $c_k$ ע”י המקדמים $a_k,b_k$ של $f(x),g(x)$ . במקרה הזה נקבל ש : $$\\align{f(x)g(x) & = \\left(\\sum_{i=0}^d a_i x^i\\right)\\left(\\sum_{j=0}^d b_j x^j\\right) = \\sum_{i=0}^d\\sum_{j=0}^d a_i b_j \\cdot x^{i+j} = \\sum_{k=0}^{2d} \\left(\\sum_{i+j=k} a_i b_j\\right) \\cdot x^{k} = \\sum_{k=0}^{2d} \\left(\\sum_{i} a_i b_{k-i}\\right) \\cdot x^{k} \\\\ &= \\overbrace{a_0b_0}^{c_0}+\\overbrace{(a_0b_1+a_1b_0)}^{c_1}x+\\overbrace{(a_0b_2+a_1b_1+a_2b_0)}^{c_2}x^2+\\cdots +\\overbrace{(a_{d-1}b_d+a_db_{d-1})}^{c_{2d-1}}x^{2d-1}+\\overbrace{a_db_d}^{c_{2d}}x^{2d}}$$ כלומר, המקדם של $x^k$ הוא סכום של מכפלות של המקדמים של $f,g$ כך שהאינדקסים שלהם נסכמים ל $k$ . הסיבה למבנה המעניין הזה הוא בעצם חוקי החזקות שהמונומים מקיימים, כלומר $x^i x^j = x^{i+j}$ . כדי לקבל קצת יותר אינטואיציה ויזואלית לכפל הזה, ולקונבולוציה שעוד מעט נגדיר, נניח שהפולינומים הם מדרגה $d=2$ ורוצים למצוא את המקדם החופשי. נכתוב מקדמי הפולינומים בטבלה, כאשר המקדמים של $f$ גדלים באינדקס כאשר זזים ימינה, והמקדמים של $g$ קטנים באינדקס, ואם נשים אותה כך שהמקדם החופשי של $f$ יהיה מעל המקדם החופשי של $g$ אז רק צריך לכפול את התא בשורה הראשונה (של $f$ ) בתא מתחתיו (של $g$ ) לכתוב בתא בשורה השלישית ואז לסכום את השורה השלישית: $a_2x^2$ $a_1x^1$ $a_0 x^0$ 0 0 0 0 $b_0 x^0$ $b_1 x^1$ $b_2x^2$ 0 0 $a_0b_0x^0$ 0 0 אם נרצה את המקדם של $x$ , אז זה יהיה אותו הדבר, רק צריך להזיז את כל המקדמים בשורה הראשונה תא אחד שמאלה: $a_2x^2$ $a_1x^1$ $a_0 x^0$ 0 0 $b_0 x^0$ $b_1 x^1$ $b_2x^2$ 0 $a_1b_0x^1$ $a_0b_1x^1$ 0 ואם נרצה את המקדם של $x^2$ אז נזיז אותם עוד תא שמאלה וכך הלאה: 0 $a_2x^2$ $a_1x^1$ $a_0 x^0$ 0 0 $b_0 x^0$ $b_1 x^1$ $b_2x^2$ 0 0 $a_2b_0x^2$ $a_1b_1x^2$ $a_0b_2x^2$ 0 לכן, סה”כ קיבלנו את ההתאמה: $$\\align{f(x) & \\Rightarrow (a_0, a_1, ...)\\\\ g(x) & \\Rightarrow (b_0, b_1, ...) \\\\ f(x)\\cdot g(x) & \\Rightarrow (c_0,c_1,...),\\; c_k=\\sum_{i=0}^k a_i b_{k-i}}$$ מאחר וחוקי החזקות האלו מתקיים גם עבור האקספוננטים שלנו, נקבל משהו דומה, פרט לכך שבמקום מקדמים דיסקרטים, יש לנו מקדמים רציפים. קונבולוציה של פונקציות הגדרה: קונבולוציה: עבור שתי פונקציות $f,g:\\RR \\to \\CC$ נגדיר את הקונבולוציה שלהן $(f*g)$ להיות האינטגרל (כאשר הוא מוגדר): $$.(f*g)(x)=\\int_{-\\infty}^\\infty f(x-y)g(y) \\dy$$ נשים לב תחילה ש”סכום האינדקסים” בתוך הקונבולוציה הוא תמיד $(x-y)+y=x$ בדיוק כמו שבכפל פולינומים הסכום תמיד היה $(k-i)+i=k$ . ההגדרה של הקונבולוציה דומה להגדרה של מכפלה פנימית, רק במקום לכפול באינטגרנד ב $f(y)$ אנחנו עושים שיקוף $y\\mapsto-y$ ואז הזזה ב $x$ . בדומה למכפלה הפנימית, יש שני מקרים בהם הקונבולוציה מוגדרת לכל $x$ עם הוכחות דומה לאלו מהמכפלה הפנימית: אם אחת מהפונקציות היא חסומה והשנייה אינטגרבילית בהחלט (כלומר ב $E^\\infty(\\RR)$ ו $E^1(\\RR)$ בהתאם). אם שתי הפונקציות נמצאות ב $E^2(\\RR)$ . מקרה נוסף מעניין שדורש קצת יותר מאמץ הוא כאשר שתי הפונקציות הן ב $E^1(\\RR)$ אך לפני שנוכיח את זה, בואו נראה דוגמא בסיסית בשביל האינטואיציה. דוגמא: קונבולוציה של פונקציות מציינות נסתכל על שתי פונקציות מציינות של קטעים, הראשונה של $[-1,1]$ והשנייה של $[-h,h]$ : $$\\align{f(x)=\\chi_{[-1,1]}(x)=\\cases{1&|x|\\leq 1 \\\\ 0 & |x|>1} \\\\ .g(x)=\\chi_{[-h,h]}(x)=\\cases{1&|x|\\leq h \\\\ 0 & |x|>h}}$$ בחישוב של הקונבולוציה, אנחנו מכפילים בין שתי הפונקציות ומבצעים אינטגרציה. הפונקציה $g(y)$ שמופיעה באינטגרל “לא זזה” כאשר $x$ משתנה, בעוד שהפונקציה $f(x-y)$ כן. מאחר ושתיהן מתארות קטע, המכפלה שלהן מתארת את החיתוך, ואז האינטגרל מתאר את אורך הקטע. בתכנה למטה מדסמוס אפשר לראות באדום את הפונקציה $g$ בירוק את $f$ והגרף השחור הוא של הקונבולוציה. נסו לשחק עם הפרמטרים $x,h$ ולראות שאתם מבינים למה גרף הקונבולוציה נראה כמו שהוא נראה. תרגיל: חשבו את הקונבולוציה של שתי פונקציות מציינות $f(x)=\\chi_{[-a,a]}$ ו $g(x)=\\chi_{[-b,b]}$ כפונקציה של הפרמטרים $a,b\\geq 0$ . תכונות של קונבולוציה לפני שנתחיל עם תכונות, נזכיר את משפט פוביני טונלי שעוזר בחישוב אינטגרלים: משפט פוביני טונלי. תהא $H(x,y):\\RR^2\\to \\CC$ פונקציה מדידה, אז $$\\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty |H(x,y)| \\dx\\right)\\dy = \\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty |H(x,y)| \\dy\\right)\\dx = \\int_{\\RR^2} |H(x,y)| \\operatorname{d(x,y)}$$ בנוסף, אם האינטגרלים האלו סופיים אז $$\\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty H(x,y) \\dx\\right)\\dy = \\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty H(x,y) \\dy\\right)\\dx = \\int_{\\RR^2} H(x,y) \\operatorname{d(x,y)}$$ באמצעות משפט פוביני טונלי, אנחנו יכולים להראות סגירות לכפל קונבולוציה. משפט: קונבולוציה של פונקציות ב $E^1(\\RR)$ . יהיו $f,g\\in E^1(\\RR)$ . אז הקונבולוציה $(f*g)(x)$ מוגדרת כמעט לכל $x$ והיא אינטגרבילית בהחלט. הוכחה: אם נניח שהקונבולוציה מוגדרת לכל $x$ , ונרצה להראות שהיא אינטגרבילית בהחלט, אז בגלל ש $$\\int_{-\\infty}^\\infty \\abs{(f*g)(x)}\\dx = \\int_{-\\infty}^\\infty \\abs{\\int_{-\\infty}^\\infty f(x-y)g(y)\\dy}\\dx \\leq \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\abs{f(x-y)}\\abs{g(y)} \\dy \\dx$$ מספיק שנראה שהאינטגרל האחרון הוא חסום ולא אינסוף. מצד שני, אם האינטגרל האחרון הוא סופי, זה אומר שהאינטגרל הראשון הוא סופי ובפרט $(f*g)(x)$ מוגדר וסופי כמעט לכל $x$ . נשים לב שהאינטגרנד בצד ימין הוא על פונקציה אי שלילית ולכן ניתן להפעיל את פוביני ולקבל שהוא שווה ל $$.\\int_{-\\infty}^\\infty \\abs{g(y)}\\left(\\int_{-\\infty}^\\infty \\abs{f(x-y)} \\dx\\right) \\dy\\overset{t=x-y}{=}\\int_{-\\infty}^\\infty \\abs{g(y)}\\left(\\int_{-\\infty}^\\infty \\abs{f(t)} \\dt\\right) \\dy = \\norm{f}_1\\norm{g}_1 הערה: כדאי לשים לב שאינטגרל של פונקציה יכול להיות לא קיים לא רק בגלל אי התכנסות בגבולות $\\pm \\infty$ של האינטגרציה, אלא גם בגלל שהפונקציה עצמה לא אינטגרבילית. למשל, עבור אינטגרל רימן, פונקציית דיריכלה המוגדרת ע”י $D(x)=\\cases{1 & x\\in \\QQ \\\\ 0 & else}$ היא כלל לא אינטגרבילית (רימן) ללא קשר לגבולות האינטגרציה. דילגנו על החלק הזה בהוכחה, אבל באופן כללי זה גם משהו שצריך להראות. בנוסף, חשוב לשים לב שמה שמראים למעלה זה ש $\\abs{(f*g)(x)}$ מוגדרת וסופית כמעט לכל $x$ ולא לכל $x$ . למשל, נסתכל על הפונקציה $f(x)=\\frac{e^{-x^2/20}}{\\sqrt{|x|}}$ . זוהי פונקציה חיובית, רציפה פרט לראשית שהיא אינטגרבילית בהחלט: עבור $x$ -ים קטנים מתקיים ש $f(x)\\sim \\frac{1}{\\sqrt{|x|}}$ שהאינטגרל שלה מתכנס ליד הראשית, ועבור $x$ -ים גדולים מתקיים ש $f(x) שהאינטגרל שלה מתכנס באינסוף. המשפט למעלה מראה שהקונבולוציה $(f*f)(x)$ קיימת כמעט לכל $x$ , אבל אם נחשב את הקונבולוציה בראשית נקבל ש $$,(f*f)(0)=\\int_{-\\infty}^\\infty f(-y)f(y)\\dy=\\int_{-\\infty}^\\infty f(y)^2\\dy=\\int_{-\\infty}^\\infty \\frac{e^{-y^2/10}}{|y|}\\dy \\geq \\int_{-1}^1 \\frac{e^{-1/10}}{|y|}\\dy =\\infty$$ לעומת זאת, לכל $x\\neq 0$ הקבונולוציה $(f*f)(x)$ מוגדרת וסופית. ניתן לראות את זה בתכנת דסמוס למטה: בסגול רואים את הפונקציה $f(y)$ בשחור את $f(x_0-y)$ ובאדום את המכפלה שלהם שעליה רוצים לעשות אינטגרציה. שימו לב שניתן להזיז את הנקודה האדומה למטה וכך לשלוט על $x_0$ . כל עוד $x_0\\neq 0$ השטח יחסית קטן (התבדרות לאינסוף יחסית קטנה מ $\\frac{1}{\\sqrt{|x|}}$ ), ורק כאשר $x_0=0$ אז שני המקומות בהן $f(y),f(x_0-y)$ גדלים לאינסוף נופלים אחד על השני (התבדרות כמו $\\frac{1}{|x|}$ ), ואז גם השטח הוא אינסופי. בשלב הזה אנחנו יודעים שעבור פונקציות במרחב $E^1(\\RR)$ ניתן לחבר אותן ולכפול בסקלר (זה מרחב ווקטורי) ועכשיו גם אפשר לעשות בינהן קונבולוציה. כבר ראינו במקרה של פולינומים איך אפשר לכפל קונבולוציה מתוך מכפלה, ומסתבר שיש לה גם תכונות שאנחנו רגילים לשייך לכפל. תכונות אריתמטיות של קונבולוציה יהיו $f,g,h\\in E^1(\\RR)$ , ו $\\alpha \\in \\CC$ סקלר, אז: לינאריות (דיסטריביוטיביות + כפל בסקלר): $f*(\\alpha g + h) = \\alpha f*g + f*h$ . אסוציאטיביות: $f*(g*h)=(f*g)*h$ . קומוטטיביות: $f*g=g*f$ . הזזה: אם נסמן $g_\\alpha(x)=g(x+\\alpha)$ אז $(f*g_\\alpha)(x)=(f*g)(x+\\alpha)$ . נגזרת: אם $f$ גזירה ואם $f*g$ קיימת וגזירה, אז $(f*g)'(x)=(f'*g)(x)$ . התמרת פורייה של קונבולוציה משפט הקונבולוציה יהיו $f,g\\in E^1(\\RR)$ , אז $$.\\widehat{f*g}(\\omega) = 2\\pi\\hat {f}(\\omega) \\cdot \\hat{g}(\\omega)$$ הוכחה: נשים לב תחילה שבגלל ש $f,g\\in E^1(\\RR)$ אז אנחנו יודעים ש $f*g$ אינטגרבילית בהחלט ולכן יש לה התמרת פורייה, ששווה ל: $$.\\widehat{f*g}(\\omega)= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} (f*g)(x)e^{-i\\omega x}\\dx= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x-y)g(y)e^{-i\\omega x}\\dy\\dx$$ כבר ראינו שהאינטגרל הזה מתכנס בהחלט ולכן מותר לנו להחליף סדר אינטגרציה ולקבל את $$.\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} g(y) \\int_{-\\infty}^{\\infty} f(x-y)e^{-i\\omega x}\\dx\\dy\\overset{x=t+y}{=}\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} g(y)e^{-i\\omega y} \\int_{-\\infty}^{\\infty} f(t)e^{-i\\omega t}\\dt\\dy=2\\pi \\hat{f}(\\omega) \\hat{g}(\\omega)$$ דוגמא: הקונבולוציה של $e^{-x^2}*e^{-x^2}$ . אם נסמן $f(x)=e^{-x^2}$ ונרצה לחשב ישירות את הקונבולוציה, אז נקבל את האינטגרל: $$\\align{(f*f)(x) & =\\int_{-\\infty}^\\infty e^{-(x-y)^2}e^{-y^2}\\dy = \\int_{-\\infty}^\\infty e^{-2((x/2)^2 + (y-x/2)^2)}\\dy \\\\ & = e^{-x^2/2} \\int_{-\\infty}^\\infty e^{-2(y-x/2)^2}\\dy \\overset{y=t+x/2}{=} e^{-x^2/2} \\int_{-\\infty}^\\infty e^{-2t^2}\\dt = \\sqrt{\\frac{\\pi}{2}}e^{-x^2/2}}$$ דרך שנייה לחשב את האינטגרל זה לעבור דרך התמרת פורייה, כי אז הקונבולוציה תהפוך לכפל פשוט פונקציות, ולאחר מכן ניתן לעשות התמרה הפוכה כדי לחזור לפונקציה שרצינו לחשב, או במילים אחרות $$.(f*f)(x)=2\\pi\\cf\\left[\\cf[f*f]\\right](-x)$$ מבחינה פורמלית, נסמן ב $f(x)=e^{-x^2}$ נזכיר שראינו ש $\\cf[f](\\omega)=\\frac{1}{2\\sqrt{\\pi}} e^{-\\frac{\\omega^2}{4}}$ . שימוש במשפט הקונבולוציה יתן לנו ש $$.\\cf[f*f](\\omega)=2\\pi\\cf[f]^2(\\omega)=\\frac{1}{2} e^{-\\frac{\\omega^2}{2}}=\\frac{1}{2}f(\\frac{\\omega}{\\sqrt{2}})$$ נחשב התמרה נוספת, תוך שימוש בתכונת ההתמרה של כפל המשתנה בסקלר ונקבל ש $$.2\\pi\\cf[\\cf[f*f]](-x)=\\pi \\cf\\left[f(\\frac{\\omega}{\\sqrt{2}})\\right](-x)=\\sqrt{2}\\pi \\cf\\left[f\\right](-\\sqrt{2}x)=\\sqrt{\\frac{\\pi}{2}} e^{-\\frac{x^2}{2}}$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_bounded_frequency": {
    "title": "חסימות בתדר",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_bounded_frequency",
    "body": "איך חוסמים את התדירות של פונקציה כאשר נתונה לנו פונקציה על הישר $f:\\RR \\to \\RR$ , הרבה פעמים יש לה יחסית “מעט מסה” בקצוות של הישר. למשל, אם האינטגרל $\\int_{-\\infty} ^\\infty |f(x)|\\dx הוא סופי, אז לכל $\\varepsilon>0$ נוכל למצוא $M_\\varepsilon$ כך שהאינטגרל מחוץ ל $[-M_\\varepsilon,M_\\varepsilon]$ הוא קטן, כלומר $\\int_{|x| . במקרים כאלו הרבה פעמים נסתכל על הפונקציה $f(x)\\chi_{[-M_\\varepsilon, M_\\varepsilon]}(x)$ שמכילה את “רב האינפורמציה” מהפונקציה המקורית ויש לה תומך סופי (היא שווה לאפס מחוץ לקטע הסופי $[-M_\\varepsilon,M_\\varepsilon]$ .) כאשר אנחנו עובדים עם התמרות פורייה, אז הלמה של רימן לבג מבטיחה לנו ש $\\hat{f}(\\omega)\\to 0$ כש $\\abs{\\omega} \\to \\infty$ , כלומר במובן מסוים כדי להבין את $\\hat{f}$ לא צריך להתרחק יותר מדיי מהראשית (לעיתים חושבים על זה כאילו ה”אנרגיה” של הפונקציה נמצאת בתדרים נמוכים). האם נוכל לעשות תהליך דומה למה שתואר למעלה, ולנקות לחלוטין את התדרים הגובים, או בצורה יותר פורמלית, האם עבור $M>0$ קיימת פונקציה $f_M$ כך ש $$? \\;\\;\\;\\widehat{f_M}(\\omega)=\\cases{\\hat{f}(\\omega) & |\\omega|\\leq M \\\\ 0 & |\\omega|>M}$$ נשים לב, שהדרישות על $\\widehat{f_M}$ בעצם אומרות שההתמרה שלה צריכה להיות $\\widehat{f_M}=\\hat{f}\\cdot \\chi_{[-M,M]}$ ופה נוכל להעזר במשפט הקונבולוציה. נזכיר ש $\\cf[\\chi_{[-M,M]}](\\omega)=\\frac{\\sin(\\omega M)}{\\omega \\pi}$ , ואם נשתמש בהתמרה ההפוכה (עבור פונקציות ב $E^2$ ) נקבל ש $\\cf[\\frac{2\\sin(xM)}{x}] = \\chi_{[-M,M]}$ ולכן $$\\cf\\left[ f*\\frac{\\sin(xM)}{\\pi x}\\right](\\omega) = \\cf\\left[ f\\right](\\omega) \\cdot \\cf\\left[ \\frac{2\\sin(xM)}{x}\\right]= \\cf\\left[ f\\right](\\omega) \\cdot \\chi_{[-M,M]}(\\omega)$$ למה פונקציות שחסומות בתדר מעניינות? אחת הסיבות שנרצה לעבוד עם פונקציות שחסומות בתדר היא המשפט הבא: משפט הדגימה של נייקוויסט-שנון (Nyquist-Shannon Sampling Theorem) תהא $f\\in E^1(\\RR)$ ו $\\hat{f}(\\omega)=0$ לכל $|\\omega|\\geq L$ אז כמעט לכל $x$ מתקיים: $$.f(x)=\\sum_{-\\infty}^\\infty f\\left(\\frac{n\\pi}{L}\\right)\\frac{\\sin(Lx-n\\pi)}{Lx-n\\pi}$$ לפני שנוכיח את המשפט, מה בעצם הוא אומר? המשפט בעצם שואל האם אפשר לעשות אינטרפולציה לפונקציה $f$ , כלומר אם אנחנו יודעים את הערך של $f$ בנקודות $f(\\frac{n\\pi}{L})$ , האם נוכל למצוא את הערכים $f(x)$ לכל $x\\in \\RR$ . באופן כללי זה כמובן לא נכון, למשל אם אנחנו דוגמים גל בנקודות השחורות למטה, אז אנחנו לא יודעים אם הגל היה עם תדירות נמוכה (הגל האדום) או תדירות גבוהה יותר (הכחול), או אפילו עוד יותר גבוהה: משפט הדגימה שאם אנחנו יודעים שהתדירות לא יכולה להיות גדולה מדיי (כלומר $f(\\omega)=0$ עבור $|\\omega|$ מספיק גדול) והדגימה היא על נקודות במרווחים מתאימים, אז אפשר ממש לשחזר את הפונקציה המקורית. המרווח בין הדגימות בדוגמא למעלה הוא לא אחיד, אבל אפשר גם להצטמצם לדגימות ב $x=\\frac{\\pi n}{4}$ . הערה: אינטרפולצית פולינומים: אנחנו בעצם מכירים מקרה דומה עם אינטרפולציה של פולינומים. אם רוצים לשחזר פולינום מדרגה $d$ אז צריך לדעת את הערכים שלו ב $d+1$ נקודות שונות. אם יודעים פחות נקודות, אז פתאום יש יותר מפולינום אחד שיתאים לדגימה. לא נכנס לזה פה, אבל אם כל המטרה שלנו היא רק לשחזר את הגל המקורי, אז עדיף דווקא לקחת דגימות רנדומיות, אבל עדיין יש מקום חשוב למשפט עצמו, כי למשל במקרים רבים הדגימות הן במרווחים קבועים בצורה “טבעית”, במיוחד כאשר מדובר על דגימות דיגיטליות, ואז לרב קוראים לבעיה הזאת Aliasing, והפתרון שלה הן שיטות Anti-Aliasing. למשל אשלייה שאפשר לראות הרבה פעמים כאשר מסתכלים על גלגלים מסתובבים זה שאם הם מסתובבים במהירויות מסויימות אז פתאום נראה כאילו המהירות שונה לחלוטין, ולפעמים הם אפילו מסתובבים בכיוון ההפוך: דוגמא: Aliasing in animation מה שקורה זה שבעצם אנחנו רואים פריימים מתוך האנימציה, והמוח שלנו מנסה להשלים אותם לפונקציה רציפה, ובדרך כלל עושה את זה באמצעות אינטרפולציה לתדירות הנמוכה יותר. דוגמא נוספת יותר סטטית היא למשל כאשר אנחנו מנסים להציג תמונה עם משבצות. אם הרזולוזציה לא מספיק גבוהה יחסית לגודל משבצות אז נראה את התופעה הבאה: דוגמא: Aliasing in checkered tiles נניח שאנחנו רוצים להציג משבצות שחורות-לבנות, אבל הן מאוד קטנות (מתאים לפונקציה עם תדר מאוד גבוה) והרזולוציה שלנו לא מספיק גדולה (אין מספיק דגימות) . למשל נניח שכל פיקסל שמיוצג בתמונה למטה ע”י מסגרת כחולה מכסה כמה משבצות: כאשר נרצה לצבוע את הפיקסל אז יש כמה אפשרויות לעשות את זה. אפשר למשל לעשות סוג של ממוצע, אבל דרך אחרת מאוד פשוטה זה להסתכל על הצבע שאמור להיות במרכז הפיקסל ולהשתמש בו עבור כל הפיקסל. הבעיה עם הדרך הזאת היא שלמשל למעלה כל הפיקסלים יצבעו בשחור. כדי לראות את הבעיה הזאת בפעולה, בתכנה למטה יש תמונת משבצות שחורות-לבנות בגודל 50 על 50, אבל, ניתן לשלוט על ה”רזולוציה” ע”י הסליידר בצד שמאל למעלה: הוכחת משפט הדגימה הוכחה למשפט הדגימה: נרמול הפונקציה: כדי לפשט קצת את הסימונים, ולא לעבוד עם $L$ כללית, נסתכל על הפונקציה $g(x):=f(\\frac{\\pi}{L}x)$ . היתרון בפונקציה הזאת היא שההתמרה שלה היא $\\hat{g}(\\omega)=\\frac{L}{\\pi}\\hat{f}(\\frac{L}{\\pi}\\omega)$ ולכן הנתון מתרגם ל $\\hat{g}(\\omega)=0$ לכל $|\\omega|\\geq \\pi$ וצריך להראות ש $$.g(\\frac{L}{\\pi}x) = \\sum_{-\\infty}^\\infty g\\left(n\\right)\\frac{\\sin(Lx-n\\pi)}{Lx-n\\pi}$$ מאחר וזה נכון לכל $x$ , זה שקול להראות שזה נכון לכל $y=\\frac{L}{\\pi}x$ ולכן צריך להראות ש $$.g(y) = \\sum_{-\\infty}^\\infty g\\left(n\\right)\\frac{\\sin(\\pi(y-n))}{\\pi(y-n)}$$ מעבר לפונקציות מחזוריות: הפונקציה $\\hat{g}(\\omega)$ היא אפס מחוץ לקטע $[-\\pi,\\pi]$ ולכן נוכל להגדיר פונקציה חדשה $\\hat{g}_P(\\omega)$ שהיא השלמה $2\\pi$ מחזורית של $\\hat {g}(\\omega)$ , או בצורה פורמלית: $$.\\hat{g}_P(\\omega + 2\\pi k) := \\hat{g}(\\omega) \\;\\; ; \\;\\; \\forall w\\in [-\\pi,\\pi],\\;k\\in \\ZZ$$ אם נרצה לחזור אחורה, מהפונקציה המחזורית להתמרה עצמה, אז $$.\\hat{g}(\\omega)=\\hat{g}_P(\\omega)\\chi_{[-\\pi, \\pi]}(\\omega)$$ טור פורייה: נשים לב שהתמרת פורייה היא תמיד רציפה, ולכן $\\hat{g}_P$ היא פונקציה רציפה ומחזורית, ולכן קיים לה טור פורייה שמתכנס אליה בנורמת $\\norm{\\cdot}_2$ ונקודתית. בסימונים של אקספוננטים נקבל ש: $$\\hat{g}_P(\\omega) = \\sum_{-\\infty}^\\infty c_n\\cdot e^{in\\omega},\\; \\; c_n=\\angles{\\hat{g}_P,e^{in\\omega}} = \\frac{1}{2\\pi}\\int_{-\\pi}^\\pi \\hat{g}_P(\\omega)e^{-in\\omega}\\dom$$ התמרה הפוכה: בתחום $[-\\pi, \\pi]$ אנחנו יודעים ש $\\hat{g}_P(\\omega)=\\hat{g}(\\omega)$ ולכן נוכל להחליף את הפונקציה באינטגרנד, וגם נוכל להגדיל את גבולות האינטגרציה, אבל אז מה שנקבל זה פחות או יותר ההתמרה ההפוכה: $$.c_n=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\hat{g}(\\omega)e^{-in\\omega}\\dom=\\frac{1}{2\\pi}g(-n)$$ סה”כ קיבלנו ש $$.\\hat{g}(\\omega) = \\hat{g}_P(\\omega)\\chi_{[-\\pi, \\pi]}(\\omega) = \\frac{1}{2\\pi}\\sum_{-\\infty}^\\infty g(-n)\\cdot e^{in\\omega}\\chi_{[-\\pi, \\pi]}(\\omega) = \\sum_{-\\infty}^\\infty \\cf\\left[g(-n)\\cdot\\frac{\\sin((x+n)\\pi)}{(x+n)\\pi}\\right](\\omega)$$ ההתכנסות עצמה היא בנורמת $\\norm{\\cdot}_2$ והתמרת פורייה היא רציפה יחסית לנורמה הזאת (לא הוכחנו את זה, אך זה נכון) ולכן אפשר להחליף סדר סכימה והתמרה. לבסוף, בגלל היחידות של ההתמרה נקבל את השוויון (כמעט לכל $x$ ): $$.g(x) =\\frac{1}{2\\pi}\\sum_{-\\infty}^\\infty g(-n)\\cdot\\frac{\\sin((x+n)\\pi)}{(x+n)\\pi} = \\sum_{-\\infty}^\\infty g(n)\\cdot\\frac{\\sin((x-n)\\pi)}{(x-n)\\pi}$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_heat_equation": {
    "title": "משוואת החום",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_heat_equation",
    "body": "משוואת החום היא משוואה דיפרנציאלית חלקית המתארת איך הטמפרטורה מתנהגת יחסית למקום וזמן. אם נסמן את פונקציית הטמפרטורה ב $u(x,t)$ אז משוואת החום היא: $$u'_t(x,t)=k\\cdot u''_{xx}(x,t)$$ כאשר $k$ הוא קבוע התלוי בחומר איתו עובדים. נסיון לפתור את משוואת החום זה אחת הסיבות שפורייה פיתח את ההתמרות שאנחנו לומדים. אינטואיציה כדי לקבל קצת אינטואיציה על הקשר בין הנגזרת לפי הזמן ולפי המקום, בואו נסתכל על הפונקציה $f(x)=\\frac{x^2(x-2)}{4}+1$ ועל שתי הנגזרות שלה: נניח שהפונקציה הזאת מתארת את החום על מוט בזמן $t=0$ ונסתכל על שלוש נקודות $x=-1,0,1$ ונשאל איך החום שם צריך להשתנות. בנקודה $x=-1$ המוט יותר חם מהאזורים לידו, כלומר זה נקודת מקסימום מקומי, ולכן נצפה שהחום יתפזר ל”שכנים”. מבחינה פורמלית, הנגזרת תהיה שווה לאפס, והנגזרת השנייה היא שלילית. נשים לב שבאופן כללי, ככל שההבדל בין הטמפרטורה בנקודה עצמה לנקודות ה”שכנות” היא יותר גדולה, נצפה שיהיה יותר איבוד של חום, ומבחינה מתמטית הנגזרת השנייה תהיה יותר שלילית. בנקודה $x=1$ קורה המצב ההפוך בו יש מינימום מקומי, ולכן נצפה שהמוט יתחמם שם מהשכנים. גם פה הנגזרת היא אפס והנגזרת השנייה היא חיובית. לבסוף, בנקודה $x=0$ , הפונקציה נראית כמעט לינארית, כלומר הנגזרת השנייה היא כמעט אפס. לכן מצד אחד של הנקודה (שמאל) יותר חם, ומצד שני יותר קר, ופחות או יותר באותו סדר גודל, לכן לא נצפה ליותר מדיי שינוי בטמפרטורה שם. האינטואיציה מפה היא שבעצם הנגזרת השנייה מודדת עד כמה ההבדל בחום בין נקודה מסויימת לשכנים שלה, ומאחר ואנחנו מצפים שהשינוי בחום לאורך הזמן תלוי בהבדל הזה, נצפה לראות שהנגזרת $u_t'$ לפי הזמן תהיה תלויה ב $u''_{xx}$ . ננסה טיפה לפרמל את זה יותר (ולפורמליזם הכללי, צריך ללכת למד”ח). נניח שאנחנו רוצים למצוא את השינוי בטמפרטורה $u(x)$ בנקודה $x$ כאשר הטמפרטורה בנקודות השכנות שלה $x\\pm h$ עבור איזשהו $h>0$ היא קבועה. אז אם ניתן לחום להתפזר מספיק זמן, בסופו של דבר נצפה שהטמפרטורה ב $x$ תהיה הממוצע $\\frac{u(x+h)+u(x-h)}{2}$ , או לחלופין השינוי יהיה: $$.\\frac{u(x+h)+u(x-h)}{2}-u(x)=\\frac{1}{2}\\left((u(x+h)-u(x))+(u(x-h)-u(x))\\right)$$ זה כמובן ה”כיוון” שבו הטמפרטורה רוצה לזוז בה, ועוד יש המהירות בה זה קורה (נזכיר שאנחנו רוצים שינוי בזמן נתון ולא ההבדל מזמן $t=0$ עד $t=\\infty$ ). למשל, ככל ש $h$ קטן יותר, והנקודות השכנות קרובות יותר, ככה נצפה שהשינוי יהיה מהיר יותר. דרך אחת ליצור את המהירות הזאת, זה לחלק ב $h^2$ ואז הביטוי למעלה יהיה: $$.\\frac{1}{2h}\\left(\\frac{u(x+h)-u(x)}{h}-\\frac{u(x-h)-u(x)}{-h}\\right)$$ “כמעט” רשום למעלה הביטוי $\\frac{u'(x+h)-u'(x-h)}{2h}$ שהגבול שלו הוא $u''_{xx}(x)$ , וטיפה יותר פורמלית, אם מניחים ש $u$ גזירה ברציפות פעמיים, אז שימוש בלופיטל יתן $$.\\lim_{h\\to 0}\\frac{u(x+h)+u(x-h)-2u(x)}{2h^2}=\\lim_{h\\to 0}\\frac{u'(x+h)-u'(x-h)}{4h}=\\frac{u''_{xx}(x)}{2}$$ פתרון משוואת החום נחזור לנסיון של פורייה לפתור את משוואת החום: $$\\dboxed{u'_t(x,t)=k\\cdot u''_{xx}(x,t)}$$ הבעייה העיקרית במשוואות דיפרנציאליות חלקיות, זה שהנגזרות הן לפי משתנים שונים. למזלנו, התמרה של נגזרות הופכת לכפל ב $i\\omega$ ואז אולי נוכל לקבל חזרה פתרון ע”י התמרה הפוכה. אם נבצע התמרה לפי $x$ ונסמן $U(\\omega,t)=\\cf[u(x,t)](\\omega)$ , אז $$.\\cf[k\\cdot u''_{xx}](\\omega)=k\\cdot i\\omega \\cf[u'_x](\\omega)=-k\\cdot \\omega^2\\cf[u](\\omega)=-k\\cdot \\omega^2U(\\omega,t)$$ תחת ההנחה שמותר לנו לשנות סדר גזירה (לפי $t$ ) ואינטגרציה, נקבל שההתמרה של אגף שמאל תהיה $$,\\cf[u'_t]=U'_t(\\omega,t)$$ וזה משאיר אותנו עם המשוואה הדיפרנציאלית הרגילה הפשוטה: $$U'_t(\\omega,t)=-k\\omega^2\\cdot U(\\omega, t)$$ כאשר הפתרון שלה הוא $$.U(\\omega,t)=U(\\omega,0)e^{-k\\omega^2t}$$ נרצה לעשות עכשיו את ההתמרה ההפוכה, וממשפט הקונבולוציה, מכפלה של פונקציות הופכת לקונבולוציה (וההפך). נזכיר ש $\\cf[e^{-x^2}]=\\frac{1}{2\\sqrt{\\pi}}e^{-\\omega^2/4}$ ולכן $$.e^{-k\\omega^2t}=e^{-(\\omega\\cdot 2\\sqrt{kt})^2/4} =\\frac{2\\sqrt{\\pi}}{2\\sqrt{kt}}\\cf[e^{-(x/2\\sqrt{kt})^2}](\\omega) =\\sqrt{\\frac{\\pi}{kt}}\\cf[e^{-x^2/4kt}](\\omega)$$ סה”כ קיבלנו ש $$\\cf[u](\\omega,t) = \\cf[u](\\omega,0)\\cdot \\cf\\left[\\sqrt{\\frac{\\pi}{kt}}e^{-x^2/4kt}\\right](\\omega)=\\frac{1}{2\\pi} \\cf\\left[u(x,0)*\\sqrt{\\frac{\\pi}{kt}}e^{-x^2/4kt}\\right](\\omega)$$ כלומר, אם אנחנו יודעים את הטמפרטורה בזמן $t=0$ אז נוכל לחשב אותה בזמן $t$ כללי ע”י: $$.\\dboxed{u(x,t)=\\frac{1}{\\sqrt{4\\pi kt}}\\int_{-\\infty}^\\infty u(y,0)e^{-\\frac{(x-y)^2}{4kt}}\\dy}$$ סימולציה של משוואת החום לחצו על המסך כדי ליצור נקודות חום. לחצו על הכפתור הפעלה בצד שמאל למטה (שקצת מוחבא) כדי להתחיל את הסימולציה."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_Fourier_transform_summerization": {
    "title": "התמרת פורייה - סיכום",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_Fourier_transform_summerization",
    "body": "התמרת פורייה על $E^1(\\RR)$ (רשימות) הגדרה: התמרת פורייה עבור פונקציה $f:\\RR\\to \\CC$ ו $\\omega\\in \\RR$ נכתוב $$.\\hat f(\\omega) := \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx = \\angles{f,e^{i\\omega x}}$$ אם הגבול הזה קיים לכל $\\omega \\in \\RR$ נקרא לפונקציה $\\hat{f}:\\RR\\to\\CC$ בשם ההתמרת פורייה של $f$ . לעיתים נכתוב גם $\\cf[f]=\\hat{f}$ . למה: אם $f\\in\\ E^1(\\RR)$ , אז $\\hat{f}(\\omega)$ מוגדרת היטב לכל $\\omega$ ומתקיים ש $\\norm{\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi} \\norm{f}_1$ . משפט: עבור $f\\in E^1(\\RR)$ ההתמרה מקיימת: הפונקציה $\\hat{f}$ היא רציפה. (הלמה של רימן לבג) $\\limfi{\\abs{\\omega}} \\hat{f}(\\omega) = 0$ . דוגמאות חשובות $\\hat{f}(\\omega)$ $f(x)$ $\\hat{f}(\\omega)=\\cases{\\frac{\\sin(\\omega b)}{\\omega \\pi} & \\omega \\neq 0 \\\\ \\frac{b}{\\pi} & \\omega = 0.}$ $\\chi_{[-a,a]}(x)$ $\\frac{1}{\\pi(\\omega^2+1)}$ $e^{-\\abs{x}}$ $\\frac{1}{1+i\\omega}$ $\\chi_{[0,\\infty)}(x) \\cdot e^{-\\abs{x}}$ $\\frac{1}{2\\sqrt{\\pi}} e^{-\\omega^2/4}$ $e^{-x^2}$ פעולות גאומטריות (רשימות)   $\\hat{f}(\\omega)$ $f(x)$ לינאריות: $\\alpha \\cdot \\hat{f}(\\omega)+\\beta \\cdot \\hat{g}(\\omega)$ $\\alpha \\cdot f(x) + \\beta \\cdot g(x)$ מתיחה: $\\frac{\\hat{f}(\\omega/\\lambda)}{|\\lambda|}$ $f(\\lambda x)$ הצמדה: $\\overline{\\hat{f}(-\\omega)}$ $\\overline{f(x)}$ הזזה לסיבוב: $e^{i\\alpha \\omega}\\hat{f}(\\omega)$ $f(x+\\alpha)$ סיבוב להזזה: $\\hat{f}(\\omega-c)$ $e^{icx}f(x)$ סינוס: $\\frac{\\cf[f](\\omega-c)-\\cf[f](\\omega+c)}{2i}$ $f(x)\\sin(cx)$ קוסינוס: $\\frac{\\cf[f](\\omega-c)+\\cf[f](\\omega+c)}{2}$ $f(x)\\cos(cx)$ נגזרות   $\\hat{f}(\\omega)$ $f(x)$ עבור $f(x),f'(x)\\in E^1(\\RR)$ $i\\omega \\hat{f}(\\omega)$ $f'(x)$ עבור $f(x),x\\cdot f(x)\\in E^1(\\RR)$ $\\hat{f}'(\\omega)$ $-ix\\cdot f(x)$ ההתמרה ההפוכה (רשימות) משפט: התמרת פורייה ההפוכה תהא $f\\in E^1(\\RR)$ . בכל נקודה $x_0\\in \\RR$ בה יש לפונקציה נגזרות מתואמות, מתקיים ש $$\\limfi{M} \\int_{-M}^M \\hat{f}(\\omega)e^{i\\omega x_0}\\dom = \\frac{f(x_0^-)+f(x_0^+)}{2}$$ מסקנה: התמרה כפולה היא (כמעט) הזהות אם $f,\\hat{f}\\in E^1(\\RR)$ ול $f$ יש נגזרות מתואמות ב $x_0$ אז מתקיים ש $$\\hat{\\hat{f}}(x_0) = \\frac{1}{2\\pi} \\frac{f(-x_0^+)+f(-x_0^-)}{2}$$ ובפרט אם $f$ רציפה ב $x_0$ אז מתקיים ש $$\\hat{\\hat{f}}(x_0) = \\frac{1}{2\\pi} f(-x_0)$$ מסקנה: אם $f,\\hat{f} \\in E^1(\\RR)$ ו $f$ רציפה עם נגזרות מתואמות, אז $f$ חסומה ומתקיים ש $$.\\norm f_\\infty \\leq \\norm {\\hat{f}}_1$$ פלנשרל (רשימות) התמרת פורייה ב $E^2(\\RR)$ תהא $f\\in E^2(\\RR)$ ועבור $M>0$ נסמן $$.f_M(x):=f(x)\\cdot \\chi_{[-M,M]}(x)=\\cases{f(x) & |x|\\leq M \\\\ 0 & |x|>M}$$ הפונקציות $f_M(x)$ נמצאות ב $E^1(\\RR) \\cap E^2(\\RR)$ ונגדיר את התמרת פורייה של $f$ להיות $$\\hat{f}(\\omega) = \\limfi{M} \\hat{f}_M(\\omega)= \\limfi{M} \\frac{1}{2\\pi} \\int_{-M}^M f(x)e^{-i \\omega x}\\dx$$ כאשר הגבול הוא בנורמת $\\norm{\\cdot}_2$ . משפט פלנשרל: יהיו $f,g\\in E^1(\\RR)\\cap E^2 (\\RR)$ אז: ההתמרות $\\hat{f}, \\hat{g}$ גם נמצאות ב $E^2(\\RR)$ , מתקיים ש: $$\\int_{-\\infty}^\\infty \\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)} \\dom = \\frac{1}{2\\pi} \\int _{-\\infty}^\\infty f(x)\\overline{g(x)} \\dx$$ או בכתיב של מכפלות פנימיות $2\\pi\\angles{\\hat{f}, \\hat{g}}=\\angles{f,g}$ , ובפרט מתקיים ש $\\norm{f}_2=\\norm{\\hat{f}}_2$ . קונבולוציה (hebrew_convolution) הגדרה: קונבולוציה: עבור שתי פונקציות $f,g:\\RR \\to \\CC$ נגדיר את הקונבולוציה שלהן $(f*g)$ להיות האינטגרל (כאשר הוא מוגדר): $$.(f*g)(x)=\\int_{-\\infty}^\\infty f(x-y)g(y) \\dy$$ טענה: הקונבולוציה מוגדרת היטב אם: אחת מהפונקציות אינטגרבילית בהחלט והשנייה חסומה (כלומר בה”כ $\\norm{f}_1, \\norm{g}_\\infty ). שני הפונקציות אינטגרביליות בריבוע, כלומר $\\norm{f}_2, \\norm{g}_2 . משפט: קונבולוציה של פונקציות ב $E^1(\\RR)$ . יהיו $f,g\\in E^1(\\RR)$ . אז הקונבולוציה $(f*g)(x)$ מוגדרת כמעט לכל $x$ והיא אינטגרבילית בהחלט. תכונות אריתמטיות של קונבולוציה יהיו $f,g,h\\in E^1(\\RR)$ , ו $\\alpha \\in \\CC$ סקלר, אז: לינאריות (דיסטריביוטיביות + כפל בסקלר): $f*(\\alpha g + h) = \\alpha f*g + f*h$ . אסוציאטיביות: $f*(g*h)=(f*g)*h$ . קומוטטיביות: $f*g=g*f$ . הזזה: אם נסמן $g_\\alpha(x)=g(x+\\alpha)$ אז $(f*g_\\alpha)(x)=(f*g)(x+\\alpha)$ . נגזרת: אם $f$ גזירה ואם $f*g$ קיימת וגזירה, אז $(f*g)'(x)=(f'*g)(x)$ . משפט הקונבולוציה יהיו $f,g\\in E^1(\\RR)$ , אז $$.\\widehat{f*g}(\\omega) = 2\\pi\\hat {f}(\\omega) \\cdot \\hat{g}(\\omega)$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_laplace": {
    "title": "התמרת לפלס",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_laplace",
    "body": "עד עכשיו ראינו כבר הרבה תכונות ושימושים להתמרת פורייה, ובפרט ראינו איך אפשר להפוך באמצעות ההתמרות האלו משוואות דיפרנציאליות למשוואות אלגבריות. אבל, אחת הדרישות החשובות שכל הזמן היינו צריכים היא שהפונקציות שלנו הן אינטגרביליות בהחלט או בריבוע. זו יכולה להיות בעיה כן יש המון פונקציות שאנחנו עובדים איתן שלא נמצאות שם, למשל כמו פולינומים, או אפילו פונקציות האקספוננט (או בעולם הממשי - הסינוס והקוסינוס). למשל, אם היינו רוצים לפתור את המשוואה הדיפרנציאלית $y(x)=y'(x)$ אז הפתרון שלה הוא $ce^x$ עבור קבוע $c$ כלשהו. זו כמובן לא פונקציה אינטגרבילית בהחלט (אלא אם כן $c=0$ ). אם בכל זאת היינו מנסים להפעיל את התמרת פורייה, אז היינו מקבלים ש $\\cf[y](\\omega)=i\\omega\\cf[y](\\omega)$ ולכן $(1-i\\omega) \\cf[y](\\omega)=0$ . הפתרון היחיד של המשוואה הזאת הוא $\\cf[y](\\omega)\\equiv 0$ ואז מההתמרה ההפוכה נקבל ש $y(x)\\equiv 0$ . כדי שנוכל לעבוד גם עם פונקציות כאלו, נגדיר את התמרת לפלס, שכפי שנראה היא פחות או יותר התמרת פורייה אבל על הציר המדומה, ולכן יהיו לה הרבה תכונות דומות למה שראינו בהתמרת פורייה. הגדרה ודוגמאות הגדרה: התמרת לפלס תהא $f:[0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין. נגדיר את התמרת לפלס $\\cl[f]$ להיות הפונקציה: $$\\cl[f](s)=\\int_0^\\infty f(t)e^{-st}\\dt$$ עבור כל $s\\in \\RR$ שם האינטגרל מתכנס. אפשר וכדאי לחשוב על התמרת לפלס כמעין התמרת פורייה על הישר המדומה. בצורה יותר פורמלית, אם נרחיב את $f$ לכל $\\RR$ ע”י כך שנגדיר אותה להיות $f(x)=0$ על $x , אז נקבל ש $$.\\cl[f](s)=\\int_0^\\infty f(t)e^{-st}\\dt = \\int_{-\\infty}^\\infty f(t)e^{-st}\\dt = 2\\pi \\cf[f](-is)$$ דוגמא: הפונקציה $f(t)=e^{zt}$ עבור $z\\in \\CC$ . נזכיר תחילה שפונקציות האקספוננט הן לחלוטין לא אינטגרביליות בהחלט או בריבוע, ולכן אי אפשר לבצע עליהן התמרת פורייה רגילה. לעומת זאת, התמרת לפלס עובדת בסדר גמור. נתחיל עם המקרה שבו $z=0$ כלומר $f(t)\\equiv 1$ . במקרה הזה נקבל ש $$.\\cl[f](s) = \\int_0^\\infty e^{-st}\\dt = \\frac{e^{-st}}{-s}\\mid_{t=0}^\\infty \\; = \\cases{\\frac{1}{s} & s>0 \\\\ \\infty & s\\leq 0} $$ אם נשנה את $z$ בכיוון הציר הממשי, כלומר $z=x\\in \\RR$ נקבל בצורה דומה $$.\\cl[f](s) = \\int_0^\\infty e^{(x-s)t}\\dt = \\frac{e^{(x-s)t}}{(x-s)}\\mid_{t=0}^\\infty \\; = \\cases{\\frac{1}{s-x} & s>x \\\\ \\infty & s\\leq x} $$ לעומת זאת, אם נשנה בכיוון הציר המדומה, כלומר $z=iy$ עבור $y\\in \\RR$ אז נקבל $$.\\cl[f](s) = \\int_0^\\infty e^{(iy-s)t}\\dt = \\frac{e^{-st}e^{iyt}}{(iy-s)}\\mid_{t=0}^\\infty \\; = \\cases{\\frac{1}{s-iy} & s>0 \\\\ \\infty/\\text{undefind} & s\\leq 0} $$ נשים לב שכאשר $t\\to \\infty$ הביטוי $e^{iyt}$ הוא חסום (על מעגל היחידה) ו $e^{-st}\\to 0$ עבור $s>0$ ולכן יש התכנסות. עבור $s\\leq 0$ לעומת זאת האינטגרל מתבדר. עבור $z=x+iy$ כללי, צריך לשלב את שני המקרים ולקבל $$.\\cl[f](s) = \\int_0^\\infty e^{(z-s)t}\\dt = \\frac{e^{(x-s)t}e^{iyt}}{(z-s)}\\mid_{t=0}^\\infty \\; = \\cases{\\frac{1}{s-z} & s>x \\\\ \\infty/\\text{undefind} & s\\leq x} $$ דוגמא: פונקציות סינוס $f(t)=\\sin(at)$ נזכר שאפשר לכתוב $\\sin(at)=\\frac{e^{iat}-e^{-iat}}{2i}$ , ובגלל שההתמרה היא לינארית (כי אינטגרל זו פונקציה לינארית) נקבל ש $$\\cl[\\sin(at)](s) = \\frac{1}{2i}\\left(\\cl[e^{iat}]-\\cl[e^{-iat}]\\right)(s) =\\frac{1}{2i}\\left(\\frac{1}{s-ia}-\\frac{1}{s+ia}\\right) =\\frac{a}{s^2+a^2}$$ והאינטגרל הזה מתכנס עבור $s>0$ עבור $s\\leq 0$ נקבל $\\infty - \\infty$ שהוא לא מוגדר ולכן חייבים לבדוק לפי ההגדרה האם בכל זאת יש התכנסות. בדקו שאין באמת התכנסות במקרה הזה, כלומר התמרת לפלס תקפה רק עבור $s>0$ . דרך נוספת לעשות את החישוב למעלה זה לשים לב שהשתמשנו בכך ש $\\sin(at)=Im(e^{iat})$ , ובגלל שבהתמרת לפלס באינטגנד אנחנו כופלים בפונקציה הממשית $e^{-st}$ , אז נקבל ש $$.\\cl[\\sin(at)]=\\cl\\left[Im(e^{iat})\\right]=Im\\left(\\cl[e^{iat}]\\right)= Im\\left(\\frac{1}{s-ia}\\right) = Im\\left(\\frac{s+ia}{s^2+a^2}\\right)=\\frac{a}{s^2+a^2}$$ בצורה דומה גם מחשבים את ההתמרה של קוסינוס להיות $\\cl[\\cos(at)] = \\frac{s}{s^2+a^2}$ . תכונות בסיסיות בדוגמאות שראינו למעלה, ההתמרה תמיד הייתה קיימת בתחום מהצורה $(s_0, \\infty)$ . מסתבר שבהרבה מהפונקציות שאנחנו עובדים איתן יהיה $s_0$ כזה: משפט תהא $f:\\RR \\to \\CC$ רציפה למקוטעין. אם קיים $s_0\\in \\RR$ ו $0 כך ש $|f(t)|\\leq Ke^{s_0t}$ לכל $t\\geq 0$ , או בצורה שקולה הפונקציה $\\frac{|f(t)|}{e^{s_0t}}$ חסומה ב $t\\geq 0$ , אז $\\cl[f](s)$ מוגדר לכל $s>s_0$ ומתקיים ש $$.\\limfi{s} \\cl[f](s)=0$$ הוכחה: נראה שהאינטגרל מתכנס בהחלט עבור $s>s_0$ ואפילו האינטגרל בהחלט שואף לאפס כאשר $s\\to \\infty$ . עבור $s>s_0$ מתקיים ש : $$\\int_0^\\infty |f(t)e^{-st}|\\dt\\leq \\int_0^\\infty K e^{(s_0-s)t}\\dt = K\\frac{e^{(s_0-s)t}}{s_0-s}\\mid_0^\\infty=\\frac{K}{s-s_0}\\overset{s\\to \\infty}{\\longrightarrow} 0$$ החסימות של $\\frac{|f(t)|}{e^{s_0t}}$ ב $t\\geq 0$ זו תכונה שנכונה עבור הרבה מאוד פונקציות ובפרט אם $f(t)$ רציפה, אז זה שקול לכך ש $\\displaystyle{\\limsup_{t\\to \\infty}} \\frac{|f(t)|}{e^{s_0t}}$ הוא סופי. למשל, עבור פולינום $f(t)$ הגבול הזה קיים ושווה לאפס לכל $s_0>0$ , ולכן התמרת לפלס קיימת ב $(s_0,\\infty)$ . התכונות הבאות, הן המקבילות לתכונות של התמרת פורייה. טענה: תכונות גאומטריות של התמרת לפלס בכל התכונות למטה, הפונקציות הן רציפות למקוטעין עם התמרת פורייה שמוגדרת בנקודה $s$ . לינאריות: $\\cl[\\alpha\\cdot f + g](s) = \\alpha \\cl[f](s)+\\cl[g](s)$ . מתיחה: עבור $a>0$ מתקיים ש $\\cl[f(at)](s) = \\frac{1}{a} \\cl[f]\\left(\\frac{s}{a}\\right)$ . כפל באקספוננט $\\Leftarrow$ הזזה: $\\cl[e^{at}f(t)](s) = \\cl[f](s-a)$ . הערה: הכיוון ההפוך של הזזה $\\Leftarrow$ כפל באקספוננט אין לנו את הכיוון ההפוך בגלל שהאינטגרל שלנו מוגדר בקטע $[0,\\infty)$ והזזה תשנה את גבולות האינטגרציה. נסו לחשוב איך להכליל את התמרת לפלס כדי שכן יהיה את הכיוון הזה. הוכחה: כל ההוכחות נובעות כמעט מיידית מתכונות האינטגרל ומההגדרה של ההתמרה. נובע מלינאריות של האינטגרל: $$.\\cl[\\alpha\\cdot f + g](s) = \\int_0^\\infty (\\alpha\\cdot f + g)(t)e^{-st}\\dt = \\alpha \\int_0^\\infty f(t)e^{-st}\\dt + \\int_0^\\infty g(t)e^{-st}\\dt =\\alpha \\cl[f](s)+\\cl[g](s)$$ נובע מהחלפת משתנים $x=at$ ונשים לב שבגלל ש $a>$ זה לא משנה את גבולות האינטגרציה. $$.\\cl[f(at)](s)=\\int_0^\\infty f(at)e^{-st}\\dt=\\int_0^\\infty f(x) e^{-sx/a}\\frac{1}{a}\\dx = \\frac{1}{a} \\cl[f](\\frac{s}{a})$$ נובע מההגדרה: $$.\\cl[e^{at}f(t)](s)= \\int_0^\\infty e^{at}f(t)e^{-st}\\dt = \\int_0^\\infty f(t)e^{-(s-a)t}\\dt = \\cl[f](s-a)$$ התמרת לפלס ונגזרות כמו בהתמרת פורייה, גם בהתמרת לפלס נוכל להפוך נגזרות לכפל במשתנה, וכך להפוך משוואות דיפרנציאליות למשוואות אלגבריות. טענה: כפל ב $t$ מותמר לנגזרת (והפוך) תהא $f:[0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין כך ש $\\frac{|f(t)|}{e^{s_0t}}$ חסומה. אם $f$ רציפה ובנוסף היא גזירה ברציפות למקוטעין, אז לכל $s>s_0$ מתקיים ש $$.\\cl[f'](s)=s\\cl[f](s)-f(0)$$ התמרת לפלס $\\cl[f]$ היא גזירה ולכל $s>s_0$ מתקיים ש $$ .\\cl[tf(t)](s)=-\\frac{d}{ds} \\cl[f](s)$$ הוכחה: בחישוב של $\\cl[f'](s)=\\int_0^\\infty f'(t)e^{-st}\\dt$ מתבקש לעשות אינטגרציה בחלקים, אבל צריך להיזהר קצת. אינטגרציה בחלקים נובעת מכלל הגזירה $$ ,\\frac{\\partial}{\\partial t}(f(t)e^{-st}) = f'(t)e^{-st} -sf(t)e^{-st} $$ רק שבגלל ש $f$ גזירה למקוטעין, אז השוויון הזה נכון למקוטעין, כלומר פרט אולי לסדרה דיסקרטית $0\\leq t_1 . למזלנו, כאשר מבצעים אינטגרציה, האינטגרל לא משתנה כאשר משנים פונציה על סדרה כזאת (למשל אם משנים את הפונקציה במספר סופי של נקודות) ולכן נקבל ש $$. \\int_0^L \\frac{\\partial}{\\partial t}(f(t)e^{-st})\\dt = \\int_0^L f'(t)e^{-st}\\dt - s \\int_0^L f(t)e^{-st}\\dt \\to \\cl[f'](s)-s\\cl[f](s) $$ נשים לב שאם נראה שאגף שמאל מתכנס, אז בגלל שאנחנו יודעים שהאינטגרל ב $\\cl[f](s)$ מתכנס, אז נקבל במתנה שגם $\\cl[f'](s)$ מתכנס. כדי לחשב את אגף שמאל נרצה להשתמש במשפט היסודי של החדו”א עבור הפונקציה $g(t)=f(t)e^{-st}$ אבל לשם כך צריך ש $g$ תהיה פונקציה גזירה ואנחנו רק יודעים שהיא גזירה למקוטעין. מסתבר שהמשפט הזה עדיין נכון אם $g$ גזירה למקוטעין כל עוד יודעים שבנוסף היא רציפה. למשל, אם בתחום $(0,L)$ נקודות האי גזירות של הפונקציה הן $0=t_0 אז נקבל ש $$.\\int_0^L g'(t)\\dt = \\sum_0^n \\int_{t_i}^{t_{i+1}}g'(t)\\dt = \\sum_0^n (g(t_{i+1})-g(t_i))=g(t_{n+1})-g(t_0)=g(L)-g(0)$$ במילים אחרות, הרציפות של $g$ אפשרה לנו לעשות סכום טלסקופי וכמובן ניתן עכשיו גם להשאיף את $L\\to \\infty$ . אם נחזור לאינטגרל המקורי, ונשתמש בעובדה ש $\\frac{|f(t)|}{e^{s_0t}}$ חסומה ו $s>s_0$ נקבל ש $$,\\cl[f'](s)-s\\cl[f](s) = f(t)e^{-st}\\mid_{t=0}^\\infty = \\limfi{s} \\frac {f(t)}{e^{s_0t}} e^{(s_0-s)t} - f(0)=-f(0)$$ וזה מסיים את ההוכחה. בשביל הכיוון השני, אם היה מותר לנו להחליף סדר אינטגרציה ונגזרת, אז בקלות נקבל את התוצאה: $$.\\frac{\\partial}{\\partial s}\\cl[f](s) = \\int_0^\\infty \\frac{\\partial}{\\partial s}( f(t)e^{-st})\\dt = -\\int_0^\\infty tf(t)e^{-st}\\dt = -\\cl[tf(t)](s)$$ אם נרצה להוכיח בצורה פורמלית, אז נעשה את התהליך הרגיל: נחליף את הנגזרת בהגדרה שלה עם הגבול, נעביר אגפים למעלה כדי להראות שהגבול הוא אפס, נוסיף ערך מוחלט, ועכשיו רק צריך לחסום מלמעלה, נבצע אי שוויון משולש לאינטגרל: $$.\\limsup_{h\\to 0}\\abs{\\frac{\\cl[f](s+h)-\\cl[f](s)}{h} + \\cl[tf(t)](s)} \\leq \\limsup_{h\\to0}\\int_0^\\infty \\abs{\\frac{f(t)e^{-(s+h)t}-f(t)e^{-st}}{h} + tf(t)e^{-st}}\\dt=(*)$$ עכשיו עושים קצת סידור אלגברי של המשוואה ומקבלים שהאינטגרנד הוא $$.\\abs{f(t)e^{-st}\\left[\\frac{e^{-ht}-1}{h}+t\\right]} = \\abs{tf(t)e^{-st}\\left[\\frac{e^{-ht}-(1-ht)}{ht}\\right]}$$ נרצה להראות שלא רק הביטוי הזה מאוד קטן כאשר $h\\to 0$ , אלא גם האינטגרל עליו שואף לאפס. נתחיל עם הגורם השני, ונשים לב שאם נציב $u=-ht$ אז רשום שם: $$.\\abs{\\frac{1}{u}(e^u-(1+u))} = \\abs{\\sum_2^\\infty \\frac{u^{n-1}}{n!}} = \\abs{u\\sum_0^\\infty \\frac{u^n}{(n+2)!}}\\leq \\abs{u}e^\\abs{u} = \\abs{ht}e^{\\abs{ht}}$$ אם נחזור חזרה לאינטגרל נקבל שהוא קטן שווה ל $$,(*)\\leq\\limsup_{h\\to 0} \\abs{h}\\int_0^\\infty \\abs{t^2 f(t)e^{(|h|-s)t}}\\dt$$ ועכשיו בעצם אם נראה שהאינטגרל החדש הזה מתכנס, אז כאשר כופלים ב $|h|$ כל הביטוי ישאף לאפס וסיימנו. אבל זה ינבע מכך שהאקספוננט שואף לאפס הרבה יותר מהר מפולינומים, כי עבור $s>s_0$ נקבל ש $$\\frac{\\abs{t^2 f(t)}}{e^{(s-|h|)t}} = \\overbrace{\\frac{\\abs{f(t)}}{e^{s_0t}}}^\\text{bounded} \\cdot \\overbrace{\\frac{t^2}{e^{(s-s_0)\\cdot t/2}}}^\\text{bounded} \\cdot \\frac{1}{e^{(s-s_0-2|h|)\\cdot t/2}} \\leq K \\frac{1}{e^{(s-s_0-2|h|)\\cdot t/2}} $$ סה”כ נקבל שאם $|h|$ מספיק קטן, אז $s-s_0-2|h|>0$ ולכן $$.|h|\\int_0^\\infty \\abs{t^2 f(t)e^{(|h|-s)t}}\\dt\\leq |h|K \\int_0^\\infty \\frac{1}{e^{(s-s_0-2|h|)\\cdot t/2}}\\dt = \\frac{2K|h|}{s-s_0-2|h|}\\to 0 $$ מסקנה: נגזרות מסדר גבוה תהא $f:[0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין כך ש $\\frac{|f(t)|}{e^{s_0t}}$ חסומה. אם הנגזרות $f^{(k)}(t)$ קיימות רציפות וחסומות ע”י $Ke^{s_0t}$ עבור $k\\leq n-1$ ובנוסף $f^{(n)}$ רציפה למקוטעין, אז לכל $s>s_0$ מתקיים ש $$.\\cl[f^{(n)}](s)=s^n\\cl[f](s)-\\sum_{k=0}^{n-1} s^{n-1-k}f^{(k)}(0)$$ התמרת לפלס גזירה אינסוף פעמים ב $s>s_0$ ומתקיים ש $$.\\cl[t^nf(t)](s)=(-1)^n\\frac{d^n}{\\ds^n}\\cl[f](s)$$ הוכחה: נובע מהטענה הקודמת יחד עם אינדוקציה. טענה: חלוקה ב $t$ מותמרת לאינטגרל (והפוך) תהא $f:(0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין כך ש $\\frac{|f(t)|}{t e^{s_0t}}$ חסומה. אז לכל $s>s_0$ מתקיים ש $$.\\cl\\left[\\frac{1}{t}f(t)\\right](s)=\\int_s^\\infty \\cl[f](u)\\du$$ תהא $f:[0,\\infty)\\to \\CC$ כך ש $\\frac{|f(t)|}{e^{s_0t}}$ חסומה. אם $g(t):=\\int_0^t f(x)\\dx$ אז $$.\\cl[g](s)=\\frac{1}{s}\\cl[f](s)$$ הוכחה: שימוש בטענה הקודמת עבור כפל ב $t$ תיתן לנו ש $$.\\cl[f](s)=\\cl\\left[t\\cdot \\frac{1}{t}f(t)\\right](s)=-\\frac{d}{\\ds}\\cl\\left[\\frac{1}{t}f(t)\\right](s)$$ זה נכון לכל $s>s_0$ ואם נעשה אינטגרציה בתחום $[a,b]$ עבור $s נקבל ש $$.\\int_a^b \\cl[f](s)\\ds =\\cl\\left[\\frac{1}{t}f(t)\\right](a) - \\cl\\left[\\frac{1}{t}f(t)\\right](b)$$ נשאיף את $b\\to \\infty$ ונשתמש בכך ש $\\limfi{b}\\cl\\left[\\frac{1}{t}f(t)\\right](b)=0$ כדי להסיק ש $$.\\int_a^\\infty \\cl[f](s)\\ds =\\cl\\left[\\frac{1}{t}f(t)\\right](a)$$ שוב נשתמש בטענה הקודמת, והפעם בכך ש $g'(s)=f(s)$ , ולכן $$.\\cl[f](s)=\\cl[g'(t)](s)=s\\cl[g](s)-g(0)=s\\cl[g](s)$$ דוגמאות דוגמא: התמרת לפלס של $t^n$ שימוש בתכונה של $\\cl[t^nf(t)](s)=(-1)^n\\frac{d^n}{\\ds^n}\\cl[f](s)$ יחד עם ההתמרה של $\\cl[1](s)=\\frac{1}{s}$ בתחום $s>0$ תיתן לנו בתחום הזה: $$.\\cl[t^n](s)=(-1)^n\\frac{d^n}{\\ds^n}\\left(\\frac{1}{s}\\right)=\\frac{n!}{s^{n+1}}$$ דוגמא: חשבו את $\\int_0^\\infty \\frac{t\\cos(t)}{e^t}\\dt$ . כפי שהתרגיל כתוב, אנחנו בעצם רוצים לחשב את $\\cl[t\\cos(t)](1)$ . לשם כך שוב נשתמש בכך שכפל ב $t$ הופך לנגזרת ולכן $$\\align{\\cl[t\\cot(t)](s) &= -\\cl[\\cos(t)]'(s) = -\\frac{1}{2}\\cl[e^{it}+e^{-it}]'(s) \\\\&= -\\frac{1}{2}\\left(\\frac{1}{s-i}+\\frac{1}{s+i}\\right)' = -\\left(\\frac{s}{s^2+1}\\right)' = \\frac{1-s^2}{(s^2+1)^2}}$$ החישוב הזה מוגדר היטב בתחום $s>0$ ובפרט עבור $s=1$ נקבל ש $\\int_0^\\infty \\frac{t\\cos(t)}{e^t}\\dt=\\cl[t\\cos(t)](1)=0$ . מצאו $f(t)$ כך ש $\\cl[f](s)=\\ln(1+\\frac{1}{s})$ . עוד לא ראינו פונקציה שההתמרה שלה מכילה את הלוגריתם, אבל נשים לב שאם נגזור את התנאי הנתון, אז נקבל ש $$.-\\cl[tf(t)](s)=\\cl[f]'(s)=\\frac{1}{1+\\frac{1}{s}}\\frac{-1}{s^2}=\\frac{-1}{s^2+s}= \\frac{1}{s+1}-\\frac{1}{s}=\\cl[e^{-t}-1](s)$$ לכן, הניחוש יהיה $f(t)=\\frac{1-e^{-t}}{t}$ ."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_laplace_differential_equations": {
    "title": "משוואות דיפרנציאליות",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_laplace_differential_equations",
    "body": "כמו עם התמרת פורייה, גם בהתמרת לפלס ניתן להשתמש כדי לפתור משוואות דיפרנציאליות. היתרון בהתמרת לפלס הוא שאין לנו את הדרישה שהפתרון יהיה אינטגרבילי בהחלט (או בריבוע) אבל מצד שני הפתרון צריך להיות בתחום $[0,\\infty)$ (או באופן כללי קטע חצי אינסופי). הרעיון הוא שההתמרה הופכת נגזרות למכפלות במשתנה של הפונקציה, ולכן משוואות דיפרנציאליות יהפכו למשוואות אלגבריות. נתחיל עם דוגמא של משוואות דיפרנציאליות לינאריות. באופן כללי ניתן לפתור משוואות כאלו עם כלים בסיסיים (למשל, אלגברה לינארית), אבל נראה שניתן גם להשתמש בהתמרת פורייה. דוגמא: משוואה לינארית הומוגנית נחפש פתרון למשוואה הבאה: $$\\align{0 &= y''(t)-7y'(t)+12y(t) \\\\ .y(0)&=3,\\;\\;y'(0)=10}$$ נניח שמותר לבצע התמרת לפלס בנקודה $s$ . יכול להיות שלא מותר ואז אולי נקבל פתרון שגוי, אך בכל מקרה בסוף התהליך נוכל לבדוק האם הפתרון שקיבלנו מקיים את המשוואה. חישוב ההתמרה על שני האגפים יתן ש $$\\align{0 & = \\cl[0] = \\cl\\left[y''-7y'+12y]\\right]=\\left(s^2\\cl[y]-sy(0)-y'(0)\\right)-7\\left(\\cl[y]-y(0)\\right)+12\\cl[y] \\\\ & =(s^2-7y+12)\\cl[y]-(s-7)y(0)-y'(0)=(s^2-7y+12)\\cl[y]-3s+11}$$ נעביר אגפים ונקבל ש: $$.\\cl[y]=\\frac{3s-11}{s^2-7y+12}=\\frac{3s-11}{(s-3)(s-4)} = \\frac{1}{s-4} + \\frac{2}{s-3}$$ כבר ראינו ש $\\cl[e^{zt}](s) = \\frac{1}{s-z}$ ולכן סה”כ קיבלנו ש $$.\\cl[y(t)]=\\cl[e^{4t}+2e^{3t}]$$ כמובן ש $y(t)=e^{4t}+2e^{3t}$ פותר את המשוואה האחרונה (ואם וכאשר נראה את קיום ההתמרה ההפוכה $\\cl^{-1}$ אז זה גם בעצם יהיה הפתרון היחיד). עכשיו ניתן לחזור למשוואה הדיפרנציאלית המקורית ולראות שזה אכן פתרון שלה (ובנוסף $\\frac{|y(t)|}{e^{4t}}$ חסומה ולכן גם כל המהלכים בדרך נכונים). הערה: הפולינום האופייני של מד\"ר עבור מד”ר מהצורה $\\sum_0^d a_k y^{(k)}(t)=0$ מגדירים את הפולינום האופייני להיות $\\sum_0^d a_kx^k$ . כמו שרואים בפתרון למעלה, כאשר התמרנו את המד”ר הופיע לנו בצורה טבעית הפולינום האופייני שלה, והמקדמים $3,4$ שהופיעו בחזקות של האקספוננטים הם בדיוק השורשים של הפולינום. מה קורה אם התנאי התחלה לא נתונים ב $t=0$ ? נניח שלמשל נתונים התנאי התחלה ב $y(1),y(3)$ במקום ב $y(0),y'(0)$ . במקרה הזה, נעשה את אותו חישוב, אבל נשאיר את $y(0),y'(0)$ כפרמטרים שאפשר לבחור, ואז נקבל את המשוואה: $$\\cl[y](s)=\\frac{(7-s)y(0)-y'(0)}{(s-3)(s-4)} =\\overbrace{(y'(0)-4y(0))}^A\\frac{1}{s-3} + \\overbrace{(3y(0)-y'(0))}^B\\frac{1}{s-4} =\\cl\\left[Ae^{3t}+Be^{4t}\\right](s)$$ לכן, סה”כ נוכל לנחש שהפתרון ללא תנאי התחלה הוא $$.y(t)=Ae^{3t}+Be^{4t}$$ מפה נוכל לנחש את הפתרון עם תנאי ההתחלה ע”י הצבה של $t=1,3$ (או באופן כללי כל שני תנאי התחלה כלשהם). נשים לב שבשלב הזה אנחנו עוד לא יודעים כלום על התנאי התחלה ובגלל ש $$\\align{A & =\\phantom{-}y'(0)-4y(0) \\\\ B &=-y'(0)+3y(0)}$$ מוגדרים ע”י מערכת לינארית הפיכה, ולכן הגדרת $A,B$ שקולה להגדרת $y(0),y'(0)$ , כלומר לא “איבדנו” שום דבר במעבר ל $A,B$ . דוגמא: פתרון פרטי למשוואה דיפרנציאלית רגילה נחפש פתרון למשוואה הבאה: $$\\align{12t+5 &= y''(t)-7y'(t)+12y(t) \\\\ .y(0) &= 4,\\;\\; y'(0)=11}$$ החלק ההומוגני של המשוואה הוא כמו בדוגמא הקודמת ולכן נצפה לראות שוב את $e^{4t}$ ואת $e^{3t}$ ומה שחסר לנו זה פתרון פרטי. נפעיל שוב את התמרת פורייה ונקבל בצורה דומה לחישוב מהדוגמא הקודמת: $$.\\cl[y''-7y'+12y] = (s^2-7y+12)\\cl[y]-(s-7)y(0)-y'(0) = (s^2-7y+12)\\cl[y]-4s+17$$ ההתמרה של אגף שמאל מהמד”ר למעלה תהיה: $$.\\cl[12t+5](s)=12\\cl[t](s)+\\cl[5](s)=12\\cdot \\frac{1}{s^2} +5\\cdot\\frac{1}{s} = \\frac {12+5s}{s^2}$$ שוב נבצע העברת אגפים ונקבל ש $$.\\cl[y]=\\frac{\\frac{12+5s}{s^2}+4s-17}{s^2-7s+12} = \\frac{4s^3-17s^2+5s+12}{s^2(s-3)(s-4)} = \\frac{1}{s-4}+\\frac{2}{s-3}+\\frac{1}{s}+\\frac{1}{s^2}$$ כמו בדוגמא הקודמת, גם פה נקבל את ה”ניחוש” שהוא גם יהיה הפתרון: $$.y(t)=e^{4t}+2e^{3t}+1+t$$ בעיה: הפתרונות שקיבלנו הם לא אינטגרבילים בהחלט או בריבוע. מה יקרה אם ננסה לפתור את המד”ר האלו עם התמרת פורייה? האם ואיזה פתרון נקבל?"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_heaviside": {
    "title": "פונקציית הביסייד",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_heaviside",
    "body": "בהגדרה של התמרת לפלס עשינו אינטגרציה על התחום $[0,\\infty)$ . יש יתרונות לחד צדדיות הזאת - למשל החסמים על $\\frac{|f(t)|}{e^{s_0t}}$ זה תנאי מאוד חלש שנותן לנו הרבה מאוד בהתמרה הזאת, אבל אם מסתכלים על כל הישר הממשי $(-\\infty, \\infty)$ זה פתאום מכריח את הגבול $\\displaystyle{\\lim_{t\\to -\\infty}} f(t) = 0$ . אפשר להגדיר את החסם בצורה סימטרית כדי להמנע מזה, אבל זה לא נוח כל כך מצד אחד, ומצד שני בהרבה מהתופעות הפיזיקליות שאנחנו מנסים למדל, ה”התפתחות” לאינסוף גם ככה נעשית רק בכיוון אחד (למשל התקדמות של הזמן) ויש לנו נקודת התחלה טבעית שאנחנו חושבים עליה כזמן $t=0$ . בנסיון לשנות את הנקודת פתיחה הזאת גם לנקודות אחרות ולפתח יותר את מה שאנחנו מסוגלים לעשות עם ההתמרה, מגדירים את פונקצית הביסייד (על שמו של המתמטיקאי ופיזיקאי Oliver Heaviside). הגדרה ודוגמאות הגדרה: פונקצית הביסייד לכל קבוע ממשי $c \\in \\RR$ נגדיר את פונקציית הביסייד: $$.H_{[c,\\infty)}(t)=\\cases{0 & t עבור $c=0$ נסמן פשוט $H(t):=H_{[0,\\infty)}(t)$ ואז נקבל גם ש $H_{[c,\\infty)}(t)=H(t-c)$ . נשים לב שבאמצעות פונקציית הביסייד אפשר (כמו שכבר ראינו) לחשוב על התמרת לפלס כאינטגרל על כל הישר ולא רק על $[0,\\infty)$ . בהינתן פונקציה כלשהי $f:[0,\\infty)\\to \\CC$ אם נשלים אותה לפונקציה כלשהי על כל הישר $\\tilde{f}:\\RR \\to \\CC$ אז נקבל ש: $$.\\cl[f](s)=\\int_0^\\infty f(t)e^{-st}\\dt=\\int_{-\\infty}^\\infty H(t)\\tilde{f}(t)e^{-st}\\dt $$ במילים אחרות, צריך לחשוב כאילו הפונקציות שלנו הן תמיד מהצורה $H(t)\\tilde{f}(t)$ . בנוסף, זה מרמז שאפשר עכשיו לעשות את הכיוון של “הזזה $\\Leftarrow$ סיבוב” גם להתמרת לפלס: טענה: תהא $f:[0,\\infty)\\to \\CC$ כך ש $\\cl[f](s)$ קיים לכל $s>s_0$ . אז לכל $c\\geq 0$ ו $s>s_0$ מתקיים ש $$.\\dboxed{\\cl\\left[H_{[c,\\infty)}(t)f(t-c)\\right](s) = \\cl\\left[H(t-c)f(t-c)\\right](s) =e^{-sc}\\cl[f](s)}$$ הוכחה: ההוכחה היא עכשיו החלפת משתנים פשוטה: $$\\align{\\cl[H(t-c)f(t-c)](s) &=\\int_0^\\infty H(t-c)f(t-c)e^{-st}\\dt = \\int_{-c}^\\infty H(x)f(x)e^{-s(x+c)}\\dx \\\\ &=\\int_{0}^\\infty f(x)e^{-s(x+c)}\\dx = e^{-sc}\\cl[f](s)}$$ דוגמא: כדי לחשב את ההתמרה של פונקציית הביסייד $H_{[c,\\infty)}$ , אפשר להשתמש בטענה הקודמת (או לחשב ישירות - תרגיל) ולקבל $$.\\cl[H(t-c)](s)=e^{-sc}\\cl[1]=\\frac{e^{-sc}}{s}$$ דוגמא: פונקציה מציינת פונקצייה מציינת על קטע $[a,b)$ היא פשוט הפרש של שתי פונקציות הביסייד: $$.\\chi_{[a,b)}=H_{[a,\\infty)} - H_{[b,\\infty)}$$ מפה נקבל שעבור $0\\leq a מתקיים ש $$.\\cl[\\chi_{[a,b)}](s)=\\frac{e^{-sa}-e^{-sb}}{s}$$ דוגמא: \"הדבקת פונקציות\" נסתכל על ה”הדבקה” של פונקציה לינארית ופונקציה קבועה: $$.h(t)=\\cases{t & 0\\leq t \\leq 1 \\\\ 1 & 1 אפשר גם לחשוב על הפונקציה הזאת כפונקציה לינארית פחות הזזה שלה, ונראה את זה פורמלית עם פונקציות הביסייד: $$.h(t) = t\\cdot \\chi_{[0,1)}(t)+\\Hs{1}(t)=t\\left(\\Hs{0}(t)-\\Hs{1}(t)\\right)+\\Hs{1}(t)=(t-0)\\Hs0(t)-(t-1)\\Hs{1}(t)$$ סה”כ ההתמרה תהיה $$.\\cl[h](s) = \\cl[t] - e^{-s}\\cl[t] = \\frac{1 - e^{-s}}{s^2}$$ דוגמא: הזזות של פולינומים באופן כללי כל מכפלה של פונקציית הביסייד בפולינום ניתן לשנות לצורה בה מופיעות הזזות, כלומר $f(t-c)\\Hs c(t)=f(t-c)H(t-c)$ . כדוגמא, נסתכל על הפונקציה $f(t)=t^2\\Hs 2(t)$ . נפתח את הפונקציה בצורה הבאה: $$.f(t)=t^2H(t-2)=(t-2+2)^2H(t-2)=(t-2)^2H(t-2)+4(t-2)H(t-2)+4H(t-2)$$ כלומר, קיבלנו הזזות של $1,t,t^2$ . סה”כ ההתמרה של הפונקציה הזאת תהיה : $$.\\cl[f(t)](s)=e^{-2t}\\cl[t^2+4t+4] = e^{-2t} \\left[\\frac{2}{s^3} + \\frac{1}{s^2}+\\frac{1}{s}\\right]$$ אם יש לנו הדבקות של פולינומים כמו בדוגמא הקודמת, אז זה בעצם סכומים של מכפלות של פולינומים בהביסייד, ולכן תמיד ניתן לפתח אותם לסכום של הזזות כאלו. שימושים במשוואות דיפרנציאליות דוגמא 1: נחפש פתרון למשוואה הדיפרנציאלית: $$,\\align{y''(t)+y(t)&=f(t) \\\\ y(0)=y'(0)&=0}$$ כאשר $$.f(t)=\\cases{\\sin(\\pi t) & 0\\leq t\\leq 1 \\\\ 0 & 1 נבצע התמרת לפלס לשני האגפים של המשוואה. עבור האגף השמאלי נקבל: $$.\\cl[y''+y](s)=s^2\\cl[y](s)-sy(0)-y'(0)+\\cl[y](s)=(1+s^2)\\cl[y](s)$$ באגף ימין נשתמש בפונקציית הביסייד כדי לקבל ש: $$.f(t)=(\\Hs0-\\Hs1)\\sin(\\pi t)=H(t)\\sin(\\pi t) - H(t-1)\\sin(\\pi(t-1+1))=H(t)\\sin(\\pi t)+H(t-1)\\sin(\\pi(t-1))$$ מכאן נקבל שההתמרה של אגף ימין של המשוואה הדיפרנציאלית היא $$,\\cl[f](s) =(1+e^{-s})\\cl[\\sin(\\pi t)](s) =(1+e^{-s})\\cl[\\sin(\\pi t)](s) = (1+e^{-s})\\frac{\\pi}{s^2+\\pi^2}$$ ועכשיו סה”כ המשוואה הדיפרנציאלית הותמרה ל : $$.\\cl[y](s)=(1+e^{-s})\\frac {\\pi}{(s^2+\\pi^2)(1+s^2)}$$ נשים לב שאם נמצא פונקציה המקיימת $\\cl[g(t)]=\\frac {\\pi}{(s^2+\\pi^2)(1+s^2)}$ אז שוב שימוש בפונקציית הביסייד יתן לנו ש $$,\\cl[y](s)=\\cl\\left[ H(t)g(t)+H(t-1)g(t-1) \\right]$$ ולכן נוכל לנחש ש $y(t)=H(t)g(t)+H(t-1)g(t-1)$ . קצת פירוק לפונקציות רציונליות יתן לנו ש: $$.\\frac {\\pi}{(s^2+\\pi^2)(1+s^2)} = \\frac{\\pi}{1-\\pi^2}\\left(\\frac {1}{s^2+\\pi^2} - \\frac {1}{1+s^2} \\right) = \\frac{1}{1-\\pi^2}\\cl[\\sin(\\pi t)-\\pi\\sin(t)](s)$$ סה”כ, נקבל את הפתרון למשוואה: $$y(t)=\\frac{1}{1-\\pi^2}\\left[H(t)\\left(\\sin(\\pi t)-\\pi\\sin(t)\\right) - H(t-1)\\left(\\sin(\\pi (t-1))-\\pi\\sin(t-1)\\right)\\right]$$ דוגמא: משוואה דיפרנציאלית חלקית נשתמש בלפלס כדי לפתור את המשוואה הבאה: $$.\\align{0 נבצע התמרת לפלס יחסית למשתנה $t$ ונסמן את ההתמרה ב $\\tilde{u}(x,s)=\\cl[u(x,t)](s)$ . קודם כל, התמרה של תנאי ההתחלה הראשון למעלה, תיתן לנו ש $$.\\tilde{u}(0,s)=0\\;,\\;\\forall s$$ שימוש בתכונות ההתמרה ותחת ההנחה שמותר להחליף סדר גזירה והתמרה (עבור המשתנה $x$ ) נקבל ש: $$.s^2\\tilde{u}(x,s)-s\\overbrace{u(x,0)}^{=0}-\\overbrace{u'_t(x,0)}^{=0}+\\frac{10}{s}=\\tilde{u}''_{xx}(x,s)$$ נקבע עכשיו את $s>0$ ואז פתרון כללי למשוואה הזאת הוא מהצורה : $$.U(x,s)=Ae^{-xs}+Be^{xs}-\\frac{10}{s^3}$$ מהתנאי התחלה עבור $x=0$ נקבל ש $0=A+B-\\frac{10}{s^3}$ . בדרך כלל מסיבות פיזיקליות נרצה להניח שההתמרה שלנו חסומה, ולכן בהכרח נקבל ש $B=0$ , כלומר סה”כ הפתרון שלנו יהיה $$.\\tilde{u}(x,s)=\\frac{10}{s^3}(e^{-xs}-1)$$ כמו בדוגמא הקודמת, בגלל ש $\\cl[5t^2]=\\frac{10}{s^3}$ אז נקבל שהביטוי למעלה שווה ל $\\cl[H(t-x)5(t-x)^2 - H(t)5t^2]$ , ולכן ננחש שהפתרון הוא $$u(t,x)=H(t-x)5(t-x)^2 - H(t)5t^2=\\cases{5(t-x)^2-5t^2 & t\\geq x \\geq 0 \\\\-5t^2 & x>t\\geq 0}$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_laplace_convolution": {
    "title": "קונבולוציה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_laplace_convolution",
    "body": "בדיוק כמו בהתמרת פורייה, גם בהתמרת לפלס נרצה לדעת מה זו ההתמרה של מכפלה של פונקציות, וגם פה זה יוביל אותנו לפעולת הקונבולוציה. בנוסף, בגלל שהתמרת לפלס היא על פונקציות שמוגדרות ב $[0,\\infty)$ אז החישוב הוא אפילו יותר פשוט. נזכיר שעבור פונקציות עם תומך ב $[0,\\infty)$ , כלומר פונקציות המתאפסות מחוץ לקבוצה הזאת : $f(x)=0, x , נח לחשוב עליהן כ $H(t)f(t)$ כאשר $H(t)$ היא פונקציית הביסייד, ועבור פונקציות כאלו כאשר $x\\geq 0$ נקבל ש $$\\left[\\;H(t)f(t)\\;*\\;H(t)g(t)\\;\\right]\\;(x) = \\int_{-\\infty}^\\infty H(x-t)f(x-t)\\cdot H(t)g(t)\\dt = \\int_{0}^\\infty H(x-t)f(x-t)\\cdot g(t)\\dt = \\int_{0}^x f(x-t)\\cdot g(t)\\dt$$ כלומר, האינטגרל בתוך הקונבולוציה הוא תמיד על קטע סופי $[0,x]$ . בדוגמא למטה אנחנו רואים את $H(t)g(t)$ באדום, את $H(x-t)f(x-t)$ בסגול, ואת המכפלה שלהם שנמצאת רק בתחום $[0,x]$ בשחור. מסקנה: קונבולוציה על פונקציות על מספרים חיוביים אם $f,g$ נתמכות ב $[0,\\infty)$ אז הקונבולוציה שלהם מקיימת: $$.\\dboxed{(f*g)(x)=\\cases{\\int_0^x f(x-t)g(t)\\dt & x\\geq 0 \\\\ 0 & x טענה: מכפלה של פונקציות הביסייד לכל $a,b\\in \\RR$ מתקיים ש $$.(H(t-a)*H(t-b))(x) = (x-a-b)H(x-a-b)$$ הוכחה: חישוב של הקונבולוציה יתן: $$(H(t-a)*H(t-b))(x)= \\int_{-\\infty}^\\infty H(x-t-a)\\cdot H(t-b) \\dt= \\int_{b}^\\infty H(x-t-a)\\dt= \\int_{-\\infty}^{x-a-b} H(s)\\ds$$ אם $x-a-b אז האינטגרנד הוא זהותית אפס בתוך התחום $(-\\infty,x-a-b]$ . אחרת הוא שווה ל 1 ב $s\\geq 0$ ולכן נקבל ש $$\\int_{-\\infty}^{x-a-b} H(s)\\ds=\\left\\{\\begin{array}{lr}0 & :x התכונות הרגילות של הקונבולוציה נכונות גם במקרה שלנו: טענה: יהיו $f,g,h$ פונקציות הנתמכות ב $[0,\\infty)$ . אז בכל מקום בו הקונבולוציה קיימת מתקיים ש: אסוציאטיביות: $f*(g*h)=(f*g)*h$ . קומוטטיביות: $f*g=g*f$ . לינאריות: לכל $\\alpha \\in \\CC$ מתקיים $(f+\\alpha g)*h=f*h+\\alpha f*g$ . משפט הקונבולוציה יהיו $f,g$ פונקציות הנתמכות ב $[0,\\infty)$ , ונניח שקיימים קבועים $K_f, K_g, s_0$ כך ש $|f(t)|\\leq K_fe^{s_0t}$ ו $|g(t)|\\leq K_g e^{s_0t}$ . אז: לכל $x\\geq 0$ מתקיים ש $$.|(f*g)(x)|\\leq K_fK_gxe^{s_0x}$$ לכל $s>s_0$ ההתמרה של הקונבולוציה מוגדרת היטב ומתקיים ש $$.\\cl\\left[f*g\\right](s)=\\cl[f](s)\\cdot \\cl[g](s)$$ הוכחה: עבור $x\\geq 0$ מתקיים ש $$.|(f*g)(x)|\\leq \\int_0^x|f(x-t)|\\cdot|g(t)|\\dt\\leq \\int_0^xK_fK_ge^{s_0(x-t+t)}\\dt=K_fK_ge^{s_0x}\\int_0^x1\\dt=K_fK_ge^{s_0x}x$$ מהחלק הראשון נקבל שהתמרת לפלס של $f*g$ מוגדת לכל $s>s_0$ , ושווה ל: $$.\\cl[f*g](s) = \\int_0^\\infty \\left(\\int_0^tf(t-y)g(y)\\dy\\right)e^{-st}\\dt$$ בגלל שכל האינטגרלים מתכנסים בהחלט, ניתן להשתמש במשפט פוביני כדי להחליף סדר אינטגרציה. התחום מוגדר ע”י $0\\leq y\\leq t$ ולכן $$.\\int_0^\\infty \\left[\\int_0^t (\\cdots) \\dy \\right]\\dt = \\int_0^\\infty \\left[\\int_0^\\infty \\chi_{[0,t]}(y)(\\cdots) \\dy\\right] \\dt = \\int_0^\\infty \\left[\\int_0^\\infty \\chi_{[0,t]}(y)(\\cdots) \\dt \\right]\\dy= \\int_0^\\infty \\left[\\int_y^\\infty (\\cdots) \\dt \\right]\\dy$$ סה”כ נקבל ש $$.\\align{\\cl[f*g](s) &= \\int_0^\\infty \\left(\\int_y^\\infty f(t-y)g(y)e^{-st}\\dt\\right)\\dy = \\int_0^\\infty g(y)e^{-sy}\\left(\\int_y^\\infty f(t-y)e^{-s(t-y)}\\dt\\right)\\dy \\\\ &= \\int_0^\\infty g(y)e^{-sy}\\left(\\int_0^\\infty f(t)e^{-st}\\dt\\right)\\dy = \\cl[g](s)\\cdot \\cl[f](s)}$$ דוגמאות דוגמא 1 כבר ראינו שבהינתן $f$ רציפה למקוטעין, אם מגדירים את $g(t)=\\int_0^tf(x)\\dx$ אז $\\cl[g](s)=\\frac{1}{s}\\cl[f](s)$ כאשר הרעיון היה להשתמש בתכונת הנגזרת, ואז להפוך אותה כדי לקבל אינטגרל. בנוסף, אנחנו יודעים ש $\\frac{1}{s}=\\cl[1](s)$ (או יותר נכון $\\frac{1}{s}=\\cl[H(t)](s)$ ) ולכן יש לנו מכפלה של התמרות. מצד שני, ניתן לכתוב $g(t)=(f(t)*H(t))(t)$ ולכן בעצם התוצאה הזאת נובעת גם ממשפט הקונבולוציה: $$.\\cl[g](s)=\\cl[H(t)*f(t)](s)=\\cl[H](s)\\cl[f](s)=\\frac{1}{s}\\cl[f](s)$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_integral_equations": {
    "title": "משוואות אינטגרליות",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_integral_equations",
    "body": "אחד השימושים למשפט הקונבולוציה הוא בפתרון משוואות אינטגרליות. בפרט, אם נתונות הפונקציות $g,h$ ורוצים למצוא פונקציה $f$ המקיימת משוואה מהצורה: $$,\\int_0^t f(t-y)g(y)\\dy=h(t)$$ מאחר ואגף שמאל הוא פשוט $(f*g)(t)$ , תחת ההנחה ששתי הפונקציות נתמכות ב $[0,\\infty)$ , אז נוכל להפעיל את משפט הקונבולוציה כדי לקבל ש: $$.\\cl[f](s)\\cdot \\cl[g](s)=\\cl[h](s)$$ עכשיו אפשר לנסות לחלץ את $\\cl[f]$ ואז לבצע התמרה הפוכה. דוגמאות דוגמא 1: ננסה למצוא פונקציה $f$ המקיימת: $$.\\int_0^tf(y)\\cos(y-t)\\dy = t\\cdot \\sin(t)$$ התמרת לפלס של אגף שמאל תיתן: $$,\\cl[f](s)\\cdot \\cl[\\cos(t)](s)=\\cl[f](s)\\cdot \\frac {s}{s^2+a^2}$$ והתמרת אגף ימין: $$\\cl[t\\cdot\\sin(t)](s)=-\\cl[\\sin(t)]'(s)=-\\frac{d}{\\ds}\\left(\\frac{1}{s^2+1}\\right)=\\frac{2s}{(s^2+1)^2}$$ לכן, סה”כ נקבל ש: $$,\\cl[f](s)=\\frac{2}{s^2+1}=\\cl[2\\sin(t)](s)$$ כלומר, כדאי לנחש (התמרה הפוכה) ש $f(t)=2\\sin(t)$ , ובאמת קל לבדוק שבאמת הפונקציה הזאת פותרת את המשוואה האינטגרלית. דוגמא 2: נשנה את המשוואה מהדוגמא הראשונה ל: $$.\\int_0^tf(y^2)\\cos(y-t)\\dy = t\\cdot \\sin(t)$$ עכשיו אין לנו בדיוק קונבולוציה, אך נוכל לשנות קצת את המשוואה כדי שכן תהיה. אם נגדיר $g(t)=f(t^2)$ , אז נקבל את המשוואה $$,\\int_0^tg(y)\\cos(y-t)\\dy = t\\cdot \\sin(t)$$ שראינו מקודם שיש לה פתרון $f(t^2)=g(t)=2\\sin(t)$ . זה אומר שהפתרון של המשוואה המקורית יהיה $f(t)=2\\sin(\\sqrt{t})$ . נשים לב שהפתרון הזה רק תקף עבור $t\\geq 0$ כי אחרת לא נוכל לקחת שורש, אבל מראש זו הייתה ההנחה שלנו כי אנחנו עושים התמרת לפלס (ומעבר לזה, בתנאי האינטגרלי מופיעים רק הערכים של $f$ על $t\\geq 0$ כלומר בשום שיטה אחרת לא נקבל מה קורה ב $t ). סה”כ, הדוגמא הזאת באה להראות שגם דברים שלא נראים במבט ראשון כמו קונבולוציה, אפשר להמיר לקונבולוציה אמיתית. דוגמא 3: פתרו את המשוואה: $$.\\int_0^t e^{t-y}f(y)\\dy=f(t)-1$$ אגף שמאל זה הקונבולוציה $f*e^t$ (תחת ההנחה הרגילה שהפונקציות שלנו נתמכות ב $[0,\\infty)$ ) ולכן התמרת לפלס של שני האגפים תיתן: $$.\\frac{1}{s-1}\\cdot \\cl[f](s)=\\cl[e^{1\\cdot t}](s)\\cdot\\cl[f](s)=\\cl[f](s)-\\cl[1](s)=\\cl[f](s)-\\frac{1}{s}$$ בידוד של $\\cl[f]$ יתן ש: $$\\cl[f](s)=\\frac{s-1}{s(s-2)}=\\frac{1}{2}\\left(\\frac{1}{s}+\\frac{1}{s-2}\\right)=\\frac{1}{2}\\left(\\cl[1+e^{2t}](s)\\right)$$ לכן סה”כ נקבל ש $f(t)=\\frac{1+e^{2t}}{2}$ יהיה פתרון למשוואה. דוגמא 4: פתרו את המשוואה $$.\\int_0^tf(y)\\dy=t-f'(t)$$ במקרה הזה נראה שאין לנו קונבולוציה, אבל היא באמצ כן מתחבאת שם, כי $\\int_0^tf(y)\\dy = (f*1)(t)$ . שימוש בתמרת לפלס יתן לנו: $$\\frac{\\cl[f](s)}{s} = \\cl[f](s)\\cdot\\cl[1](s)=\\cl[t](s)-\\cl[f'](s)=\\frac{1}{s^2}-\\left(s\\cl[f](s)-f(0)\\right)$$ חילוץ של $\\cl[f]$ יתן ש: $$,\\cl[f](s)=\\frac{\\frac{1}{s}+sf(0)}{1+s^2}=\\frac{1}{s}-(1-f(0))\\frac{s}{s^2+1}=\\cl[1-(1-f(0))\\cos(t)](s)$$ ולכן נקבל ש $$.f(t)=1-(1-f(0))\\cos(t)$$ נשים לב שהצבה של $t=0$ תיתן ש $f(0)=f(0)$ , כלומר יש לנו בעצם פרמטר חופשי (או לחלופין צריך תנאי התחלה בשביל פתרון יחיד). אפשר לראות שבאמת אם מוסיפים $\\cos(t)$ לפונקציה $f(t)$ אז הפונקציה החדשה שנקבל עדיין תפתור את המשוואה האינטגרלית/דיפרנציאלית."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_laplace_summerization": {
    "title": "התמרת לפלס - סיכום",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_laplace_summerization",
    "body": "הגדרה ודוגמאות הגדרה: התמרת לפלס תהא $f:[0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין. נגדיר את התמרת לפלס $\\cl[f]$ להיות הפונקציה: $$\\cl[f](s)=\\int_0^\\infty f(t)e^{-st}\\dt$$ עבור כל $s\\in \\RR$ שם האינטגרל מתכנס. $Domain$ $\\cl[f](s)$ $f(t)$ $s>Re(z)$ $\\frac{1}{s-z}$ $e^{zt}$ $s>0$ $\\frac{a}{s^2+a^2}$ $\\sin(at)$ $s>0$ $\\frac{s}{s^2+a^2}$ $\\cos(at)$ תכונות משפט תהא $f:\\RR \\to \\CC$ רציפה למקוטעין. אם קיים $s_0\\in \\RR$ ו $0 כך ש $|f(t)|\\leq Ke^{s_0t}$ לכל $t\\geq 0$ , או בצורה שקולה הפונקציה $\\frac{|f(t)|}{e^{s_0t}}$ חסומה ב $t\\geq 0$ , אז $\\cl[f](s)$ מוגדר לכל $s>s_0$ ומתקיים ש $$.\\limfi{s} \\cl[f](s)=0$$ טענה: תכונות גאומטריות של התמרת לפלס בכל התכונות למטה, הפונקציות הן רציפות למקוטעין עם התמרת פורייה שמוגדרת בנקודה $s$ . לינאריות: $\\cl[\\alpha\\cdot f + g](s) = \\alpha \\cl[f](s)+\\cl[g](s)$ . מתיחה: עבור $a>0$ מתקיים ש $\\cl[f(at)](s) = \\frac{1}{a} \\cl[f]\\left(\\frac{s}{a}\\right)$ . כפל באקספוננט $\\Leftarrow$ הזזה: $\\cl[e^{at}f(t)](s) = \\cl[f](s-a)$ . טענה: כפל ב $t$ מותמר לנגזרת (והפוך) תהא $f:[0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין כך ש $\\frac{|f(t)|}{e^{s_0t}}$ חסומה. אם $f$ רציפה ובנוסף היא גזירה ברציפות למקוטעין, אז לכל $s>s_0$ מתקיים ש $$.\\cl[f'](s)=s\\cl[f](s)-f(0)$$ התמרת לפלס $\\cl[f]$ היא גזירה ולכל $s>s_0$ מתקיים ש $$ .\\cl[tf(t)](s)=-\\frac{d}{ds} \\cl[f](s)$$ מסקנה: נגזרות מסדר גבוה תהא $f:[0,\\infty)\\to \\CC$ פונקציה רציפה למקוטעין כך ש $\\frac{|f(t)|}{e^{s_0t}}$ חסומה. אם הנגזרות $f^{(k)}(t)$ קיימות רציפות וחסומות ע”י $Ke^{s_0t}$ עבור $k\\leq n-1$ ובנוסף $f^{(n)}$ רציפה למקוטעין, אז לכל $s>s_0$ מתקיים ש $$.\\cl[f^{(n)}](s)=s^n\\cl[f](s)-\\sum_{k=0}^{n-1} s^{n-1-k}f^{(k)}(0)$$ התמרת לפלס גזירה אינסוף פעמים ב $s>s_0$ ומתקיים ש $$.\\cl[t^nf(t)](s)=(-1)^n\\frac{d^n}{\\ds^n}\\cl[f](s)$$ פונקציית הביסייד הגדרה: פונקצית הביסייד לכל קבוע ממשי $c \\in \\RR$ נגדיר את פונקציית הביסייד: $$.H_{[c,\\infty)}(t)=\\cases{0 & t עבור $c=0$ נסמן פשוט $H(t):=H_{[0,\\infty)}(t)$ ואז נקבל גם ש $H_{[c,\\infty)}(t)=H(t-c)$ . הערה: פונקצייה מציינת ניתן לכתוב פונקציה מציינת כהפרש של פונקציות הביסייד: $$.\\chi_{[a,b)}(t)=\\Hs{a}(t)-\\Hs{b}(t)$$ טענה: תהא $f:[0,\\infty)\\to \\CC$ כך ש $\\cl[f](s)$ קיים לכל $s>s_0$ . אז לכל $c\\geq 0$ ו $s>s_0$ מתקיים ש $$.\\cl\\left[H_{[c,\\infty)}(t)f(t-c)\\right](s) = \\cl\\left[H(t-c)f(t-c)\\right](s) =e^{-sc}\\cl[f](s)$$ קונבולוציה מסקנה: קונבולוציה על פונקציות על מספרים חיוביים אם $f,g$ נתמכות ב $[0,\\infty)$ אז הקונבולוציה שלהם מקיימת: $$.\\dboxed{(f*g)(x)=\\cases{\\int_0^x f(x-t)g(t)\\dt & x\\geq 0 \\\\ 0 & x טענה: יהיו $f,g,h$ פונקציות הנתמכות ב $[0,\\infty)$ . אז בכל מקום בו הקונבולוציה קיימת מתקיים ש: אסוציאטיביות: $f*(g*h)=(f*g)*h$ . קומוטטיביות: $f*g=g*f$ . לינאריות: לכל $\\alpha \\in \\CC$ מתקיים $(f+\\alpha g)*h=f*h+\\alpha f*g$ ."
  }}
