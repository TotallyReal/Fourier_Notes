{"/Fourier_Notes/notes/Fourier/Fourier%20Course%20Information": {
    "title": "Course notes in Fourier Analysis",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Fourier%20Course%20Information",
    "body": "Ofir David Description short description for Fourier analysis Syllabus Fourier series: Definitions, Dirichlet’s convergence theorem, … Fourier transforms: Definition,… Prerequisites Basic Linear Algebra: Linear system of equations, matrices, eigenvalues and eigenvectors. Inner Products: Inner products and norms, orthonormal bases, orthogonal projection. Calculus: Limits, continuity, derivatives and integrals on finite and infinite segments. Recommended books Table of contents Motivation Inner product spaces - a reminder Orthogonal sets - a reminder Infinite Orthonormal basis The Fourier Series Fourier Transform"
  },"/Fourier_Notes/notes/Fourier/Motivation": {
    "title": "Motivation",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Motivation",
    "body": "Course: Fourier Course Information What do we analyze? The “Fourier Analysis” is one of the most useful tool when we try to study real (and complex) functions, which naturally appear in many places, for example: Physics: represent position, speed, force, temperature, etc. , Digital representations: like pixels in images, sound in audio file, or movement in animation “Abstract” quantities: stock market prices, or even measuring “popularity” in social media sites. These “real functions” come in several flavors, according to their domains, and the most common types are: Functions on the real line: $f:\\RR \\to \\RR$ , Periodic functions, namely $f:\\RR \\to \\RR$ , with some period $T>0$ , so that $f(x+T)=f(x)$ , Finite and discrete domain $f:\\{1,...,n\\} \\to \\RR$ , or Infinite and discrete domain $f: \\ZZ \\to \\RR$ , or as we usually call them sequences. Having so many “real functions” types, which represent so many objects in our world, it is very important to find mathematical structures and tools to study them. We already learned some of them in basic courses in calculus and algebra: First, we need to choose the type of functions: Bounded, Continuous, differentiable, piecewise continuous, integrable etc. Linear algebra: Usually this chosen family forms a vector space, allowing us to use all the tools from linear algebra. Calculus: Using the real line’s structure, we can look at continuity, derivative, integral, etc. Geometry: We can also measure the “size”, namely the norm, of a function in several ways. For example, the maximum distanance from zero $\\norm{f}_\\infty = \\sup|f(x)|$ , or measure the area beneath the graph of the function (in absolute value): [!figure]- Figure: These norms satisfy the well known triangle inequality $\\norm{f+g} \\leq \\norm f + \\norm g$ , which let us construct a geometry on our space, since we can measure distances, e.g. $$dist(f,g):=\\norm{f-g}.$$ Angles: More generally, we can think of “angles” between vectors, and draw intuition from the standard Euclidean spaces (e.g. the Pythagoras theorem). More formally, we add an inner product structure (as we shall see soon). The main idea in Fourier analysis, is that we add one more very interesting structure: Symmetry: These families usually have very nice “symmetries”. Usually we think of symmetries like reflections - we have the original object, we take its reflection though a mirror, and somehow get the “same” object (or at the very least it looks the same). Something similar happens with our real functions - for example, functions on the real line are “symmetric” to left and right translations: we can “move” functions to the left and right to get another function in the family : [!figure] Figure: $$ f(x) \\mapsto f(x+c) $$ We can think of this as advancing in position or time (depending on what our functions represent). This “symmetry” structure seems simple at first, however it leads to very interesting questions. Are there interesting “patterns” that our functions exhibit under this symmetries? For example, the function “repeats” itself when we move to the left or right? Are there “basic” patterns that we should look for when given such a function? This “pattern” search question is the main goal of our course, and in a sense the “Fourier analysis” can be thought of as pattern investigation of these functions. Remark: In our course we will mainly be interested in the periodic functions, and functions on the real line. In this study, we will naturally also see the space of infinite sequences. The finite discrete Fourier transforms are also very important, but will not be part of this course. However, they appear frequently in the theoretical side of computer science, if you wish to study more about them. “Real life” problems: Let see some examples for this pattern search in real life problems. Image compression In our modern day life, we take images and videos of almost everything that happens to us. More over, as our technology improves, we can save more and more information in each image. However, the problem with this approach is that this information takes a lot of space. One solution is to keep buying new space, whether it is a new hard drive or extra cloud space. Another solution is to try compressing the information instead. There are many compression algorithms and some of which (for example, the JPEG image compression) use ideas based on Fourier transforms. Pixelwise compression: Suppose we have a grayscale image file, where the color of each pixel is represented by an integer in $[0,255]$ going from black at $0$ all the way to white at $255$ . One very simple way to compress the image, is to change the pixel value from 255 options to something smaller, for example 16 or 8. This already gives us some compressions, and for some images it will be hard to notice the changes. However, as the number of pixel values decreases, we start to see jumps in colors. This is not too surprising, because we are trying to approximate our image by step (piecewise constant) functions. So even though two nearby pixels can have nearby color values originally, we can still have a jump in the color. This is because our compression looks at each pixel separately. To improve our compressions, we can try to exploit “patterns” in the image. For example, if there is a gradient section, namely the color changes linearly from one place to another, then we would only need two parameters to describe it (e.g. to write $ax+b$ ). Pattern compression: This idea of looking for patterns is measured in a sense in the Fourier transform coefficients. The Fourier transform “decomposes” the full image (and not each pixel separately) into sine and cosine wave patterns with weights which determines how “strong” that pattern is in the picture. For example, in a $10\\times10$ pixels image, there are $10\\cdot 10=100$ possible patterns, which you can see on the figure below on the left. Given any such image, we can for example keep only the “first” $[1,i]\\times [1,j]$ patterns, and plot it on the $(i,j)$ position as we see on the right with the cat picture. Note for example that the picture at the $(7,7)$ position only has $7\\cdot 7=49$ out of the $100$ patterns from the original picture (less than half!) and is already quite similar to the final image.     As in turns out, most weights in this pattern decomposition are quite small, and removing their corresponding patterns completely doesn’t change the image too much. Thus, in general we can throw away all the information about this small weight waves, and get a compression without losing too many details: For more details and some actual python code which runs the compressions above: Python image compression Some interesting YouTube videos about image compressions: Youtube: JPEG compression - By Computerfile Youtube: FFT image compression - By Steve Brunton To summarize, one way (of many) to think about Fourier transform is as extracting patterns from our data and the a weight for each pattern. If the data has nice “wavy” data, then the Fourier transform can help understanding it. Music and noise reduction When playing music on a piano (and many other instruments as well), we usually play different notes that sound “harmonious” together. For example, the chord C on the piano is a combination of the notes C,E,G. Each of these notes correspond to a “pure” sine sound wave with a given frequency, and when they are all played together, these waves sum up as can be seen in the image below.     Suppose now that we can hear this combination of notes together. Can we find out what are the notes being played? For example, can we extract from the C+E+G orange wave in the image, the C,E,G red, green and blue waves? This is one type of “pattern” search that Fourier transformation allows us to do. More over, when we are listening to the chord, there is probably a lot of background white noise, which we should think of as something “without any pattern”. Is it possible to extract from the “noisy” chord the original notes as well? This too can be done using Fourier transformation. See for example: Youtube: Denoising data with Fourier transformation - By Steve Brunton Youtube: Extract Musical Notes from Audio in Python with FFT - By Jeff Heaton Openprocessing: Visualizing Fourier transformation of audio Channel separation When opening a website online, usually our computer sends a request to our internet provider to send us exactly the information needed to view the website. However, when we watch television, or listen to the radio, our television and radio (usually) don’t send any requests, and instead get all the possible channels that we paid for, in a single stream of information. So the question is, how do they separate this single stream into the different channels? %% add some image with radio waves %% Here too, one of the ways of extracting these different channels is by in a sense coding them on different “Fourier patterns”. Once we find these patterns from the signal that we get, we can use it to reconstruct the original information sent over it. The tools of our craft What type of tools and ideas are we going to use? Divide and conquer - the orthonormal basis One of our main tools, is to decompose our large space of functions into smaller sections (=patterns), where we can understand each section separately, and once we do, combine them back together to understand our original function. You already learned about this idea back in your first course in linear algebra, which had the mysterious name of eigenvalues and eigenvectors. This was further expanded in the second course, about inner product spaces, where you saw the importance of geometry and working with orthonormal basis of eigenvalues. Here, we would take this idea to the next stage - the infinite dimensional space of functions - where our building blocks orthonormal basis would consist of the sine and cosine functions. Sine and cosine and the complex plane As mention several times, the sine and cosine functions are going to be our building blocks for the Fourier transform. At first glance, it is not clear why it happens and why they are so important, and in particular each one of one them on its own. However, they become quite interesting once combined - they are just two parts ( $x$ and $y$ coordinates) of the same very simple and very important process: Moving at constant speed on a circle. This clues us as to why these trigonometric functions arise so naturally - a lot of natural phenomena have natural “rotational” behavior, and when we see the sine or cosines function, it is usually just because we don’t see the “whole 2 dimensional picture”.     More over, this is also why many of the computations that will be done with sines and cosines in our course can be much more natural in the 2-dimensional complex plane. Recall (or learn in a few weeks) that by definition we have that $$e^{i\\theta} := \\cos(\\theta) + i\\sin(\\theta),$$ namely the sine and cosine functions are the imaginary and real parts of the nice (complex) exponential function. While complex functions seem a bit more complicated than real functions, they are in many ways much simpler, and in particular the exponential function is quite easy to work with. For example, it satisfies the well known exponential property of $e^{z+w}=e^z\\cdot e^w$ . While we won’t necessarily use this during the course, since complex functions are not a prerequisite, once you feel more at ease with the complex exponential function, you should go over the material and translate it to its complex notation. However, as a small taste for why this rotational behavior is so important, consider the following spring: Is the string moving to the right (which is our space’s “symmetry” action) or is it rotating around its axis? The rainbow colors might help to answer this question, but without them it would be impossible to decide. Indeed, the exponential property basically says that translation to the right and left $\\theta \\mapsto \\theta + \\phi$ is exactly the same as rotating since $e^{i(\\theta+\\phi)}=e^{i\\theta} \\cdot e^{i\\phi}$ (as an exercise, think what it means in the language of eigenvalues and eigenvectors). This transformation between left and right translation, and the simpler rotation is what stands at the mathematical heart of the Fourier transform. Some mathematical applications Differential equations Another important property that the sines and cosines (and even better, the exponential function) have, is that it is easy to differentiate them. More specifically $\\sin'(x)=\\cos(x)$ and $\\cos'(x)=-\\sin(x)$ . Such relations make it much simpler to solve differential equations where the functions are combinations of these sine and cosine functions. Luckily for us, the Fourier transform shows that most of the interesting functions can be written as such combinations, and this is quite useful for solving general differential equations. Number and group theory These Fourier transforms, and in particular it discrete finite versions, appear frequently in number theory when studying finite groups and fields. For example, they appear when studying Diophantine equations, namely integer solution for integral equations (e.g. solutions to $x^2 + y^2 = n$ for some integer $n$ ). In this area the “Fourier transforms” are usually called group representations. %%## Probability%% %%add here%% False[[invalid wikilink: Fourier Course Information#Table of contents]] , Next: Inner product spaces-&gt;"
  },"/Fourier_Notes/notes/Fourier/Inner%20product%20spaces%20-%20a%20reminder": {
    "title": "Inner product spaces - a reminder",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Inner%20product%20spaces%20-%20a%20reminder",
    "body": "Course: Fourier Course Information The Euclidean prototype We start by recalling some elements from the theory of inner products, which are our ways to give standard vector spaces a new geometric structure. The prototype of this structure is the standard Euclidean dot product on $\\RR^3$ defined by $$\\angles {(x_1, x_2, x_3),(y_1,y_2,y_3)}=x_1y_1+x_2y_2+x_3y_2.$$ This dot product measures two very important quantities. First is the length of a vector: $$\\norm {(x_1,x_2,x_3)} := \\sqrt{\\angles {(x_1,x_2,x_3),(x_1,x_2,x_3)}} = \\sqrt{x_1^2+x_2^2+x_3^2}.$$ The second are angles between vectors - if $\\theta$ is the angle between two vectors $u,v$ , then $$\\cos(\\theta)=\\frac{\\angles {u,v}}{\\norm{u}\\norm{v}}.$$ In particular, two vectors are perpendicular, namely $\\theta = \\frac{\\pi}{2}$ if $$u\\perp v \\iff \\angles {u,v} = 0.$$ This structure stands at the heart of the standard Euclidean geometry that we live in, and can be used to describe many of the interesting properties that our world has. In particular, one of the first theorems that most of us learn about, and is essential to many of the results in geometry: The Pythagorean theorem”: If $u \\perp v$ are perpendicular, then $\\norm{u+v}^2 = \\norm u^2 + \\norm v^2$ The inner product With this geometric intuition, let us recall its generalization as “False[[invalid wikilink: Inner product]]”. In the following, our field will always be one of $\\FF = \\RR,\\CC$ . Definition: Inner Products Let $V$ be a vector space. An inner product on $V$ is a function $$\\angles{\\cdot,\\cdot}:V\\times V \\to \\FF$$ such that Positive: For any $v\\in V$ we have $\\angles {v,v} \\geq 0$ with equality if and only if $v=0.$ Hermitian: For any $u,v\\in V$ we have that $\\angles {u,v} = \\overline{\\angles {v,u}}$ . Linear in first coordinate: For any $u,v,w\\in V$ and $\\alpha \\in \\FF$ we have $$\\angles {u+\\alpha v,w} = \\angles {u,w} + \\alpha \\angles {v,w}.$$ Let’s recall some interesting examples for inner product spaces: Examples: Standard inner product: Over $\\FF^n$ we have the standard inner product defined by: $$\\angles {(x_1,...,x_n),(y_1,...,y_n)} = \\sum_1^n x_i\\overline{y_i}.$$ Integrals: For continuous functions in $[0,1]$ we have the inner product: $$\\angles {f,g} =\\int_0^1 f(x)\\overline{g(x)}\\dx$$ Weighted integral: For a nonnegative function $w:[0,1]\\to[0,\\infty)$ we define: $$\\angles {f,g}_w:=\\int_0^1f(x)\\overline{g(x)}w(x)\\dx$$ The last two examples are for infinite dimensional inner products spaces (where the second example is a specific case of the third one). If we extend our space of continuous functions even a little bit to piecewise continuous functions, we suddenly have two interesting problems that need to be solved: The zero function: Consider the function: This is clearly a piecewise continuous function with one discontinuity at $x=1$ . While it is not the zero function, it does satisfy $\\angles{f,f}=0$ , so that the function $\\angles{\\cdot,\\cdot}$ is no longer an inner product. However, this is the only problem with the definition, which can be fixed easily by “increasing” the zero function to contain such functions. More formally, we will say that two functions are equivalent $f_1\\sim f_2$ if $f_1-f_2$ is almost zero, in the sense that $\\angles{f_1-f_2, f_1-f_2}=0$ . The inner product is well defined modulo this equivalence, namely if $f_1 \\sim f_2$ and $g_1 \\sim g_2$ , then $\\angles{f_1,g_1}=\\angles{f_2,g_2}$ . This should not be two surprising because for piecewise continuous function $f_1 \\sim f_2$ just means that $f_1(x)=f_2(x)$ except for finitely many points. Infinite integrals: Consider the function $f:[0,1]\\to\\RR$ defined by $f(x)=\\cases{\\frac{1}{x} & x>0 \\\\ 0 & x=0}.$ This too is a piecewise continuous function with only one discontinuity at $x=0$ . However this time $\\angles{f,f}=\\int_0^1\\frac{1}{x^2}\\dx = \\infty$ , so our inner product doesn’t necessarily return finite numbers. This problem can be solved by restricting our space of functions to “small” enough functions, namely just $f$ which satisfy $$\\int_0^\\infty |f(x)|^2\\dx If we have two such functions $f,g$ , then $$\\abs{f(x)\\overline{g(x)}} \\leq \\max\\{\\abs{f(x)}^2,\\abs{g(x)}^2\\}\\leq \\abs{f(x)}^2 + \\abs{g(x)}^2.$$ It follows that the integral in the inner product $$\\angles {f,g} =\\int_0^1 f(x)\\overline{g(x)}\\dx$$ converges in absolute value (and is at most $\\int_0^1 \\abs{f(x)}^2 dx + \\int_0^1\\abs{g(x)}^2\\dx$ ), and therefore converges. Definition: The space of piecewise continuous functions. For a segment $I\\subseteq \\RR$ we define: $$E^2(I)=\\{f:I\\to \\CC\\;\\mid\\;f\\text{ piecewise continuous, }\\int_I\\abs{f(x)}^2\\dx We sometimes also denote it by $E(I)$ . [!remark] Remark: $\\mathcal{L}^2$ v-functions. This space can (and should be) extended a little bit, and instead of piecewise continuous function, we can talk about integrable functions, and then it is denoted by $\\mathcal{L}^2$ . However, for simplicity we will continue to work with the piecewise continuous functions. The norm Once we have the inner product (which “corresponds” to angles), we also have the norm (which is the “length” of a vector): Definition: Norms Let $V$ be a vector space. A norm on $V$ is a function $$\\norm {\\cdot} : V \\to \\RR$$ such that: Positive: For any $v \\in V$ we have that $\\norm v\\geq 0$ with equality if and only if $v=0$ . Absolute homogeneous: For any $v\\in V$ and $\\alpha \\in \\FF$ we have $$\\norm {\\alpha v} = |\\alpha|\\cdot \\norm v.$$ Triangle inequality: For any $u,v\\in V$ we have $$\\norm {u+v} \\leq \\norm u + \\norm v.$$ There are many examples of interesting norms, but probably one of the most useful one for us is the induced norm: Theorem: The induced norm. If $V$ is an inner product space, then $\\norm v := \\sqrt {\\angles {v,v}}$ is a norm function. In particular, in the Euclidean geometry, this norm is the standard $\\norm {x_1,...,x_n} = \\sqrt{\\sum_1^n |x_i|^2}$ , and in our new and improved integral inner product on functions, we get: $$ \\norm f = \\sqrt{\\int_0^1 |f(x)|^2\\dx }. $$ The importance of the norm functions, is that we use them to measure the size of vectors, and more over we can talk about distances between vector, by defining $$\\mathrm{dist}(u,v):=\\norm{u-v}.$$ Different norms define different distances, which have all sorts of relations between them. The norms above are usually called the $\\mathcal {L}^2$ norms (since we take the square and then square root), and denoted by $\\norm {\\cdot}_2$ . A couple more interesting norms that we will encounter later on are the $\\mathcal{L}^1$ and $\\mathcal {L}^\\infty$ norms, both in the continues and discrete cases, defined respectively by: $$\\begin{align} \\norm f_1 & := \\int_0^1 |f(x)|\\dx & \\norm {(x_1,...,x_n)}_1 & := \\sum_1^n |x_i| \\\\ \\norm f_\\infty & := \\sup|f(x)| & \\norm {(x_1,...,x_n)}_\\infty & := \\max_i |x_i| \\end{align}.$$ Actually, these are part of a family of norms called $\\mathcal{L}^p$ norms for $1\\leq p\\leq \\infty$ defined by: $$\\begin{align} \\norm f_p & := \\left(\\int_0^1 |f(x)|^p\\dx\\right)^{1/p} & \\norm {(x_1,...,x_n)}_p & := \\left(\\sum_1^n |x_i|^p\\right)^{1/p} \\end{align}.$$ For $p=1,2$ we get the $\\mathcal{L}^1$ and $\\mathcal{L}^2$ norms, and in the limit $p\\to\\infty$ we obtain the $\\mathcal{L}^\\infty$ norm. While there are similarities between the discrete and continuous case, the discrete finite dimensional norms are much more well behaved than their infinite dimensional analogues. We will later discuss more in depth about these infinite dimensional norms, but for now let’s get a little bit of visual intuition about the finite dimensional case, by simply drawing the unit circle $\\norm{(x,y)}_p=1$ in the plane. As you can see, the standard unit circle for $p=2$ is an actual circle, while for $p=1$ we get a diamond, and as $p\\to \\infty$ it approaches (and in the limit is) a square lined up to the X and Y axes. Also, for $p the formula defined above is not a norm, and we can “see” it, since the interior of the unit circle is not convex. &lt;-False[[invalid wikilink: Fourier/Motivation]] , Back to table of contents , Next: Orthogonal sets-&gt;"
  },"/Fourier_Notes/notes/Fourier/Orthogonal%20sets%20-%20a%20reminder": {
    "title": "Orthogonal sets - a reminder",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Orthogonal%20sets%20-%20a%20reminder",
    "body": "Course: Fourier Course Information Orthonormal sets Once we have a vector space, usually the next step is finding a nice basis. In the finite dimensional case, we had the standard basis, denoted by $\\{e_1,...,e_n\\}$ where $$e_i= (0,...,0,\\overbrace{1}^i,0,...,0).$$ This is a simple yet powerful basis which arises naturally and used in many places. One of the main reasons for its usefulness is that it also has nice “geometric” properties, and in a sense it provides a “direction” to the space. Trying to understand and generalize this properties, led to the definition of orthonormal bases. Definition: Orthogonal and orthonormal bases Let $V$ be an inner product space and let $\\Omega \\subseteq V$ be a subset. The set is called: Orthogonal: if $0 \\not \\in \\Omega$ and any distinct pair of vectors $v\\neq u$ in $\\Omega$ are perpendicular, namely $\\angles {u,v} =0$ . Orthonormal: if in addition $\\norm v = 1$ for all $v\\in \\Omega.$ We can also write the orthonormality condition it in a more “compact” way. For any $u,v\\in \\Omega$ : $$\\angles {u,v}=\\begin{cases}1 & u = v \\\\0 & u\\neq v\\end{cases}.$$ The main advantage of this orthogonality definition, is that it lets us take our intuition from the standard Euclidean geometry to general spaces. In particular we have the following: Theorem: Orthonormal coefficients Suppose that $\\{v_1,...,v_n\\}\\subseteq V$ is an orthogonal set. Then: Pythagoras: $\\norm {\\sum_1^n v_i}^2 = \\sum_1^n \\norm {v_i}^2$ . Coefficients: If $v\\in span \\{ v_1, ..., v_n \\}$ , namely $v=\\sum_1^n \\alpha_i v_i$ , then $\\alpha_i = \\frac{\\angles {v,v_i}}{\\norm {v_i}^2}$ . In particular for an orthonormal set we get $$v = \\sum_1^n \\angles {v,v_i} v_i \\; , \\text{ and } \\; \\norm{v}^2 = \\sum_1^n |\\angles {v,v_i}|^2.$$ Proof: Under the assumption that $\\angles {v_i,v_j} = 0$ for $i\\neq j$ , we get that $$\\norm{\\sum_1^n v_i}^2 = \\angles {\\sum_{i=1}^n v_i, \\sum_{j=1}^n v_j}=\\sum_{i,j=1}^n \\angles{v_i,v_j}=\\sum_{i=1}^n \\angles{v_i,v_i}=\\sum_1^n \\norm {v_i}^2.$$ This is a straight forward computation: $$\\angles {v,v_j}= \\angles{\\sum_{i=1}^n \\alpha_i v_i,v_j} = \\sum_{i=1}^n \\alpha_i \\angles{v_i,v_j} = \\alpha_i \\norm {v_i}^2$$ As a simple example you should have in mind, in the standard Euclidean space, for a vector $\\bar{x}=(x_1,...,x_n)$ the i’th coordinate, namely the i’th coefficient in the standard basis, is just $x_i = \\angles{\\bar{x}, e_i}$ . The last theorem already suggests that having a basis of orthonormal vectors can be quite helpful, since it is very easy to find the coefficients (both theoretically, and if needed numerically). Moreover, whenever we have an orthogonal set, it is already half of its way to being a basis: Lemma: Any orthogonal set is linearly independent. In particular, in an inner product space of dimension $n$ , an orthogonal set of size $n$ is a basis. Proof: Let $\\Omega$ be an orthogonal set, and suppose that we can write zero as linear combination of its elements (which by definition is a finite combination). This means that $$ 0 = \\sum_1^n \\alpha_i \\omega_i $$ for some elements $\\omega_i \\in \\Omega$ . However, since the set is orthogonal, we know that $\\alpha_i=\\frac{\\angles {0,\\omega_i}}{\\norm {\\omega_i}^2}=0$ , so that this combination is trivial, and we conclude that $\\Omega$ is linearly independent. In an infinite dimensional space, life is much more interesting, and the definition of a spanning set, and therefore of a basis is slightly different, though it has the same intuition (it spans, but with limits). For now, lets see a couple of interesting examples in this infinite dimensional case with the inner product $\\angles{f,g}=\\int_0^1 f(x)g(x)\\dx$ . Example: Orthogonal step functions Consider the following set of functions which contains the constant 1 function, and for each integer $0\\leq p$ , and integer $0\\leq n \\leq 2^p - 1$ the function $$s_{p,i}(x) = \\begin{cases}-1 & \\frac{n}{2^p} In words: each such function chooses a section of length $1/2^p$ and has $-1$ on half of it and $+1$ on the second half. It should not be very hard to convince yourselves that these functions form an orthogonal set (prove it!). What would you change to make it an orthonormal set? The second, and our main, example, is the smooth version of the previous one. Theorem: Sines and Cosines Consider the inner product on functions on $[-\\pi,\\pi]$ defined by $$\\frac {1}{\\pi}\\int_{-\\pi}^\\pi f(x)\\overline{g(x)}\\dx .$$ The set of functions $$\\Omega=\\{\\cos(kx) : k\\geq 0\\} \\cup \\{\\sin(kx) : k\\geq 1\\}$$ is (almost) orthonormal - the only exception is that $cos(0x)\\equiv 1$ has norm $2$ instead of $1$ . This last theorem is quite important that this is going to be our Fourier patterns. The proof of elementary, and you should try to do it by yourself. Once you do have a proof, you are welcome to check on of these three proof methods: Proof: Using **Complex integrations**: If you know some complex integration, this is a very simple exercise using exponentials. For example, writing $\\cos(nx)=\\frac{e^{inx}+e^{-inx}}{2}$ , we get that $$\\align{\\angles {\\cos(mx),\\cos(nx)} & =\\angles {\\frac{e^{imx}+e^{-imx}}{2},\\frac{e^{inx}+e^{-inx}}{2}} \\\\ & =\\frac {1}{4}(\\angles {e^{imx},e^{inx}} + \\angles {e^{imx},e^{-inx}} + \\angles {e^{-imx},e^{inx}} + \\angles {e^{-imx},e^{-inx}})}.$$ These expressions are easy to compute: $$\\angles {e^{inx},e^{imx}} = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi e^{i(m-n)x}dx.$$ If $m=n$ , then the integrand is simply $1$ , so the integral is $\\frac{2\\pi}{\\pi} = 2$ . Otherwise, when $m-n\\neq 0$ we get that $$\\angles {e^{inx},e^{imx}} = \\frac{1}{\\pi} \\frac{1}{i(m-n)}e^{i(m-n)x} \\mid_{-\\pi}^\\pi = 0.$$ Proof: Using **Trigonometric identities**: Ignoring the complex integration, we can use directly the trigonometric equalities. Recall that the product of two cosine satisfies: $$\\cos(\\theta)\\cos(\\varphi) = \\frac {\\cos(\\theta-\\varphi)+\\cos(\\theta+\\varphi)}{2}.$$ Using this equality, we obtain that for $m,n\\geq 0$ we have that $$\\begin{align} \\angles {\\cos(mx),\\cos(nx)} & =\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\cos(mx)\\overline{\\cos(nx)} \\dx \\\\ & =\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\frac{\\cos((m-n)x)+\\overline{\\cos((m+n)x)}}{2} \\dx = \\begin{cases} 2&m=n=0\\\\ 1&m=n\\geq 1 \\\\ 0 & m\\neq n \\end{cases}. \\end{align} $$ A similar computation can be done for the rest of the inner products. Proof: Using **Integration tricks**: Finally, one more way is by using some basic integral operations in an interesting way. The integral above for $m=n=0$ is trivial, and in general it is symmetric, so let’s assume that $m\\neq 0$ . Using integration by parts, with the idea that integrating\\differentiating $\\cos(x)$ twice we return to the same place, we obtain the following: $$\\begin{align}\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\cos(mx)\\overline{\\cos(nx)} \\dx & = \\overbrace{\\frac{1}{m\\pi}\\sin(mx)\\cos(nx)\\mid _{-\\pi}^\\pi}^{=0} + \\frac {n}{m\\pi}\\int_{-\\pi}^\\pi \\sin(mx)\\sin(nx) \\dx\\\\& = -\\overbrace{\\frac {n}{m^2\\pi} \\cos(mx)\\sin(nx)\\mid_{-\\pi}^\\pi} +\\frac {n^2}{m^2\\pi} \\int_{-\\pi}^\\pi \\cos(mx)\\cos(nx) \\dx\\end{align}.$$ It then follows that $$\\left(1-\\frac{n^2}{m^2}\\right)\\frac {1}{\\pi}\\int_{-\\pi}^\\pi \\cos(mx)\\overline{\\cos(nx)} \\dx = 0,$$ so unless $m=n$ , the integral must be $0$ . As for the $m=n\\geq 1$ case, namely integrating $\\cos^2(nx)$ , you should first convince yourself that $\\int_{-\\pi}^\\pi \\cos^2(nx) \\dx = \\int_{-\\pi}^\\pi \\sin^2(nx) \\dx$ (why?). Given that, we obtain that $$2\\int_{-\\pi}^\\pi \\cos^2(nx) \\dx = \\int_{-\\pi}^\\pi \\cos^2(nx) \\dx+\\int_{-\\pi}^\\pi \\sin^2(nx) \\dx =\\int_{-\\pi}^\\pi 1 \\dx = 2\\pi,$$ so we conclude that for $n\\geq 1$ we have $$ \\angles {\\cos(nx),\\overline{\\cos(nx)}} = 1.$$ The importance of orthogonal, and in particular orthonormal bases, is that it is very easy to find the coefficients in these bases. Orthogonal projection At this point we already have one very interesting property of orthogonal (and better yet) orthonormal sets - it is very easy to find the coefficients for orthogonal bases. However, these sets have more interesting properties, and one of them is related to a very interesting problem that rises up naturally in many places: Problem: Given a point $v\\in V$ and a set $W\\subseteq V$ , find the distance from $v$ to $W$ , and if possible find the closest point to $v$ . It is not hard to see why this sort of problem appear in many places, since it basically looks for the shortest way from one point to get to some set. Just to make it even more interesting, sometimes this problem is hidden in other problems as well. For example, the famous problem of given a collection of points on the plane, and looking for the best linear approximation, is not only the same problem as above, but with the extra detail that $W$ is a subspace: When $W$ is a subspace, the solution in general is simple, where the main idea is what we expect to see in the standard Euclidean plane: True[[invalid wikilink: Fourier/images/orthogonality/orthogonal projection.png]] As the image suggests, the point $w\\in W$ closest to $v$ is its orthogonal (perpendicular) projection on $W$ . Finally, the most important part, is that this orthogonal projection is very easy to compute: Theorem: Orthogonal projection Let $W\\leq V$ be a finite dimensional subspace, and let $\\{w_1,...,w_k\\}$ be an orthonormal basis for $W$ . The orthogonal projection to $W$ is defined by $$P_W(v):=\\sum_1^k \\angles {v,w_i}w_i.$$ This orthogonal projection of $v$ is the closest point to $v$ in $W$ . Examples Example: Orthogonal step function approximation Let’s try to approximate the function $f(x)=x$ using the orthogonal step functions we defined previously, and we shall use only the first four such functions. (soon to be pictures) These four function form an orthogonal set, and in order to use the theorem we need to normalize them, namely $\\hat{f}_i := \\frac{f_i}{\\norm {f_i}}$ , so that $$ \\hat{f}_1 = f_1, \\;\\; \\hat{f}_2 = f_2, \\;\\; \\hat{f}_3 = \\sqrt{2}f_3, \\;\\; \\hat{f}_4 = \\sqrt{2}f_4.$$ The coefficients are then going to be $$\\angles{x,\\hat{f}_1}=\\frac{1}{2}, \\;\\; \\angles{x, \\hat{f}_2}=\\frac{1}{4}, \\;\\; \\angles{x, \\hat{f}_3} = \\angles{x, \\hat{f}_4} =\\frac{1}{16}. $$ This means that we get the approximation: $$ x \\sim \\frac{1}{2}\\cdot f_1 + \\frac{1}{4}\\cdot f_2 + \\frac{1}{16} \\cdot 2 \\cdot (f_3+f_4).$$ And finally, unsurprisingly, we get the approximation: Exercise Compute $\\angles{x, f_i}$ for any of the step function in the orthogonal basis. Do it one time directly using the integral, and one time “geometrically”. Try to conjecture how the approximation will look like, when taking all the functions with $p\\leq P$ for some positive integer $P$ , and then prove it. Try to do the same for other function (e.g. $f(x)=x^2$ ) - can you say in general how the approximation will look like? Note that in the example above we had $P=2$ since the corresponding segments of our functions were of length $1=1/2^0$ , $1/2$ and $1/2^2$ . Example: Approximation with sine waves. Similar to the previous example, we can try to approximate $f(x)=x$ using sine waves (explain why we don’t need cosines). Add some explanations… There was a very interesting phenomenon with the last approximation: The “distance” between the approximation and our original $f(x)=x$ function goes to zero. In our case this means that the integral $$\\int_{-\\pi}^\\pi (f(x)-2\\sum_1^N (-1)^{k+1} \\frac{\\sin(kx)}{k})^2 \\dx \\to 0.$$ In other words, the area (more or less, up to the square in the integrand) between the function goes to zero. It does not mean that there is a pointwise convergence! For example, in all of our approximations, the values in both $x=\\pm \\pi$ remain zero, while the value of the original function is $\\pm \\pi$ . However, we do have pointwise convergence in ALL the other points. This leads to a very interesting question, which we will later try to solve: Problem: When does the norm convergence implies also pointwise convergence? And if there isn’t pointwise convergence, then what can we say? Exexcise: Sine and cosine approximation To get some intuition about the last question, try to approximate some other “nice” function and try to guess the answer for this question for those functions. Try it on a range of families of functions like: constant, linear, polynomial, exponential, unbounded function, noncontinuous, etc. &lt;-Previous: Inner products , False[[invalid wikilink: Fourier Course Information#Table of contents]] , Next: Orthonormal basis -&gt;"
  },"/Fourier_Notes/notes/Fourier/Infinite%20Orthonormal%20basis": {
    "title": "Infinite Orthonormal basis",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Infinite%20Orthonormal%20basis",
    "body": "Course: Fourier Course Information Orthonormal basis - from finite to infinite The concept of a basis is fundamental in linear algebra (and generally in mathematics). Its formal definition is just a set of linearly independent vectors which span the given vector space. The other equivalent definition, was that $\\{v_1,...,v_n\\}$ is a basis for $V$ , if and only if any vector $v\\in V$ has a unique presentation as a linear combination $$v = \\sum_1^n \\alpha_i v_i.$$ This allowed us to identify between the abstract vector $v$ and the coordinate vector $(\\alpha_1,...,\\alpha_n)$ (Though remember that different bases produce different coordinate vectors). Moreover, when our vector space can be spanned by finitely many vectors, we have seen that not only there exists a finite basis to the space, but all of the bases have the same size, which we call the dimension of the vector space. This has led to all sorts of interesting results, for example a linearly independent set which is the size of the dimension of the space, is also spanning, and therefore a basis. When dealing with “non finite dimensional” vector spaces, we need to reconsider the definition of a basis. Suddenly finite sums might not be enough to describe the whole space. For simplicity, consider a countable infinite set $S=\\{v_i \\mid i\\in \\NN\\}$ , for which we ask whether it is spanning or linearly independent. There are two main approaches: The algebraic approach: Here we only know how to add finitely many vectors, so the set $S$ is spanning, if every vector is a finite linear combination of finitely many vectors in $S$ , and it is linearly independent if any finite subset of $S$ is linearly independent. The analytic approach: In special and interesting cases, we might be able to define infinite sums, namely $\\sum_1^\\infty \\alpha_i v_i$ , and then the definition is the analogue to what we have for finite sets. As the spaces we deal with are special and interesting, this will be our next step. Convergence in norm As usually, before talking about the limit of infinite sums, we need to define what are limits. Definition: Convergence in norm Let $V$ be a normed vector space. We say that a sequence of vectors $v_n$ converges in norm to $v$ in $V$ if $\\norm{v_n - v}\\to 0$ and we denote it by $v_n \\overset{\\norm{\\cdot}}{\\longrightarrow} v$ , or just $v_n \\to v$ if there is no ambiguity. Remark: Note that we have defined convergence with convergence, namely: $$v_n \\normto v \\iff \\norm{v_n -v}\\to 0.$$ This is not a cyclic definition, since the new norm convergence on the left is defined by the old known convergence of real numbers on the right. Before we go over a few examples to get some intuition, recall our three “main” norms on function spaces (say on $[0,1]$ ): $$\\align{\\norm{f}_1& = \\int_0^1|f(x)|\\dx \\\\ \\norm{f}_2& = \\sqrt{\\int_0^1|f(x)|^2\\dx} \\\\ \\norm{f}_\\infty& = \\sup_{0\\leq x\\leq 1} |f(x)|. } $$ The $\\mathcal{L}^2$ norm is the induced norm from the standard inner product, and comes with all the benefits of such a norm. Unless said otherwise, the “norm” convergences will mean that norm. While the $\\cl^1$ and $\\cl^\\infty$ are not induced norm, they have nice geometric intuition: The norm $\\norm{f}_1$ measures the area between $f$ and the $x$ -axis in absolute value. In particular two functions are close in this norm is the area between them is small. The norm $\\norm{f}_\\infty$ measures the “maximal” (supremum) distance of $f$ from the $x$ -axis. In particular, a function $f$ is close to $g$ in the infinity norm if it is in a small strip around $g$ (Add images) Example 1: The functions $f_n(x) = \\frac{x}{n}$ Looking at the picture, we guess that the functions should converge to the zero function, and this is indeed the case for all of the norms above. Indeed, we have that: $$\\align{\\norm{f_n}_1&=\\frac{1}{2n} \\to 0 \\\\ \\norm{f_n}_2&=\\frac{1}{\\sqrt{3n^2}} \\to 0 \\\\ \\norm{f_n}_\\infty &=\\frac{1}{n} \\to 0 .}$$ Example 2: The functions $f_n(x) = x^n$ This time, the functions look a bit more complicated, but still “seem” to be converging to the zero function. Let’s check if this is true: $$\\align{\\norm{f_n}_1&=\\frac{1}{n+1} \\to 0 \\\\ \\norm{f_n}_2&=\\frac{1}{\\sqrt{2n+1}} \\to 0 \\\\ \\norm{f_n}_\\infty &=1 \\not\\to 0 .}$$ This time, unlike the previous example, the sequence converge to zero both in the $\\mathcal{L}^1$ and $\\mathcal{L}^2$ but not in $\\mathcal{L}^\\infty$ . Excercise: Show that in the space $C[0,1]$ of continuous functions with the $\\cl^\\infty$ norm, the sequence $x^n$ of functions does not converge. Corollary: Different norm can have different convergence of the same sequence. Excercise: Show that for any $f\\in C[0,1]$ we have that $\\norm{f}_1 \\leq \\norm{f}_2 \\leq \\norm{f}_\\infty$ . Conclude that if $f_n$ converges in $\\cl^\\infty$ , then it converges in both $\\cl^1$ and $\\cl^2$ , and similarly if $f_n$ converges in $\\cl^2$ , then it converges in $\\cl^1$ . So intuitively, the infinity norm is the “strongest norm”: convergence there implies convergence “everywhere”. Pointwise convergence One more important type of convergence for functions which we learned about in calculus, which is not a norm convergence (why?) is the pointwise convergence: Definition: Pointwise convergence of functions We say that a sequence of functions $f_n$ converges pointwise in a set $X$ , if $f_n(x)$ is a convergent sequence for any $x\\in X$ . Lemma: If $f_n \\to f$ in $\\cl^\\infty$ (unifrom convergence) then $f_n \\to f$ pointwise. The last lemma is an interesting one, and helped quite a lot in calculus. It meant that whenever we want to look for uniform convergence, the only candidate was the pointwise limit (if exists) which is usually much easier to find. Unfortunately, there is no such connection with the induced norm. Example: Pointwise convergence doesn't imply $\\norm{\\cdot}_2$ convergence Consider the functions $f_n(x)=\\cases{n& x\\in(0,\\frac{1}{n}) \\\\ 0 & else}$ . Then this sequence converges pointwise to the zero function (check!) however $$\\norm {f_n - 0}_2^2 = \\int_0^{1/n} n^2 \\dx =n \\not \\to 0 . $$ Example: $\\norm{\\cdot}_2$ convergence doesn't imply pointwise convergence The idea is to build functions which are (1) have very small area, and (2) this area “shifts” around. Then (1) will imply norm convergence, and (2) will mean that no one point will actually converge. More specifically, our functions will be characteristic (step) functions which are 1 on segment of size $\\frac{1}{2^n}$ and zero otherwise, and we move around this segment. Formally, the first few functions are: $$\\align{f_1(x) \\equiv 1=\\chi_{[0,1]} \\\\ f_2(x)=\\chi_{[0,1/2]} \\; &, \\; f_3(x) = \\chi_{[1/2,1]} \\\\ f_4 = \\chi_{[0,1/4]} &, \\; f_5=\\chi_{[1/4,2/4]}\\;,\\; f_6=\\chi_{[2/4,3/4]}\\;,\\; f_6=\\chi_{[3/4,1]}}$$ Since our step functions always have height 1, then the normed squared equals to the area, which in turn equals to the width $\\frac{1}{2^n}\\to 0$ . On the other hand, this sequence doesn’t converge point wise to any point! Continuous functions Given an inner product space $V$ with the inner product $\\angles{\\cdot,\\cdot}$ , we now know how to generate a norm $\\norm{v}:=\\sqrt{\\angles{v,v}}$ , and from the previous section use this norm to define limits. Of course, once we have “limits” we can ask whether given functions are continuous, and fortunately our standard arithmetic functions are continuous. Theorem: Inner products are continuous Let $V$ be an inner product space and suppose that $v_n \\normto v$ and $u_n \\normto u$ . In addition, let $\\alpha_n \\to \\alpha$ in the field. Then the following functions are continuous: Addition: $v_n + u_n \\normto v + u$ . Negation: $-v_n \\normto -v$ . Scalar multiplication: $\\alpha_n v_n \\to \\alpha v$ . Inner product: $\\angles{v_n , u_n} \\to \\angles{v,u}$ . Norm: $\\norm {v_n} \\to \\norm {v}$ . Proof: All of the proofs are similar to the standard continuity proofs in $\\RR$ . Let’s prove as example only parts (4) and (5). Indeed, for part (4) we have $$\\align{|\\angles{v_n, u_n}-\\angles{v, u}| & \\leq |\\angles{v_n, u_n}-\\angles{v_n, u}| + |\\angles{v_n, u}-\\angles{v, u}| \\\\ &= |\\angles{v_n, u_n - u}| + |\\angles{v_n - v, u}| \\leq \\norm{v_n}\\norm{u_n-u} + \\norm{v_n-v}\\norm{u} = (*) ,} $$ where the last inequality is the Cauchy-Shwartz inequality. By assumption, we have that $\\norm{u_n-u} , \\norm{v_n-v} \\to 0$ . Since $\\norm{u}$ is constant and $\\norm{v_n} \\leq \\norm{v_n-v} + \\norm{v}$ is bounded, we conclude that $(*)$ converges to zero, which is exactly what we wanted to show. Part (5) now follows by taking $u_n=v_n$ , together with the fact that the square root function is continuous. $\\square$ Infinite sums and complete orthonormal bases Now that we have the definition of limits in our normed space, we can define infinite sums $\\sum_1^\\infty v_k$ as the limit (if exists) of the partial sums $\\sum_1^N v_k$ . Remark: Some notations issues Since we work with convergence of functions, where norm convergence and pointwise convergence are different, we will use $\\sum_1^\\infty \\angles{v,e_k}e_k \\sim v$ to indicate norm convergence and $\\sum_1^\\infty \\angles{v,e_k}e_k = v$ for pointwise convergence. Later we will see that they coincide in many cases. With this new infinite sum definition and the continuity we saw above, many of the results from finite dimensional spaces carry on naturally to infinite dimensional spaces. For example: Lemma: Coefficients of orthonormal sets Suppose that $\\{e_k|k\\in\\NN\\}$ is an orthonormal set, and $v=\\sum_1^\\infty \\alpha_k v_k$ . Then $\\alpha_n = \\angles{v, e_n}$ . Proof: We have that $$\\align{ \\angles{v, e_n} & = \\angles{\\sum_1^\\infty \\alpha_k e_k, e_n} = \\angles{\\limfi{N} \\sum_1^N \\alpha_k e_k, e_n} \\\\ &= \\limfi{N} \\angles{ \\sum_1^N \\alpha_k e_k, e_n}},$$ where in the last equality we used the continuity of inner products. Since $\\angles{ \\sum_1^N \\alpha_k e_k, e_n}=\\alpha_n$ for any $N\\geq n$ , we see that the limit is $\\alpha_n$ and we are done. With this in mind, we can define our new infinite orthonormal basis. Definition: An orthonormal basis Let $V$ be an inner product space, and $\\{e_k\\}$ an orthonormal set. We say that it is a orthonormal basis if every $v\\in V$ is the limit $\\sum_1^n\\angles{v, e_k}e_k \\normto v$ . Note that by the definition of a complete orthonormal basis each vector has a presentation $v\\sim\\sum_1^\\infty \\alpha_k e_k$ , and by the lemma above it is unique, namely $\\alpha_k=\\angles{v, e_k}$ , or in other words we again have a unique presentation for every vector, just like for finite dimensional vector spaces and their bases. We can now define the main orthonormal basis that we work with. Theorem: The Fourier system $E[-\\pi, \\pi]$ Let $V=E[-\\pi,\\pi]$ be the vector space of piecewise continuous functions on $[-\\pi,\\pi]$ , with the inner product $\\angles{f,g}:=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)\\overline{g(x)} \\dx$ . Then the functions $$\\{\\frac {1}{2}\\}\\cup\\{\\cos(nx) \\mid n\\geq 1\\} \\cup \\{\\sin(nx) \\mid n\\geq 1\\}$$ form a complete orthonormal basis. Proof: We already know that this system is orthonormal. The proof that it is “spanning” is more complicated and we will not prove in this course. However, for those interested, you should read about the Stone-Weierstrass theorem. We can now extend two of the results we know for finite dimensional inner product spaces to the infinite dimensional case: Claim: Let $V$ be an inner product space with an orthonormal system $\\{e_k \\mid k\\in\\NN\\}$ . Bessel inequality: For every $v\\in V$ we have $\\sum_1^\\infty |\\angles{v,e_k}|^2 \\leq \\norm{v}^2$ . Parseval identity: If the orthonormal system is a basis, then there is an equality $\\sum_1^\\infty |\\angles{v,e_k}|^2 = \\norm{v}^2$ for all $v$ . On the other hand, if there is such an equality for all $v$ , then the system is basis. Generalized Parseval identity: If the orthonormal system is a bsis and $u,v$ are any two vectors, then: $$\\angles{u,v} = \\sum_1^\\infty \\angles{u,e_i}\\overline{\\angles{v,e_i}}.$$ Proof: The claim here is in a sense the limit of the analogue claim for finite dimensional spaces, indeed, the main object here is none other than $$\\sum_1^\\infty |\\angles{v,e_k}|=\\limfi{N} \\sum_1^N |\\angles{v,e_k}| = \\norm{\\limfi{N} \\sum_1^N \\angles{v,e_k}e}.$$ Moreover, recall that these approximations are the orthogonal projections of $v$ to the spaces spanned by $\\{e_1,..., e_N\\}$ , so that we additionally have that $$\\sum_1^N \\angles{v,e_k}e \\perp (v-\\sum_1^N \\angles{v,e_k}e),$$ and therefore $$\\norm{v}^2 = \\norm{\\sum_1^N \\angles{v,e_k}e}^2 + \\norm{v-\\sum_1^N \\angles{v,e_k}e}^2 =(*).$$ The orthogonal projection in $(*)$ implies the standard finite dimensional Bessel inequality $\\sum_1^N |\\angles{v,e_k}|^2 \\leq \\norm{v}^2$ for any $N$ , and taking the limit produces the general Bessel inequality. Using $(*)$ we see that $\\norm{\\sum_1^N \\angles{v,e_k}e} \\to \\norm{v}$ if and only if $\\norm{v-\\sum_1^N \\angles{v,e_k}e} \\to 0$ or equivalently $\\sum_1^N \\angles{v,e_k}e \\normto v$ . Hence, Parsevel identity holds for all $v\\in V$ if and only if the system is complete. Finally, for an orthonormal basis, using the continuity of the inner product, we obtain that : $$\\align{\\angles{u,v}&=\\angles{\\sum_{i=1}^\\infty\\angles{u,e_i}e_i,\\sum_{j=1}^\\infty\\angles{v,e_j}e_j}\\\\&=\\sum_{i,j=1}^\\infty\\angles{u,e_i}\\overline{\\angles{v,e_j}}\\angles{e_i,e_j}=\\sum_{i=1}^\\infty\\angles{u,e_i}\\overline{\\angles{v,e_i}}}$$ Example: The function $f(x)=x$ Recall that we have already seen that $\\angles{x, \\sin(nx)}=2\\frac{(-1)^{n+1}}{n}$ . On the other hand, $$\\angles{x, \\cos(nx)} = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} x\\cos(x)\\dx 0$$ for all $n$ , since $x\\cos(nx)$ is an odd function. Thus we conclude that $x \\sim 2\\sum_1^\\infty \\frac{(-1)^{n+1}}{n} \\sin(nx)$ . Computing the Parseval identity, we obtain that $$\\frac{2\\pi^2}{3}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} x^2\\dx = \\sum_1^\\infty \\frac{4}{n^2}.$$ So while we already knew from calculus that $\\sum_1^\\infty \\frac{1}{n^2}$ converge, now we can also find out its limit $\\frac{\\pi^2}{6}$ . Lemma: The Riemann Lebesgue Lemma For an orthonormal basis $\\{e_k \\mid k\\in \\NN \\}$ and any vector $v$ we have that $\\angles{v,e_k}\\to 0$ . -Add intuition- Proof: We know that $\\norm v = \\sum_1^\\infty |\\angles{v,e_k}|^2$ so we must have that $\\limfi{n} |\\angles{v,e_k}| = 0$ . &lt;-Previous: Orthogonal sets , False[[invalid wikilink: Fourier Course Information#Table of contents]] , Next: Fourier series -&gt;"
  },"/Fourier_Notes/notes/Fourier/Fourier%20Series": {
    "title": "Fourier Series",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Fourier%20Series",
    "body": "Course: Fourier Course Information The Fourier series After reminding ourselves the notations and results from inner product spaces, and combining it with the norm convergence in our new infinite dimensional vector spaces, we are ready to study the Fourier transforms on periodic function. In the following sections we would like to study the inner product space $E[-\\pi,\\pi]$ of piecewise continuous function on $[-\\pi,\\pi]$ (or equivalently $2\\pi$ periodic functions on $\\RR$ ) with the inner product $$\\angles{f,g}=\\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x)\\overline{g(x)}\\dx.$$ Our orthonormal basis will consist of $$\\{\\frac{1}{\\sqrt{2}}, \\sin(x), \\cos(x), \\sin(2x), \\cos(2x),...\\}.$$ Remark: Equality between functions Recall that in our space $E[-\\pi,\\pi]$ we identify between two function $f,g$ if $f-g$ is zero almost everywhere (namely $f\\equiv g$ for all but finitely many points). In particular, if both $f$ and $g$ have the same Fourier series, then $f\\sim g$ are the same in $E[-\\pi, \\pi]$ (both equivalent to the same Fourier series) and therefore are equal almost everywhere. Definition: Fourier coefficients The Fourier coefficients of $f\\in E[-\\pi,\\pi]$ are $\\angles{f,\\frac{1}{\\sqrt{2}}}$ , $\\angles{f,\\sin(nx)}$ , and $\\angles{f,\\cos(nx)}$ for all $n\\geq 1$ . In particular we have the classic Fourier series expansion: $$f \\sim \\sum \\angles{f,\\frac{1}{\\sqrt{2}}}\\frac{1}{\\sqrt{2}} + \\sum_{n=1}^\\infty \\left( \\angles{f,\\cos(nx)}\\cos(nx) + \\angles{f,\\sin(nx)}\\sin(nx)\\right).$$ When there is no ambiguity, we will use the notation: $$\\align{a_0&:=\\angles{f,\\cos(0x)}=\\sqrt{2}\\angles{f,\\frac{1}{\\sqrt{2}}} \\\\a_n&:=\\angles{f,\\cos(nx)} \\; , \\text{for } n\\geq 1\\\\b_n&:=\\angles{f,\\sin(nx)} \\; , \\text{for } n\\geq 1,}$$ for which the Fourier series looks like: $$f \\sim \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(nx) + b_n\\sin(nx)\\right).$$ Before you look at the examples below (which are $f(x)=x,\\;|x|,\\;\\chi_{[-\\pi,0]}-\\chi_{[0,\\pi]}$ ), try to compute some Fourier series for simple and nice functions (and in particular for these examples). To visualize these result, you can use the following Desmos program: Enter the $a_0$ in the simple $a_0$ cell. The syntax for the $a_n$ (and similarly $b_n$ ) should be a = a_n for n=[1,...,k] To plot steps functions like $f(x)=\\chi_{[-\\pi,0]}-\\chi_{[0,\\pi]}$ use the notation f={-pi&lt;x&lt;0:-1 , 0&lt;=x&lt;pi:1} or more generally {condition1: value1, condition2: value2, ...} Once you finish putting in the coefficients and choosing your function $f(x)$ , you can close the menu and choose the approximation level using the $k=?$ slider below the graph. You can also go directly to the Desmos site here. Examples: Fourier series $f(x)=x$ : In this case, we already saw that $b_n=2\\frac{(-1)^{n+1}}{n}$ and $a_n=0$ for all $n$ (since $x\\cos(nx)$ is an odd function). $f(x)=\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ : This is also an odd function, so that $a_n=0$ for all $n$ . As for the $b_n$ , the product $f(x)\\sin(nx)$ is even, so we have that $$\\align{b_n & = \\angles{\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]},\\sin(nx)} = \\frac{1}{\\pi} \\int_0^\\pi \\sin(nx) \\dx - \\frac{1}{\\pi} \\int_{-\\pi}^0 \\sin(nx) \\dx \\\\ & = \\frac{2}{\\pi} \\int_0^\\pi \\sin(nx) \\dx = \\frac{2}{n\\pi} \\cos(nx)\\mid_0^\\pi=\\frac{2}{n\\pi}\\left((-1)^n-1\\right).}$$ It follows that $b_{2n}=0$ while $b_{2n+1}=\\frac{4}{(2n+1)\\pi}$ . All in all, we get that $$\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}\\sim\\sum_0^\\infty \\frac{4}{(2n+1)\\pi} \\sin((2n+1)x).$$ $f(x)=|x|$ : Now we have an even function, so that $b_n=0$ for all $n$ . Similarly, since $f(x)\\cos(nx)$ is even we get: $$\\align{a_n&=\\angles{|x|,\\cos(nx)}=\\frac{2}{\\pi} \\int_0^\\pi x\\cos(nx)\\dx \\\\ &= \\frac{2}{n\\pi} \\left( x\\sin(nx)\\mid_0^\\pi+ \\int_0^\\pi \\sin(nx)\\dx\\right)=\\frac{2}{n^2\\pi}\\left((-1)^n-1\\right)}.$$ It follows that $a_{2n}=0$ while $a_{2n+1}=\\frac{-4}{(2n+1)^2\\pi}$ . However, if $n=0$ then the computation above is not well defined as we divided by zero! So for this case we need a separate computation, we produces $$\\angles{|x|,1}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}|x|\\dx = \\pi.$$ Finally, we get that $$|x|\\sim \\frac{\\pi}{2} - \\sum_1^\\infty \\frac{4}{(2n+1)^2\\pi} \\cos((2n+1)x).$$ $f(x)=\\sin^2(x)$ : While we can compute all the coefficients using the integrals, here we can use another method. Recall that $\\sin(x)=Im(e^{ix})=\\frac{e^{ix}-e^{-ix}}{2i}$ . Using this identity, we see that $$\\align{\\sin^2(x) &= \\left(\\frac{e^{ix}-e^{-ix}}{2i}\\right)^2 = \\frac{e^{2ix}-2+e^{-2ix}}{-4} \\\\ & = \\frac{\\cos(2x)-1}{-2}= \\frac{1}{2}-\\frac{\\cos(2x)}{2}.}$$ This is already Fourier series expression of $f(x)$ , and since there is always a unique such combination, this is the Fourier series of $f(x)$ . Pointwise convergence In our Fourier series $f \\sim \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(nx) + b_n\\sin(nx)\\right)$ , the limit on the right is the norm convergence, and we have already seen that it doesn’t imply pointwise convergence. However, as can be seen in the Fourier series in the step function $\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ above, in most of the points we do have pointwise convergence. Moreover, the only points where it doesn’t seem to have pointwise convergence are the two edge points at $\\pm \\pi$ and in $0$ . These are exactly the “problematic” points where the function is not continuous. Leaving the edges aside for now, where does the Fourier series converges at the point $x=0$ ? This is actually quite simple to find, since $$\\sum_0^\\infty \\frac{4}{(2n+1)\\pi} \\overbrace{\\sin((2n+1)0)}^{=0}=0.$$ More over, the intuition is that the approximation seems to converge to $-1$ just to the left of $x=0$ and to $+1$ just to the right of $x=0$ , and more or less go straight from $-1$ to $+1$ in the middle. In other words, the value at $x=0$ is exactly the average of the two values to its left and to its right! We actually see something similar at the edge points $\\pm \\pi$ . If we think of $f$ as a periodic function, then as it passes through the point $x=\\pi$ it jumps from $+1$ to $-1$ , which again has average $0$ which is where our blue approximation function is at. These observation are true in a more generalized setting where the functions are “smooth enough”. However, since we don’t even assume that our functions are continuous, we need a slightly different definition than just saying differentiable functions. Definition: Half derivative Let $f\\in E[-\\pi,\\pi]$ and denote the right limit at a point by $f(a^+)=\\displaystyle{\\lim_{x\\to a^+}}f(x)$ We define the right half derivative as the limit, if it exists, of $$\\displaystyle{\\lim_{x\\to a^+}}\\frac{f(x)-f(a^+)}{x-a}.$$ Similarly we define the left half derivative. For simplicity of notation, we identify between the edge points $\\pm \\pi$ and consider its left and right half derivatives as the left half derivative at $\\pi$ and right half derivative at $-\\pi$ . We denote by $E'[-\\pi,\\pi]\\subseteq E[-\\pi,\\pi]$ all the piecewise continuous function on $[-\\pi,\\pi]$ that have left and right half derivative at every point in $[-\\pi,\\pi]$ We this new definition of “differentiable” function, we have the following: Theorem: Dirichlet's theorem for pointwise convergence Let $f\\in E'[-\\pi,\\pi]$ . Then at a given point $\\lambda \\in(-\\pi,\\pi)$ the Fourier series converges pointwise to the average of the left and right limits, namely $$\\frac{f(\\lambda^+)+f(\\lambda^-)}{2} = \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(n\\lambda) + b_n\\sin(n\\lambda)\\right).$$ At the edge points $\\lambda =\\pm \\pi$ , it converges to $\\frac{f(-\\pi^+)+f(\\pi^-)}{2}$ . In particular, the series converges pointwise at each point where $f$ is continuous. Remark: Gluing the edges. As the last theorem suggest, we should actually think about the edge points $\\pm \\pi$ as a single point. Example: We already saw that this theorem holds for the discontinuities of $f=\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ , namely the pointwise convergence there are to the average of the left and right limits. Let’s consider the pointwise convergence at $x=\\frac{\\pi}{2}$ . Since $f(x)$ is continuous there and $f(\\frac{\\pi}{2})=1$ , this is going to be the limit. Writing the pointwise convergence of the Fourier series, we get: $$1=\\sum_0^\\infty \\frac{4}{(2n+1)\\pi} \\sin((2n+1)\\frac{\\pi}{2})=\\frac{4}{\\pi}\\sum_0^\\infty \\frac{(-1)^n}{2n+1} .$$ Try to prove this using other methods (clue: $\\arctan(1)=\\frac{\\pi}{4}$ ). As another example, recall that the approximation for $f(x)=x$ looks like: True[[invalid wikilink: Pasted image 20240131120746.png]] Here the function is “continuous” except for the edges, where it jumps from $f(\\pi^-)=\\pi$ to $f(-\\pi^+)=-\\pi$ . The average is of course $\\frac{f(-\\pi^+)+f(\\pi^-)}{2}=0$ , and the pointwise convergence at these points are $$2\\sum_{k=1}^{\\infty}\\left(-1\\right)^{k+1}\\overbrace{\\frac{\\sin\\left(\\pm k\\pi\\right)}{k}}^{=0}=0.$$ Excercise: Compute the Fourier series for $f(x)=\\chi_{[0,\\pi]}$ . Where are its discontinuity points, and what should be the pointwise convergence there for the Fourier series? Uniform convergence We have now seen that for a “smooth” enough function $f(x)$ , our Fourier series not only converges in norm but also pointwise (more or less). Can we strengthen this to a uniform convergence? In general, the answer is no. Our Fourier approximations are continuous (combination of sine and cosine functions), and the uniform limit of continuous functions is again continuous. So unless $f(x)$ by itself is continuous, this is false. However, if $f(x)$ is continuous and “smooth” enough, then we do actually have uniform convergence. Before stating the theorem and proving it, recall one of the main tools to prove uniform convergence: Theorem: Weierstrass theorem for uniform convergence Let $f_n:I\\to \\CC$ be a sequence of functions. If $\\sum_1^\\infty M_n for some sequence of numbers satisfying $\\sup_x |f_n(x)|\\leq M_n$ , then $\\sum_1^\\infty f_n(x)$ converges uniformly and in absolute value. In our case, we are dealing with sums of the form $$\\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n\\cos(nx) + b_n\\sin(nx)\\right).$$ Since the sine and cosine functions are bounded by $1$ , using Weierstrass’ theorem, if $$\\sum_1^\\infty \\left(|a_n|+|b_n|\\right) then the Fourier series will converge uniformly. We already know that this sum is “not too large”, since Bessel inequality implies that $$\\sum_1^\\infty \\left(|a_n|^2+|b_n|^2\\right)\\leq \\norm{f}^2,$$ however, this is not enough in general to show that the sum without the square is finite (for example, if $a_n=b_n=\\frac{1}{n}$ ). It turns our that the “smoother” $f$ is, the faster both $a_n$ and $b_n$ converge to zero, and the better the chance for the series above to converge. In particular, we have the following result for “smooth” functions. Theorem: Term by term derivatives Suppose that $f\\in C[-\\pi, \\pi]$ is continuous and $f(\\pi)=f(-\\pi)$ and that $f'(x)\\in E[-\\pi,\\pi]$ (and in particular $f'(x)$ exists except for finitely many points). Then writing the Fourier series of $f$ as: $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)),$$ we have that the Fourier series of $f'$ is the element wise derivative: $$f'(x)\\sim \\sum_1^\\infty (-na_n \\sin(nx)+ nb_n\\cos(nx)).$$ Note that going the other way around, given the $n$ -th Fourier coefficients of $f'$ , we can obtain the $n$ -th Fourier coefficients of $f$ by dividing by $n$ , so they converge to zero very fast. In particular we have the following: Corollary: Uniform convergence Under the conditions of the Theorem above, the Fourier series of $f$ converges uniformly to $f$ . Proof: Denote by $a_n,b_n$ , and $\\tilde{a}_n,\\tilde{b}_n$ the Fourier coefficients of $f(x)$ and $f'(x)$ respectively. By the theorem we know that $|a_n|=\\frac{|\\tilde{n}_n|}{n}, |b_n|=\\frac{|\\tilde{a}_n|}{n}$ for $n\\geq 1$ . Using this fact and the Cauchy Shwarz inequality, we get $$\\sum_1^N (|a_n|+|b_n|)=\\sum_1^N \\frac{1}{n}(|\\tilde{a}_n|+|\\tilde{b}_n|)\\leq \\sqrt{\\sum_1^N \\frac{2}{n^2}}\\sqrt{\\sum_1^N\\left( |\\tilde{a}_n|^2+|\\tilde{b}_n|^2\\right)}.$$ If we can show that the last expression is bounded (and therefore converge) we could use Weierstrass theorem to prove uniform convergence. We already know that $\\sum_1^\\infty \\frac{1}{n^2}$ is finite (and even know how to compute it), and for the other term we can use Bessel’s inequality to obtain: $$\\sum_1^N \\left(|\\tilde{a}_n^2| + |\\tilde{b}_n^2|\\right) \\leq \\norm{f'},$$ which completes the proof. Example: $f(x)=|x|$ This function is periodic continuous, and its derivative is $f'=\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ which is in $E[-\\pi,\\pi]$ so we can apply the last lemma. We already computed its Fourier series: $$|x|\\sim \\frac{\\pi}{2} - \\sum_1^\\infty \\frac{4}{(2n+1)^2\\pi} \\cos((2n+1)x).$$ Since $$\\sum_1^\\infty\\sup_x|\\frac{4}{(2n+1)^2\\pi} \\cos((2n+1)x)|\\leq \\sum_1^\\infty \\frac{4}{(2n+1)^2\\pi} we can use Weierstrass’ theorem to conclude that its convergence is indeed uniform. Furthermore, taking the term by term derivatives, we obtain another Fourier series that we already saw, namely: $$\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]} \\sim \\sum_1^\\infty \\frac{4}{(2n+1)\\pi} \\sin((2n+1)x).$$ Note that the Fourier coefficients of $\\chi_{[0,\\pi]}-\\chi_{[-\\pi,0]}$ (which is not continuous) behave like $\\frac{1}{n}$ while the coefficients of the continuous $|x|$ behave like $\\frac{1}{n^2}$ . Proof of term by term derivatives: The main idea of the proof is to move between the Fourier coefficients of $f$ and $f'$ via integration by parts. More specifically, write the Fourier series of both $f$ and $f'$ : $$\\align{f(x)& \\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx))\\\\ f'(x) &\\sim \\frac{\\tilde{a}_0}{2} + \\sum_1^\\infty (\\tilde{a}_n \\cos(nx)+ \\tilde{b}_n\\sin(nx))}$$ Using integration by parts we have that $$\\align{\\tilde{a}_n &= \\angles{f',\\cos(nx)}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f'(x)\\cos(x)\\dx\\\\ &=\\frac{1}{\\pi}\\left(f(x)\\cos(nx)\\mid_{-\\pi}^{\\pi} - \\int_{-\\pi}^{\\pi}nf(x)\\sin(nx)\\dx \\right)\\\\&=\\overbrace{(f(\\pi)-f(-\\pi))}^{=0}(-1)^n+n\\angles{f,\\sin(nx)} = nb_n.}$$ A similar computation shows that $\\tilde{b}_n = -na_n$ . This shows that we can take the derivative element wise. Excercise: Suppose that we are given $f$ as in the theorem, however, we don’t know if $f(\\pi)=f(-\\pi)$ . Find the Fourier coefficients of $f'(x)$ . Try to use the last theorem, instead of reproving it. We saw that we can take the derivative term by term. Similarly, we can also take the integral element wise in the Fourier series. Remark: Integrals vs Derivative We usually study derivatives before integrals, and we sometimes get the feeling that they are easier. While this is usually true when trying to actually compute these functions, when speaking about the results, the integral, namely the antiderivative of a function is usually much “better” than the derivative of a function. Indeed, if we measure a function by how smooth it is, namely how many derivatives it has, then we always get that the anti derivative has at least (!) one derivative. Theorem: Term by term integrals Suppose that $f\\in E[-\\pi, \\pi]$ and denote its Fourier series by $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ Then for any $[c,d]\\subseteq [-\\pi,\\pi]$ we have the term by term integral: $$\\int_c^d f(t)\\dt = \\frac{a_0(d-c)}{2} + \\sum_1^\\infty (\\frac{a_n}{n} (\\sin(nd)-\\sin(nc)) - \\frac{b_n}{n}(\\cos(nd)-\\cos(nc)).$$ Proof: The main step here is to note that we can write $$\\align{\\int_c^d f(t) \\dt &= \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(t)\\pi\\chi_{[c,d]}\\dt = \\angles{f,\\pi\\chi_{[c,d]}} \\\\ & =\\angles{\\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)),\\pi\\chi_{[c,d]}}.}$$ Using the fact that the inner product is continuous (with respect to the $\\norm{\\cdot}_2$ norm), we can take the summation outside the integral to obtain: $$\\align{\\int_c^d f(t) \\dt &=\\angles{\\frac{a_0}{2}, \\pi\\chi_{[c,d]}} + \\sum_1^\\infty\\angles{a_n \\cos(nx)+ b_n\\sin(nx),\\pi\\chi_{[c,d]}} \\\\ & = \\int_c^d\\frac{a_0}{2} \\dt + \\sum_1^\\infty\\int_c^d(a_n \\cos(nx)+ b_n\\sin(nx))\\dt \\\\ & = \\frac{a_0(d-c)}{2} \\dt + \\sum_1^\\infty(\\frac{a_n}{n} (\\sin(nd)-\\sin(bc))- \\frac{b_n}{n}(\\cos(nd)-\\cos(nc))\\dt.}$$ Is the indefinite integral differentiable? Let $f(x)=|x|$ which is continuous, and define $$F(x)=\\int_{-\\pi}^{x} f(t)\\dt-\\frac{\\pi^2}{2},$$ so that $F'(x)=f(x)$ . What can you say about the convergence of the Fourier series of $F(x)$ ? What is it $\\cl_2$ , $\\cl_\\infty$ and pointwise limit? Check to see if your guess is correct. Proof: It is not hard to show that $$F(x)=\\cases{-\\frac{x^2}{2}&x\\leq 0\\\\ \\frac{x^2}{2} & x\\geq 0}.$$ Write it Fourier series as $$F(x) \\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ Since $F(x)$ is an odd function, we immediately get that $a_n=0$ for all $n$ . As for $b_n$ , using symmetry and integration by parts we get $$\\align{b_n&=\\angles{F(x),\\sin(nx)}=\\frac{2}{\\pi}\\int_0^\\pi \\frac{x^2}{2} \\sin(nx) \\dx \\\\ & = -\\frac{2}{\\pi n} \\left[ \\frac{x^2\\cos(nx)}{2}\\mid_0^\\pi -\\int_0^\\pi x\\cos(nx)\\dx \\right] = -\\frac{2}{\\pi n} \\left[ \\frac{\\pi^2(-1)^n}{2} - \\int_0^\\pi x\\cos(nx)\\dx \\right].}$$ A second integration by parts gives: $$\\int_0^\\pi x\\cos(nx)\\dx = x\\frac{\\sin(nx)}{n}\\mid_0^\\pi - \\int_0^\\pi \\frac{\\sin(nx)}{n}\\dx=\\frac{\\cos(nx)}{n^2}\\mid_0^\\pi=\\frac{(-1)^n-1}{n^2}.$$ All together we have that $b_n=-\\frac{2}{\\pi n} \\left[ \\frac{\\pi^2(-1)^n}{2} + \\frac{(-1)^n-1}{n^2} \\right]$ . Considering the converge of the Fourier series, since $F\\in E[-\\pi, \\pi]$ we automatically get the $\\cl_2$ -norm convergence. Also, the function is continuous “everywhere” (except the edge points) and has half derivatives, so Dirichlet’s theorem applies and we have pointwise convergence in $(-\\pi,\\pi)$ . On the edges, the pointwise convergence is to the average: $$\\frac{\\displaystyle{\\lim_{x\\to-\\pi+}F(x)+\\lim_{x\\to\\pi-}F(x)}}{2}=\\frac{\\pi^2/2-\\pi^2/2}{2}=0.$$ The function $F(x)$ is not continuous at the edges, since $F(-\\pi)\\neq F(\\pi)$ so we can’t use the theorem about uniform convergence. In general the fact that the conditions of the theorem don’t hold doesn’t mean that its result doesn’t hold. However, in this case it is true, since if there is uniform convergence in $[-\\pi,\\pi]$ , then considering the functions as $2\\pi$ periodic there will be uniform convergence everywhere, so the limit (of continuous function) is going to be continuous. As this is not true for $F$ , the convergence is not uniform. The last exercise shows that taking the integral of a continuous function doesn’t necessarily gives us a smooth function, since there can be issues with the edge points. However, we can fix these problems in a way. Excercise: Let $f_0(x)=sign(x)$ which is not continuous. Show that there are $f_1(x),f_2(x)$ such that $f_2'(x)=f_1(x)$ and $f_1'(x)=f_0(x)$ in almost every $x$ and both $f_1,f_2$ are continuous (including edge points). Proof: The main idea is to start with $|x|$ as before, and to change it by a scalar to $f_1(x)=|x|+c$ . This keeps $f_1(x)$ as a continuous function with $f_1'(x)=f_0(x)$ . Let $F$ be as in the previous exercise, and define $$F_C(x)=\\int_{-\\pi}^{x} (f_1(t)+C)\\dt-\\frac{\\pi^2}{2} = F(x)+Cx.$$ The function $F_C(x)$ is still continuous in $(-\\pi,\\pi)$ and $F_C'(x)=f_1(x)$ . Also, it limit at the edge points are $$\\lim_{x\\to-\\pi^+}F_C(x)=-\\frac{\\pi^2}{2}-C\\pi\\;\\;;\\;\\;\\lim_{x\\to\\pi^-}F_C(x)=\\frac{\\pi^2}{2}+C\\pi.$$ Choosing $C=-\\frac{\\pi}{2}$ will cause them to be equal and therefore make $F_C(x)$ continuous. Example: Fourier in differential \\\\ integral equations Let’s try to find continuous $f$ solving the equation $$ \\int_0^x f(t)\\dt = x - f'(x) .$$ If the equation above holds, then in particular $f'(x)$ is by itself defined and continuous. Let’s write the Fourier expansion of $f$ as $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ We would like to take the term by term derivative, but not all the conditions of the theorem hold - we don’t know that $f(-\\pi)=f(\\pi)$ . However, we can assume that it is true and continue on, but we need to check that our final solution does satisfy this condition (or alternatively, it satisfies the equation we started with). Under this assumption, we can write: $$f'(x)\\sim \\sum_1^\\infty (-n a_n \\sin(nx)+ n b_n\\cos(nx)).$$ For any function in $E[-\\pi,\\pi]$ we can do term by term integration, so putting everything together we obtain the equation: $$\\frac{a_0x}{2} + \\sum_1^\\infty (\\frac{a_n}{n} \\sin(nx)- \\frac{b_n}{n}(\\cos(nx)-1)) = x - \\sum_1^\\infty (-n a_n \\sin(nx)+ n b_n\\cos(nx)).$$ Moving everything to the same side, we get $$\\left(\\frac{a_0}{2}-1\\right)x + \\sum_1^\\infty \\frac{b_n}{n} + \\sum_2^\\infty \\left( a_n(\\frac{1}{n}-n) \\sin(nx) - b_n(\\frac{1}{n}-n\\right)\\cos(nx))=0.$$ If all the “coefficients” are zero, then of course it solves the equation. However, note that the last equation is not a Fourier expansion (namely, it is not a linear combination of a basis), so we don’t know automatically that this is the only solution. In any case, one way to make all the coefficient zero is by taking: $$a_0=2,\\; a_n =0\\; \\forall n\\geq 2,\\; b_n =0 \\; \\forall n\\geq 1.$$ We are now left with $f(x)=1+a_1\\cdot\\cos(x)$ , and note that the functions in this family do satisfy $f(-\\pi)=f(\\pi)$ . Finally, just to be on the safe side, let’s see that they do satisfy our original equation: $$\\align{\\int_0^x f(t)\\dx & = \\int_0^x (1+a_1\\cos(t))\\dx = x + a_1 \\sin(t)\\mid_0^x = x+a_1\\sin(x)\\\\ x-f'(x) & = x-(1+a_1\\cos(x))' = x + a_1\\sin(x)}.$$ Convergence summarization Let’s summarize our convergence results in the space $E[-\\pi,\\pi]$ : Norm and pointwise convergence implications for sequence $f_n$ of functions: $$\\align{\\norm{\\cdot}_\\infty & \\Rightarrow \\norm{\\cdot}_2 \\Rightarrow \\norm{\\cdot}_1 \\\\ \\norm{\\cdot}_\\infty&\\Rightarrow \\text{pointwise}.}$$ Neither the pointwise of $\\norm\\cdot_2$ convergence imply one another. For any $f\\in E[-\\pi,\\pi]$ there is $\\norm\\cdot_2$ convergence of the Fourier series: $$f(x)\\sim \\frac{a_0}{2} + \\sum_1^\\infty (a_n \\cos(nx)+ b_n\\sin(nx)).$$ The coefficients are determined uniquely by $f$ , and if two functions $f,g$ have the same Fourier coefficients, then they are the same in $E[-\\pi,\\pi]$ , namely $f(x)=g(x)$ except for finitely many points. If $f$ has half derivatives at every point, then the Fourier series converges point wise to the averages: $$\\frac{f(x^+)+f(x^-)}{2}.$$ In particular, if $f$ is continuous at $x$ , then the Fourier series converges pointwise to $f(x)$ . For any $f\\in E[-\\pi,\\pi]$ we can do term by term integration of the Fourier series. If $f$ is continuous (including edge points) and $f'\\in E[-\\pi,\\pi]$ , then we can also take derivatives term by term. &lt;-Previous: Orthonormal basis , False[[invalid wikilink: Fourier Course Information#Table of contents]] , Next: Fourier Transform -&gt;"
  },"/Fourier_Notes/notes/Fourier/Fourier%20Transform": {
    "title": "Fourier Transform",
    "keywords": "Fourier Analysis",
    "url": "/Fourier_Notes/notes/Fourier/Fourier%20Transform",
    "body": "Course: Fourier Course Information Up until now we talked about functions on $[-\\pi,\\pi]$ , or equivalently $2\\pi$ periodic functions. We now shift our focus to general function on $\\RR$ . Inner products The first issue we encounter with these general functions, is that the corresponding inner products is on an infinite segment : $$\\angles{f,g}:=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}f(x)\\overline{g(x)}\\dx.$$ This integral can diverge even for very nice functions, and in particular $\\angles{1,1}=\\infty$ . To solve this problem we consider a subfamily of function, which is large enough to contain most of the functions that we work on. Definition: Norms on functions on $\\RR$ For a function $f:\\RR \\to \\CC$ we define: $$\\begin{align} \\norm f_1 & = \\int_{-\\infty}^{\\infty} \\abs{f(x)} \\dx \\\\ \\norm f_2 & = \\frac{1}{2\\pi} \\left(\\int_{-\\infty}^{\\infty} \\abs{f(x)}^2 dx\\right)^{1/2} \\\\ \\norm f_\\infty & = \\sup_x \\abs{f(x)}. \\end{align}$$ For $p=1,2,\\infty$ we denote by $$E^p(\\RR) = \\{f:\\RR \\to \\CC \\;\\mid\\; f \\text{ piecewise continuous}, \\norm f_p In the $[-\\pi,\\pi]$ finite case, we have that $\\norm f_1 \\leq \\norm f_2 \\leq \\norm f_\\infty$ (maybe up to a scalar normalization). This is no longer true in the infinite case, for example $f\\equiv 1$ has $\\norm f_2 = \\norm f_1 =\\infty$ while $\\norm f_\\infty =1$ . Actually, you can show that: Excercise: For each $p\\in\\{1,2,\\infty\\}$ find a function $f$ such that $\\norm f_p , while the other two norms are infinity. We already saw that the inner product is not well defined for just bounded functions in $E^\\infty (\\RR)$ , but it is well defined for function in $E^2(\\RR)$ . Lemma: Inner product on $E^2(\\RR)$ The map $\\angles{f,g}$ defined above is an inner product on $E^2(\\RR)$ . Proof: Once we know that $\\angles{f,g}$ is well defined, the rest of properties of inner products are easy to prove. We already saw in Inner product spaces - a reminder that the integral converges, but as a reminder, because $\\abs{f(x)\\overline{g(x)}}\\leq \\abs{f(x)}^2+\\abs{g(x)}^2$ , we get that the integral even converges in absolute value since $$\\int_{-\\infty}^\\infty\\abs{f(x)g(x)}\\dx \\leq \\norm{f(x)}_2^2+\\norm{g(x)}_2^2 We can now look for an orthonormal basis and do a similar process as with the Fourier series. However, our main goal is to study functions through their periodicity (namely translation to the left and right), and the only periodic function which is in $E^2(\\RR)$ is the zero function. Instead, we will just “extend” what we saw in the finite segment case to all of $\\RR$ . The Fourier Transform in $E^1(\\RR)$ Definition: The Fourier transform For a function $f:\\RR \\to \\CC$ and $\\omega \\in \\RR$ we write: $$\\hat f(\\omega) := \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx = \\angles{f,e^{i\\omega x}}.$$ If this limit exists for every $\\omega \\in \\RR$ , we call $\\hat{f}:\\RR \\to \\CC$ the Fourier transform of $f$ . We sometimes denote this transform by $\\hat{f} = \\mathcal{F}[f]$ . We will soon see that this transform is well defined for our functions in $E^1(\\RR)$ , but first, let’s see some examples. Example: The function $\\chi_{[a,b]}$ . For $a define the characteristic function: $$f(x)=\\chi_{[a,b]}(x)=\\cases{1&x\\in[a,b]\\\\0&else}.$$ Computing its Fourier transform, we have: $$2\\pi \\hat{f}(w)= \\int_{-\\infty}^{\\infty} \\chi_{[a,b]}(x)e^{-i\\omega x}dx = \\int_{a}^{b} e^{-i\\omega x}dx =\\frac{1}{\\omega} ie^{-i\\omega x}\\mid_a^b = \\frac{e^{-i\\omega a}-e^{-i\\omega b}}{\\omega i}. $$ Note that the computation above is invalid when $\\omega=0$ for which we have: $$\\hat{f}(0)=\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\chi_{[a,b]}(x)dx=\\frac {b-a}{2\\pi}. $$ The function $\\hat f(\\omega)$ is easily seen to be continuous for $\\omega \\neq 0$ , but you can also check that it is continuous at $\\omega = 0$ as well. Also, since $e^{i\\omega x}$ is bounded, as $\\omega \\to \\infty$ we see that $\\hat{f}(\\omega)\\to 0$ More over, when $a=-b$ is symmetric, the expression above is the real function: $$\\hat{f}(\\omega)=\\cases{\\frac{\\sin(\\omega b)}{\\omega \\pi} & \\omega \\neq 0 \\\\ \\frac{b}{\\pi} & \\omega = 0.}$$ Example: The function $e^{-a|x|}$ , for $a>0$ . First we note that $f(x):=e^{-a|x|}\\in E^1(\\RR)$ . Next, we have that $$\\begin{align} 2\\pi \\cdot \\hat f(\\omega) & = \\int_{-\\infty}^{\\infty} e^{-a|x|} e^{-i\\omega x}\\dx = \\int_{-\\infty}^{0} e^{(a-i\\omega) x}\\dx + \\int_{0}^{\\infty} e^{(-a-i\\omega) x}\\dx \\\\ & = \\int_{0}^{\\infty}\\left(e^{-(a-i\\omega)x}+e^{(-a-i\\omega)x}\\right)\\dx=\\left[\\frac{e^{-ax}e^{i\\omega x}}{i\\omega-a}-\\frac{e^{-ax}e^{-i\\omega x}}{i\\omega+a}\\right]_{0}^{\\infty}. \\end{align}$$ Since $e^{i\\omega x}$ is bounded by 1, and $e^{-ax}\\to 0$ as $x\\to\\infty$ , we conclude that $$2\\pi \\cdot \\hat f(\\omega) = \\frac{1}{i\\omega+a}-\\frac{1}{i\\omega-a} = \\frac{2a}{\\omega^2+a^2}. $$ Basic properties Before proving some basic results about the Fourier transform, we need to actually show that it is well defined for our functions, namely that $\\angles{f,e^{i\\omega x}}$ is finite when $f\\in E^1(\\RR)$ . Lemma: If $f\\in E^1(\\RR)$ , then $\\hat{f}(\\omega)$ is well defined for all $\\omega$ and $\\norm {\\hat{f}}_\\infty \\leq \\norm f_1$ . Proof: This is a simple upper bound computation: $$\\abs{\\hat{f}(\\omega)}=\\abs{\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx} \\leq \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\abs{f(x)}\\abs{e^{-i\\omega x}}dx=\\frac{1}{2\\pi} \\norm f_1.$$ Remark: Generalized inner products As we mentioned before, the “inner product” $\\angles{f,e^{i\\omega x}}$ is no longer a standard an inner product, however there is some method to this madness. This is is a product between a function in $L^1(\\RR)$ and $L^\\infty(\\RR)$ , where it is usually between two functions in $L^2(\\RR)$ . These two pairs of numbers satisfy the simple “equation” $\\frac{1}{1}+\\frac{1}{\\infty} = \\frac{1}{2} + \\frac{1}{2} = 1$ , and as it turns out, this is enough for the “inner product” to be defined. For more details, you should read about Holder inequality. In the Fourier series section, we used $\\sin(nx), \\cos(nx)$ as our basis. Using the complex exponents is much easier (once we know how to work with them) and we almost get automatically the following results. Claim: Arithmetic operations Let $f\\in E^1(\\RR)$ . Then: The Fourier transform is linear, namely $\\mathcal{F}(\\alpha f + g) = \\alpha \\mathcal{F}(f)+\\mathcal{F}(g)$ . Translation-&gt;Rotation: Setting $f_\\alpha(x) := f(x+\\alpha)$ , we have $$\\hat{f}_\\alpha (\\omega) = e^{i\\alpha\\omega}\\hat{f}(\\omega).$$ Multiplication: If $\\lambda \\neq 0$ , then setting $g(x)=f(\\lambda x)$ we have $$\\hat g (\\omega) = \\frac {\\hat{f}(\\omega/\\lambda)}{\\abs{\\lambda}}.$$ Conjugation: $\\overline{\\cf[f](\\omega)}=\\cf[\\bar{f}](-\\omega)$ . Proof: Follows from the fact that integrals are linear. (and 3) Both claims follow from change of parameters: $$\\hat{f_\\alpha}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x+\\alpha)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega (x-\\alpha)}dx = e^{i\\omega\\alpha}\\hat f(\\omega).$$ For $\\lambda>0$ we have: $$\\hat{g}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(\\lambda x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega/\\lambda) x}\\frac{1}{\\lambda}dx = \\frac{1}{\\lambda}\\hat f(\\frac{\\omega}{\\lambda}).$$ For $\\lambda , the proof is similar. $$2\\pi \\overline{\\hat{f}(\\omega)} = \\overline{\\int_{-\\infty}^{\\infty} f(x) e^{-i\\omega x}\\dx} = \\int_{-\\infty}^{\\infty} \\overline{f(x)} e^{i\\omega x}\\dx = 2\\pi \\hat{\\overline{f}}(-\\omega)$$ If $f\\in L^1(\\RR)$ and we multiply it by some bounded function $g\\in L^\\infty(\\RR)$ then it is easy to check that $f(x)\\cdot g(x)\\in L^1(\\RR)$ , and actually $\\norm {f\\cdot g}_1 \\leq \\norm {f}_1 \\norm{g}_\\infty$ . In this case, we can ask what is the Fourier transform of $f\\cdot g$ . Probably the most important bounded function we work with is $e^{ix}$ which not only appear in the computation of the Fourier transform, but we also saw it in the claim above where a translation became a rotation. As it turns out the other direction works as well. Claim: Rotation->translation. Let $f\\in E^1(\\RR)$ and for $c\\in \\RR$ let $h_c(x):=e^{icx}f(x)$ . Then $$\\hat{h_c}(\\omega)=\\hat{f}(\\omega-c).$$ In the sine and cosine notations, we have: $$\\align{ \\cf[f(x)\\sin(x)]&=\\frac{\\cf[f](\\omega-c)-\\cf[f](\\omega+c)}{2i} \\\\ \\cf[f(x)\\cos(x)]&=\\frac{\\cf[f](\\omega-c)+\\cf[f](\\omega+c)}{2} }$$ Proof: This is again a simple computation: $$\\hat{h_c}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{icx}f(x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega-c)x}dx = \\hat f(\\omega-c).$$ There is already an interesting phenomenon suggested by these results. When we translated $f$ it transformed into a rotation of $\\hat{f}$ and when we rotated $f$ it transformed into a translation in $\\hat{f}$ . Something similar happened with the multiplication (and conjugation), but more “stable” - multiplication transformed into multiplication (same with conjugation). This duality is very important and we will see that more as we progress. Lets see a couple more examples. Corollary: If $f$ is even (resp. odd) then $\\cf[f]$ is even (resp. odd). If $f$ is real (resp. pure imaginary), then $\\overline{\\cf[f](\\omega)}=\\cf[f](-\\omega)$ (resp. $=-\\cf[f](-\\omega)$ ). In particular, if $f$ is real and even (e.g. cosine functions), then $\\hat{f}$ is real and even, and if $f$ is real and odd, then $\\hat{f}$ is pure imaginary and odd. Proof: Take the multiplication identity with $\\lambda = -1$ , so that $g(x)=f(-x)$ . In this case we get that $\\hat{g}(\\omega)=\\hat{f}(-\\omega)$ . If $f$ is symmetric, then $g=f$ , so that $\\hat{f}(\\omega)=\\hat{f}(-\\omega)$ is symmetric, and if $f$ is odd, then $g(x)=-f(x)$ in which case $$\\hat{f}(-\\omega)=\\widehat{g}(\\omega)=\\widehat{-f}(\\omega)=-\\widehat{f}(\\omega)$$ is odd. The second part is proved similarly from the conjugation identity. Continuity and derivatives Our next step is to show some properties on the function $\\hat {f}$ itself, and we start by showing that it is continuous. Claim: The Fourier transform is continuous If $f\\in E^1(\\RR)$ , then: The function $\\hat{f}$ is continuous. (The Riemann-Lebesgue lemma): $\\limfi{\\abs{\\omega}} \\hat{f}(\\omega) = 0$ . Proof: For the second claim, let’s consider the following example with $f(x)=e^{-\\abs{x}/10}$ The function itself is drawn in green, and we also draw $f(x)\\cdot \\sin(10x)$ and $f(x)\\cdot \\sin(10.2 x)$ in blue and red, where we use sine function instead of complex exponents, just so we can draw them. The transform should be thought of as the integrals over these function. In this example we want to show that since $10$ and $10.2$ are close, their integrals over $f(x)\\cdot \\sin(10x)$ and $f(x)\\cdot \\sin(10.2 x)$ are also close. The idea is that (1) if we are near the center the functions themselves are very close so their integrals are close and (2) if we are far away from the center, while this no longer holds, the total area beneath $f(x)$ is very small, so even when multiplying by some sine functions it remains small. Thus, together they will be small. More formally, fix some $M>0$ which “measures” how far we are from the center, then: $$\\align{ 2\\pi\\abs{\\hat{f}(\\omega+h)-\\hat{f}(\\omega)} & \\leq \\int_{-\\infty}^\\infty\\abs{e^{-i(\\omega+h)x}-e^{-i\\omega x}}\\abs{f(x)}\\dx = \\int_{-\\infty}^\\infty\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx \\\\ & = \\int_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx + \\int_{\\abs{x}> M}\\abs{e^{-ihx}-1}\\abs{f(x)}\\dx \\\\ & \\leq \\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\int_{\\abs{x}\\leq M}\\abs{f(x)}\\dx + 2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx \\\\ & \\leq \\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\norm f_1 + 2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx.}$$ Choose your favorite $\\varepsilon>0$ . Since $\\norm f_1 , we can choose $M$ big enough so that $$2\\int_{\\abs{x}> M}\\abs{f(x)}\\dx Next, for $\\abs{x}\\leq M$ , for all $\\abs{h}$ small enough, using the continuity of $e^{-ihx}$ we can make sure that $$\\sup_{\\abs{x}\\leq M}\\abs{e^{-ihx}-1} \\cdot \\norm f_1 Together we see that as $h\\to 0$ we have that $\\hat{f}(\\omega+h)\\to \\hat{f}(\\omega)$ , or in other words $\\hat{f}$ is continuous. The Riemann Lebesgue lemma is proved in a similar fashion. When computing the integral in $f(\\omega)$ , when we are far away from the center the integral will be very small, regardless of $\\omega$ . When we are close to the center, we can approximate our function by a step function. For each such step, when $\\omega$ is very large, we should expect very high frequency fluctuations so every “positive area” will more or less cancel out with a “negative area”. More formally, for any choice of $M>0$ we get a simple upper bound: $$\\align{ 2\\pi\\abs{\\hat{f}(\\omega)} & = \\abs{\\int_{-\\infty}^\\infty e^{-i\\omega x}f(x)\\dx} \\leq \\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}f(x)\\dx} + \\int_{\\abs{x}>M} \\abs{f(x)}\\dx.}$$ As before, for any $\\varepsilon>0$ , we can choose $M$ big enough so that the second summand is as small as we want : $$\\int_{\\abs{x}>M} \\abs{f(x)}\\dx Since our function $f$ is Riemann integrable, we can approximate it by a step function $h$ , namely $h(x)=\\sum_1^n \\lambda_i \\chi_{[a_i,b_i]}$ is a finite combination of steps, and $\\int_{\\abs{x}\\leq M}\\abs{h(x)-f(x)}\\dx . We can use it to upper bound the first summand: $$\\align{ \\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}f(x)\\dx}&=\\abs{\\int_{\\abs{x}\\leq M} e^{-i\\omega x}\\left(f(x)-h(x)\\right)\\dx + \\int_{\\abs{x}\\leq M} e^{-i\\omega x}h(x)\\dx} \\\\ & \\leq \\int_{\\abs{x}\\leq M} \\abs{f(x)-h(x)}\\dx + \\abs{\\sum_{i=1}^n\\lambda_i \\int_{\\abs{x}\\leq M} e^{-i\\omega x}\\chi_{[a_i,b_i]}(x)\\dx} \\\\ & \\leq \\frac{\\varepsilon}{4}+\\sum_{i=1}^n\\abs{\\lambda_i}\\hat{\\chi}_{[a_i,b_i]}(\\omega). }$$ We already saw that $\\hat{\\chi}_{[a_i,b_i]}(\\omega)=\\frac{e^{-i\\omega a}-e^{-i\\omega b}}{\\omega i}\\to0$ as $\\omega \\to \\infty$ , and since there are only finitely many such summands ( $n$ now is fixed), for all $\\omega$ large enough this sum is at most $\\frac{\\varepsilon}{2}$ . Putting everything together, we get that for all $\\varepsilon>0$ and for all $\\omega$ large enough, we have that $\\abs{\\hat{f}(\\omega)} , or in other words $\\hat{f}(\\omega)\\to 0$ . Next we turn to derivatives, which also contain an interesting duality. Claim: Fourier transform and derivatives Suppose that both $f\\in E^1(\\RR)$ , and $f'\\in E^1 (\\RR)$ . Then $$\\cf[f'](\\omega)=i\\omega \\cf[f](\\omega).$$ Similarly, if $xf(x)\\in E^1 (\\RR)$ , then $\\hat{f}$ is differentiable and $$\\cf[-ixf(x)](\\omega) = \\cf[f]'(\\omega).$$ Remark: Before we give a proper proof, note that if we are allowed to change the order of derivation and integration, then the second part is almost automatic. However, we don’t know that we can do this, and we basically prove it here. Proof: For the first part, we use integration by parts $$\\align{ 2\\pi \\widehat{f'}(\\omega)&=\\int_{-\\infty}^{\\infty} f'(x)e^{-i\\omega x}\\dx=f(x)e^{-i\\omega x}\\mid_{-\\infty}^{\\infty}+i\\omega\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}\\dx. }$$ Using the fact that $f(x)=f(0)+\\int_0^x f'(t)\\dt$ and $\\norm {f'}_1 is finite, we see that $\\limfi{x} f(x)$ exists. Since we also know that $\\norm{f}_1 is finite, this limit must be zero. Similarly, we get that $\\displaystyle{\\lim_{x\\to -\\infty}}f(x)=0$ , and since $e^{-i\\omega x}$ is bounded, we conclude that $$f(x)e^{-i\\omega x}\\mid_{-\\infty}^{\\infty} = 0. $$ The other summand is simply $2\\pi i \\omega \\hat{f}(\\omega)$ which completes the first part of claim. For the second part, by definition we have that $$\\align{ 2\\pi\\left(\\cf[f]'(\\omega) -\\cf[-ixf(x)] \\right) & =\\lim_{h\\to 0}\\int_{-\\infty}^{\\infty}f(x)\\frac{e^{-i(\\omega+h)x}-e^{-i\\omega x}}{h}\\dx + i\\int_{-\\infty}^{\\infty}xf(x) e^{-i\\omega x}\\dx \\\\ & = \\lim_{h\\to 0}\\int_{-\\infty}^{\\infty}f(x)e^{-i\\omega x}\\left( \\frac{e^{-ihx}-(1-ihx)}{hx}x \\right)\\dx. }$$ As $e^z$ is analytic, we can find some constant $C$ such that $\\abs{\\frac{e^{-ihx}-(1-ihx)}{hx}} for $\\abs{hx} . If $\\abs{hx}>1$ , then $\\abs{\\frac{e^{-ihx}-(1-ihx)}{hx}} . Using the fact that $\\norm {xf(x)}_1 , the same ideas from before show that the limit is zero. Example: Fourier transform of the Gaussian Letting $f(x)=e^{-x^2}$ , its Fourier transform is $$\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2}e^{-i\\omega x}\\dx,$$ which doesn’t seem to simple to compute. Instead, let us use the fact that we can take the derivative beneath the sign of the integral to get that : $$ 2\\pi \\hat{f}'(\\omega)= \\int_{-\\infty}^{\\infty} e^{-x^2}(-ix)e^{-i\\omega x}\\dx = \\frac{i}{2} \\int_{-\\infty}^{\\infty} (-2x)e^{-x^2}e^{-i\\omega x}\\dx.$$ Taking $u(x)=e^{-x^2}$ , and $v(x)=e^{-i\\omega x}$ , our integral is $u'(x)v(x)$ so we can do integration by parts. It follows that $$ 2\\pi \\widehat{f}'(\\omega)= \\frac{i}{2} e^{-x^2}e^{-i\\omega x}\\mid_{-\\infty}^{\\infty} - \\frac{1}{2}\\omega \\int_{-\\infty}^{\\infty} e^{-x^2}e^{-i\\omega x}=\\pi\\omega \\hat{f}(\\omega).$$ The solution to the differential equation $\\hat{f}'(\\omega)=\\frac{\\omega}{2}\\hat{f}(\\omega)$ is $ae^{-\\omega^2/4}$ . Finally, since $$a = \\hat{f}(0)=\\frac {1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2}\\cdot 1 \\dx = \\frac{1}{2\\sqrt{\\pi}},$$ we conclude that $$\\hat{f}(\\omega) = \\frac{1}{2\\sqrt{\\pi}} e^{-\\omega^2/4}.$$ &lt;- Previous: Fourier Series , False[[invalid wikilink: Fourier Course Information#Table of contents]] , ??? -&gt;"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_motivation": {
    "title": "מוטיבציה - התמרת פורייה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_motivation",
    "body": "מה אנחנו חוקרים? “התמרת פורייה” זה אחד הכלים הכי שימושיים כאשר רוצים להבין פונקציות ממשיות (ומרוכבות). אלו פונקציות שכמובן מופיעות בצורה טבעית בשלל מקומות בעולם, למשל: פיזיקה: פונקציות מקום, מהירות, כח, טמפרטורה, לחץ וכו’, פונקציות דיגיטליות: צבעי פיקסלים בתמונות, קבצי אודיו, או תנועה באנימציות, פונקציות “אבסטרקטיות”: כמו מחירי מניות בבורסה, או מדדי פופלריות באתרי מדיה חברתית. ה”פונקציות הממשיות” האלו מגיעות בכל מני צורות, כתלות בתחום הגדרה שלהן, כאשר המקרים הכי נפוצים הם: פונקציות על הישר הממשי: $f:\\RR\\to\\RR$ , פונקציות מחזוריות, כלומר $f:\\RR\\to\\RR$ עם זמן מחזור $ T>0 $ כך ש $f(x+T)=f(x)$ . פונקציות דיסקרטיות על קבוצה סופית $f:\\{1,2,...,n\\}\\to\\RR$ , פונקציות דיסקרטיות על קבוצה אינסופית $f:\\ZZ\\to\\RR$ , שאנחנו סדרך קוראים להן פשוט סדרות. בגלל שיש כל כך הרבה סוגים של פונקציות ממשיות, ומאחר והן כל כך שימושיות, זה מאוד חשוב למצוא מבנים וכלים מתמטיים על מנת לחקור אותם. כבר למדנו על כמה כאלו בקורסים הראשונים באלגברה וחדו”א, כמו: קודם כל, צריך לבחור את הסוג פונקציות : חסומות, רציפות, גזירות, אולי רק רציפות למקוטעין, אינטגרביליות וכו’. אלגברה לינארית: בדרך כלל המשפחת פונקציות האלו תהווה מרחב ווקטורי, מה שמאפשר לנו להשתמש בכל הכלים מאלגברה לינארית (פונקציות לינאריות, בסיס, מימד וכו’). חדו”א: עבור פונקציות על הישר הממשי, אפשר גם לדבר על הרציפות שלהן, נגזרות, אינטגרלים וכו’. גאומטריה: אנחנו יכולים גם לבחור “גודל”, או בצורה פורמלית את הנורמה, של פונקציה בכל מני דרכים, כל אחת עם היתרונות והחסרונות שלה. למשל, אפשר למדוד את השטח (בערך מוחלט) מתחת לגרף של הפונקציה: אחת התכונות החשובות של נורמות כאלו היא שהן מקיימות את אי שוויון המשולש $\\norm{f+g} \\leq \\norm f + \\norm g$ , מה שמאפשר לנו להגדיר מרחקים במרחב , כלומר $dist(f,g):=\\norm{f-g}$ ובכך בעצם להגדיר “גאומטריה” על המרחב. זוויות: בצורה יותר כללית, אנחנו יכולים לחשוב על “זוויות” בין הפונקציות, ואז לנסות למשוך אינטואיציה מהמרחבים האוקלידים הרגילים (למשל, רעיונות כמו משפט פיתגורס). בצורה יותר פורמלית, אנחנו מוסיפים מכפלה פנימית למרחבים שלנו (שנזכיר את הרעיונות שם בהמשך). הרעיון המרכזי בהתמרות פורייה, הוא שיש לנו מבנה נוסף מאוד מעניין על המרחבים האלו: סימטריה: לכל המשפחות שהזכרנו יש “סימטריות” מאוד נחמדות. אנחנו בדרך כלל חושבים על סימטריה כשיקופים - יש לנו את האובייקט ה”מקורי”, וכאשר אנחנו משקפים אותו דרך המראה, אנחנו מקבלים את “אותו האובייקט” (או לכל הפחות הוא נראה כמו המקורי). דבר דומה קורה עם הפונקציות הממשיות שלנו - למשל, פונקציות על הישר הממשי ניתן להזיז ימינה ושמאלה ושוב לקבל פונקציות באותה משפחה: $$ \\text{Translation left and right: }f(x) \\mapsto f(x+c)$$ על הסימטריה הזאת של תזוזה ימינה/שמאלה ניתן לחשוב כתזוזה במרחב או בזמן (כתלות במה הפונקציות שלנו מתארות). המבנה ה”סימטרי” הזה נראה די פשוט תחילה, אבל הוא מוביל לשאלות מאוד מעניינות. למשל, האם יש “תבניות” מעניינות שחוזרות על עצמן יחסית לסימטריות שלנו? האם יש “תבניות יסודיות” כאלו שכדאי לחפש בכל הפונקציות? החיפוש הזה של תבניות כאלו הוא אחד המטרות העיקריות של הקורס, ובמובן מסוים “התמרות פורייה” נועדו בדיוק כדי לחקור את התביות האלו."
  },"/Fourier_Notes/notes/FourierHebrew/basic%20properties": {
    "title": "תכונות בסיסיות",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/basic%20properties",
    "body": "התמרת פורייה היא פעולה מאוד מעניינת והיא משמרת כל מני תכונות ופעולות שאנחנו רגילים לעבוד איתן. כפי שנראה מייד, רובן נובעות כמעט מיידית עקב העובדה שההתמרה מוגדרת באמצעות פונקציית האקספוננט $e^{iwx}$ המרוכבת, כאשר הכח הגדול שיש לפונקציה הזאת הוא שהיא לוקחת סכומים (הזזות) לכפל (סיבובים). לפני שנראה ונוכיח את התכונות האלו, נזכיר שההתמרה שלנו עובדת לכל $f\\in E^1(\\RR)$ ואף מתקיים ש $\\norm {\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi}\\norm f_1$ . לא קשה לבדוק שאם $g\\in E^\\infty (\\RR)$ היא חסומה, אז $\\norm{f\\cdot g}_1 \\leq \\norm{f}_1\\norm{g}_\\infty$ ולכן $f\\cdot g\\in E^1(\\RR)$ וגם לה אפשר לבצע התמרה, ובפרט זה נכון לפונקציות סיבוב מהצורה $e^{icx}$ . טענה: פעולות אריתמטיות תהא $f\\in E^1(\\RR)$ . אז : התמרת פורייה היא לינארית: כלומר $\\cf(\\alpha f + g) = \\cf(f)+\\cf(g)$ . כפל בסקלר: אם $\\lambda\\neq0$ , אז עבור $g_\\lambda(x)=f(\\lambda x)$ נקבל ש $$\\hat g_\\lambda (\\omega) = \\frac {\\hat{f}(\\omega/\\lambda)}{\\abs{\\lambda}}.$$ הזזה $\\Leftarrow$ סיבוב: עבור ההזזה $f_\\alpha(x):=f(x+\\alpha)$ נקבל את ההתמרה: $$.\\hat{f}_\\alpha(\\omega) = e^{i\\alpha\\omega}\\hat{f}(\\omega)$$ סיבוב $\\Leftarrow$ הזזה: עבור $c\\in \\RR$ נסמן $h_c(x)= e^{icx} f(x)$ . אז $$\\hat{h}_c(\\omega) = \\hat{f}(\\omega - c)$$ הצמדה: מתקיים ש $$.\\overline{\\cf[f](\\omega)}=\\cf[\\overline{f}](-\\omega)$$ הוכחה: נובע מכך שהאינטגרל זו פונקציה לינארית. עבור $\\lambda>0$ עושים החלפת משתנים (ל $\\lambda x$ ): $$\\hat{g}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(\\lambda x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega/\\lambda) x}\\frac{1}{\\lambda}dx = \\frac{1}{\\lambda}\\hat f(\\frac{\\omega}{\\lambda}).$$ עבור $\\lambda ההוכחה דומה. נובע מהחלפת משתנים (ל $x+\\alpha$ ): $$\\hat{f_\\alpha}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x+\\alpha)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega (x-\\alpha)}dx = e^{i\\omega\\alpha}\\hat f(\\omega).$$ פה אפילו לא צריך לעשות החלפת משתנים, רק החלפת פרמטר: $$\\hat{h_c}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{icx}f(x)e^{-i\\omega x}dx = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} f(x)e^{-i(\\omega-c)x}dx = \\hat f(\\omega-c).$$ נובע מכך שהצמדה של אינטגרל זה אינטגרל של הצמדה: $$2\\pi \\overline{\\hat{f}(\\omega)} = \\overline{\\int_{-\\infty}^{\\infty} f(x) e^{-i\\omega x}\\dx} = \\int_{-\\infty}^{\\infty} \\overline{f(x)} e^{i\\omega x}\\dx = 2\\pi \\hat{\\overline{f}}(-\\omega)$$ מהטענה למעלה קיבלנו שההתמרה לינארית, כלומר היא מעבירה חיבור של פונקציות וכפל בסקלר לחיבור וכפל בסקלר. בנוסף, השילוב של כפל בסקלר ממשי והכפל $\\lambda x$ שהם בעצם מתיחות/כיווצים עוברים גם למתיחות וכיווצים ובפרט $$\\sqrt{|\\lambda|} \\cdot f(\\lambda x) \\Rightarrow \\sqrt{|\\lambda|^{-1}} \\hat{f}(\\lambda^{-1} \\omega)$$ לבסוף, גם ההצמדה של פונקציה מותמרת (פחות או יותר) להצמדה של ההתמרה. הפעולות של סיבוב והזזה לא מותמרות ל”עצמן”, אבל הן כן עוברות אחת לשניים: הזזה מותמרת לסיבוב וסיבוב מותמר חזרה להזזה. הדואליות הזאת היא משהו מיוחד שמרמז שאולי אם נבצע את ההתמרה פעמיים, אז נחזור לפונקציה שממנה יצאנו, ובאמת נראה משהו כזה בהמשך. לפני זה, בעוד שנוח מאוד לעשות חישובים עם האקספוננטים, הרבה פעמים נרצה לעבוד ממש עם פונקציות ממשיות, ומסקנה מיידית של סיבוב-&gt; הזזה נותן לנו את התוצאה הבאה: טענה: סיבוב $\\Leftarrow$ הזזה, בעולם הממשי עבור כפל בסינוסים וקוסינוסים נקבל: $$\\align{ \\cf[f(x)\\sin(x)]&=\\frac{\\cf[f](\\omega-c)-\\cf[f](\\omega+c)}{2i} \\\\ \\cf[f(x)\\cos(x)]&=\\frac{\\cf[f](\\omega-c)+\\cf[f](\\omega+c)}{2} }$$ ברגע שיש לנו את התכונות האלו והפעולות ש”נשמרות” תחת ההתמרה, שאלה מעניינת שאפשר לשאול זה מה אפשר להגיד על פונקציות “מיוחדות” יחסית לפעולות האלו. למשל, מה קורה לפונקציה שמראש צמודה לעצמה, כלומר $f(x)=\\overline{f(x)}$ לכל $x$ או במילים אחרות פונקציה ממשית. טענה: פונקציות שמורות אם פונקציה $f$ היא זוגית (בהתאם אי זוגית), אז $\\cf[f]$ היא זוגית (בהתאם אי זוגית). אם פונקציה $f$ היא ממשית (בהתאם מדומה טהורה), אז $\\overline{\\cf[f](\\omega)}=\\cf[f](-\\omega)$ (בהתאם שווה ל $-\\cf[f](-\\omega)$ ). בפרט, אם $f$ היא ממשית וזוגית (למשל קוסינוס), אז $\\hat{f}$ היא ממשית וזוגית, ואם $f$ היא ממשית ואי זוגית (למשל סינוס), אז $\\hat{f}$ היא מדומה טהורה ואי זוגית. הוכחה: נסתכל על זהות הכפל בסקלר עם $\\lambda = -1$ , ונסמן $g(x)=f(-x)$ . במקרה הזה נקבל ש $\\hat{g}(\\omega)=\\hat{f}(-\\omega)$ . אם $f$ זוגית, כלומר $g=f$ אז כמובן שגם $$\\hat{f}(\\omega)=\\hat{g}(\\omega)=\\hat {f}(-\\omega)$$ או במילים אחרות $\\hat{f}$ היא זוגית. באותה צורה אם $f$ היא אי זוגית אז גם $\\hat{f}$ היא אי זוגית. שאר הטענה נובעת עם הוכחה דומה יחד עם העובדה ש $f$ ממשית (בהתאם מדומה טהורה) אם $\\bar{f}=f$ (בהתאם $\\bar{f}=-f$ )."
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_delta_function": {
    "title": "התמרת פורייה ההפוכה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_delta_function",
    "body": "איך מוצאים את ההתמרה ההפוכה כאשר למדנו על טורי פורייה, התחלנו עם פונקציה מחזורית על $f:[-\\pi,\\pi]\\to \\RR$ , חישבנו ממנה את מקדמי פורייה $a_n,b_n$ והדרך העיקרית להבין בעזרתם את הפונקציה הקבועה הייתה להרכיב חזרה אתה פונקציה , כלומר $$.f(x) \\sim \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left(a_n \\cos(nx)+b_n \\sin(nx)\\right)$$ זה כמובן נבע מתכונות של בסיס אורתונורמלי, כי אם נסמן $$,\\{v_1,v_2,v_3,...\\}=\\{ \\frac{1}{\\sqrt{2}}\\}\\cup \\{\\cos(nx),\\sin(nx)\\;\\mid\\; n\\in \\NN\\}$$ אז מה שרשום למעלה זה בעצם $f\\sim \\sum _1^\\infty \\angles{f,v_k}\\cdot v_k$ . היינו רוצים לעשות דבר דומה גם בהתמרת פורייה. הפעם ה”מקדמים” שלנו הם $\\hat{f}(\\omega)=\\angles{f,e^{i\\omega x}}$ והם מוגדרים לכל $\\omega$ . אם זה היה דומה לפונקציות המחזוריות, אז היינו מחפשים משהו בסגנון $\\sum_\\omega \\hat{f}(\\omega) e^{i\\omega x}$ , אבל בגלל שהמקדמים הם לא דיסקרטים, אז נצטרך להסתכל על אינטגרל. תחת ההנחה שמותר לנו לעשות את כל המעברים הבאים, נקבל ש $$\\align{f(x_0)&\\overset{?}{=}\\int_{-\\infty}^\\infty \\hat{f}(\\omega)e^{i\\omega x_0}\\dom=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(x)e^{-i \\omega x}e^{i\\omega x_0}\\dx \\dom \\\\& =\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x) \\left(\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom\\right) \\dx }$$ יש שני דברים מוזרים בחישוב הזה: הפונקציה $\\omega\\mapsto e^{i\\omega(x_0-x)}$ היא לא אינטגרבילית בהחלט על כל השיר, כי בערך מוחלט היא שווה ל 1. יותר מכך, אפילו אם היה לה אינטגרל, שנסמן ב $g(t,x)$ , איזה פונקציה זו אמורה להיות כדי ש $$?\\;\\;\\;f(x_0)\\overset{?}{=}\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty f(x)g(x_0,x)\\dx$$ נזכיר שאם נשנה פונקציה בנקודה אחת, האינטגרל שלה לא ישתנה, כלומר אם נשנה את הערך $f(x_0)$ בנקודה אחת $x_0$ אז אגף ימין לא ישתנה, בעוד שאגף שמאל ישתנה. למרות שתי הבעיות האלו, עדיין אפשר “להגדיר” את ההתמרה ההפוכה, ולמרות שלא נכנס לכל ההוכחה המתמטית, בואו ננסה להבין קצת מאיפה היא מגיעה. נתחיל קודם עם הבעיה השנייה. למרות שאין אינטגרל שמקיים את המעבר הזה, יש פונקציה מאוד פשוטה שמתחילה עם $f$ ומסיימת עם $f(x_0)$ שהיא פשוט ההצבה בנקודה $x_0$ ונסמן אותה ב $$. \\;\\; \\delta_{x_0}(f):=f(x_0)$$ נשים לב תחילה שזו פונקציה מהצורה $\\delta_{x_0} : E^1(\\RR)\\to \\RR$ בדיוק כמו פונקציית האינטגרל (כלומר היא לוקחת פונקציה ומחזירה סקלר) ובנוסף כמו האינטגרל זוהי פונקציה לינארית, כי לפי ההגדרה: $$.\\;\\;\\delta_{x_0}(\\alpha f_1 + f_2) = \\alpha f_1(x_0) + f_2(x_0) = \\alpha \\delta_{x_0}(f_1)+\\delta_{x_0}(f_2)$$ אמנם אי אפשר להגדיר אותה ע”י רק אינטגרל, אבל האם ניתן לקרב אותה ע”י אינטגרל? אם הפונקציה $f$ היא רציפה, אז כן! למשל, אם ניקח את הפונקציות: $$\\sigma_{h,x_0}(x):=\\frac{2}{h}\\cdot \\chi_{[x_0-h,x_0+h]}(x)=\\cases{\\frac{2}{h} & |x-x_0|\\leq h\\\\ 0 & else}$$ אז נקבל ש $$\\int_{-\\infty}^{\\infty}f(x)\\sigma_{h,x_0}(x)\\dx = \\frac{2}{h} \\int_{x_0-h}^{x_0+h}f(x)\\dx\\overset{h\\to 0}{\\longrightarrow} f(x_0)$$ במילים אחרות, פונקצית ההצבה שלנו היא גבול של פונקציות שמבוטאות באמצעות אינטגרלים. זה גם מסביר קצת את הבעיה הראשונה. במקום לחשוב על $$x\\mapsto\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom$$ כפונקציה במשתנה $x$ שלא מוגדרת היטב כי האינטגרל לא מוגדר היטב, צריך לחשוב עליה כגבול של פונקציות (שיקבע לפי גבולות האינטגרציה כמו שנראה למטה) ואז פתאום נקבל את ההתמרה ההפוכה: $$.\\align{f(x_0)=\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x) \\left(\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom\\right) \\dx }$$ פונקצית דלתא של דירק עם האינטואיציה מלמעלה, נגדיר את פונקציית דלתא של דירק, עם אותו סימון כמו פונקציית דלתא למעלה: סימון: פונקציית דלתא של דירק נסמן $\\delta_a:\\RR \\to \\RR$ להיות ה”פונקציה” שמקיימת $$f(a)=\\int_I f(t)\\delta_a(t)\\dt $$ עבור כל פונקציה רציפה ב $a$ וקבוצה $I$ שמכילה את $a$ כנקודה פנימית. כאשר $a=0$ נסמן פשוט $\\delta(t):=\\delta_0(t)$ . הערה: בדרך כלל הקבוצה $I$ הולכת להיות כל הישר, אך כמובן שהפונקציה תלויה רק האיך ש $f$ מתנהגת באזור הנקודה $a$ . שימו לב שכמו שאמרנו למעלה, זהו סימון ואנחנו לא באמת מבצעים אינטגרציה במובן הרגיל. מה שכן, בגלל שפונקציית ההשמה $f\\mapsto f(a)$ מקיימת תכונות דומות לפונקצית האינטגרל, אז זהו סימון נוח. באופן רשמי פונקציה כזאת נקראת פונקציונלי. בפרט “נניח” שאפשר לבצע החלפת משתנים, ואז נקבל ש הזזה של פונקציית דיראק מתקיים: $$f(a+b)=\\int_{-\\infty}^{\\infty} f(t)\\delta_a(t-b)\\dt $$ ובפרט: $$f(b)=\\int_{-\\infty}^{\\infty} f(t)\\delta(t-b)\\dt $$ הוכחה: אם נרשה החלפת משתנים $s=t-b$ אז נקבל ש $$.\\int_{-\\infty}^{\\infty} f(t)\\delta_a(t-b)\\dt=\\int_{-\\infty}^{\\infty} f(s+b)\\delta_a(s)\\dt=f(a+b)$$ סימון: התכנסות לפונקציית דלתא כאשר נסמן התכנסות של פונקציות $g_n(x)\\to\\delta_a(x)$ לפונקציית דירק, הכוונה היא ש $$\\int_{-\\infty}^{\\infty}f(x)g_n(x)\\dx \\to f(a)$$ לכל פונקציה $f\\in E^1(\\RR)$ רציפה בנקודה $a$ . תחת הסימון הזה, בעצם בחלק הקודם הראנו ש $\\sigma_{h,a}\\to\\delta_a$ כאשר $h\\to 0$ . נשים לב שגם אם נחשוב על פונקציית דירק כגבול של אינטגרלים, אז לא מפתיע שהיא מקיימת תכונות כמו אינטגרציה ובפרט השימוש בהחלפת משתנים כמו שראינו למעלה. כדי להראות שתוצאה דומה נכונה עבור פונקציות האקספוננטים שלנו נציין תחילה טענה ללא הוכחה. עוד מעט נוכיח משהו יותר חזק, כרגע זה רק לשם האינטואיציה. טענה: מתקיים ש $\\displaystyle{\\lim_{M\\to\\infty}} \\frac{\\sin(Mx)}{\\pi x} = \\delta(x)$ . באמצעות הטענה הזאת נוכל עכשיו להראות ש: גבול של אקספוננטים הוא דירק: לכל $a\\in\\RR$ מתקיים ש $$\\limfi{N} \\frac{1}{2\\pi} \\int_{N}^{N} e^{i\\omega(t-a)}\\dom \\to \\delta_a(t)$$ הוכחה: בגלל שהראנו שאפשר להזיז את פונקציית דירק, כלומר $\\delta_a(t)=\\delta(t-a)$ אז נוכל לסמן $T=t-a$ ולהראות שהגבול הוא $\\delta(T)$ . עתה נקבל: $$\\frac{1}{2\\pi} \\int_{-N}^{N} e^{i\\omega T}\\dom = \\frac{1}{2\\pi}\\left( \\int_{-N}^{N} \\cos(\\omega T)\\dom + \\int_{-N}^{N} i\\sin(\\omega T)\\dom\\right)$$ פונקציה הסינוס היא אי זוגית, ולכן האינטגרל שלה מתאפס, וסה”כ נשאר עם $$\\frac{1}{2\\pi} \\int_{-N}^{N} e^{i\\omega T}\\dom = \\frac{1}{2\\pi} \\frac{\\sin(\\omega T)}{T}\\mid_{-N}^N = \\frac{\\sin(N T)}{\\pi T}$$ והתוצאה נובעת מהטענה הקודמת. עכשיו נוכל לחזור לטיעון שלנו בראש הדף בשביל לקבל שקיימת לנו התמרה הפוכה שמקיימת $$\\align{f(x_0) & =\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x) \\left(\\int_{-\\infty}^\\infty e^{i\\omega (x_0-x)} \\dom\\right) \\dx }$$ בנקודות רציפות של $f$ . פונקצית דלתא באי רציפות מאחר והפונקציות שלנו באופן כללי הן לא רציפות, אז הטענה מלמעלה (שלא הוכחה לחלוטין) לא תעזור ועדיין נרצה לדעת מה קורה בנקודות אי רציפות. כדי לקבל קצת אינטואיציה בואו נסתכל על דוגמאות פשוטות וננסה להבין מה קורה שם, כאשר נשים דגש על הנקודה בראשית, כלומר אנחנו רוצים להבין את $$.\\int_{-\\infty}^\\infty f(x)\\frac{\\sin(Mx)}{\\pi x} \\dx$$ פונקציות מציינות אם נסתכל על הפונקציה $f(x)=\\chi_{[a,b]}(x)$ אז נקבל את האינטגרל $$\\int_{a}^b \\frac{\\sin(Mx)}{\\pi x} \\dx = \\int_{Ma}^{Mb} \\frac{\\sin(x)}{\\pi x} \\dx$$ הראו כתרגיל שהאינטגרל $\\int_0^\\infty \\frac{\\sin(x)}{x}\\dx$ מתכנס. מסתבר שגם אפשר לחשב את האינטגרל הזה (למשל עם כלים מפונקציות מרוכבות) ולהראות שהוא שווה ל $\\frac{\\pi}{2}$ . אם אנחנו מאמינים לזה, אבל הגבול כאשר $M\\to \\infty$ למעלה תלוי באיפה נמצאת הראשית יחסית לקטע $(a,b)$ : $$\\limfi{M} \\int_{a}^b \\frac{\\sin(Mx)}{\\pi x} \\dx = \\cases{1 & a נשים לב שאם הפונקציה היא רציפה בראשית, כלומר $a או $0\\notin [a,b]$ אז באמת קיבלנו שהגבול של האינטגרל הוא הערך הראשית. אחרת, קיבלנו $\\frac{1}{2}$ שהוא “ממוצע” הערכים, כלומר $$.\\frac{f(0^-)+f(0^+)}{2}$$ הפונקציה הלא רציפה הכי פשוטה מתחיל עם הפונקציה $$.f(x)=\\cases{\\frac{1}{1+x^2}&x\\geq 0 \\\\ \\frac{-1}{1+x^2} & x זוהי כמובן פונקציה רציפה פרט לראשית, ובחרנו אותה כך שהאינטגרל שלה יתכנס, כלומר $f\\in E^1(\\RR)$ . בנוסף זוהי פונקציה אי זוגית, ולכן האינטגרל $$.\\int_{-\\infty}^\\infty f(x) \\frac{\\sin(Mx)}{\\pi x}\\dx = 0$$ כמובן, שעבור פונקציות אי זוגיות, הממוצע של הגבולות בראשית (בהנחה והם קיימים) גם חייב להיות אפס. שינוי רחוק מהראשית מה יקרה אם נשנה אותה רחוק מהראשית, נניח בתחום $[100,200]$ ? אם נסמן את הפונקציה החדשה ב $\\tilde{f}(x)$ אז נקבל ש $$\\align{\\int_{-\\infty}^\\infty \\tilde{f}(x) \\frac{\\sin(Mx)}{\\pi x}\\dx & = \\overbrace{\\int_{-\\infty}^\\infty f(x) \\frac{\\sin(Mx)}{\\pi x}\\dx}^{=0} + \\int_{100}^{200} (\\tilde{f}(x)-f(x)) \\frac{\\sin(Mx)}{\\pi x}\\dx \\\\ & = \\int_{100}^{200} \\frac{\\tilde{f}(x)-f(x)}{\\pi x} \\sin(Mx)\\dx }$$ בתחום $[100,200]$ הפונקציה $g(x)=\\frac{\\tilde{f}(x)-f(x)}{\\pi x}$ היא תחילה הפרש של שתי פונקציה ב $E^1(\\RR)$ ולכן היא גם שם, וחילקנו ב $x$ , שלא יוצר בעיות כי אנחנו לא ליד האפס. עכשיו נשארנו עם הגבול של האינטגרל $$\\limfi{M}\\int_{100}^{200} g(x) \\sin(Mx) \\dx$$ אבל ראינו שאינטגרל כזה שואף לאפס, כי זו בדיוק הלמה של רימן לבג. אפילו לא היינו צריכים לעצור ב 200, ויכולנו לעשות אינטגרל עד אינסוף, ואותו הדבר בחלק השלילי. במילים אחרות, הגבול של האינטגרל כולו תלוי רק באיך ש $f$ נראית מאוד קרוב לראשית! כרגע לפחות אנחנו יודעים שהאינטגרל הוא אפס אם הפונקציה שלנו אי זוגית + שינויים “הרחק” מהראשית, כלומר היא “אי זוגית רק בסביבת הראשית”. זה כמובן גורר שסכום הגבולות מימין ומשמאל הראשית (אם קיימים) הוא עדיין אפס. מה קורה בראשית? אזור הראשית הוא יותר בעייתי, כי אי אפשר פשוט לעשות את ה”מעבר”: $$ f(x)\\cdot \\frac{\\sin(Mx)}{\\pi x} = \\frac{f(x)}{\\pi x} \\cdot \\sin(Mx)$$ ולהשתמש ברימן לבג, כי באופן כללי הפונקציה $\\frac{f(x)}{x}$ היא כבר לא צריכה להיות עם אינטגרל סופי. מצד שני, לפונקציה $\\frac{\\sin(Mx)}{\\pi x}$ כן יש גבול בראשית, ולכן אם במקרה $f(x)$ הייתה קבועה מימין לראשית, נניח ב $(0,\\varepsilon)$ אז נוכל ממש לחשב שם את האינטגרל (זה היה החישוב הראשון שלנו עם פונקציות מציינות). אם היא לא קבועה, אז בתקווה יש לנו את הדבר הכי טוב הבא בתור, ויש לה שם גבול שנסמן ב $f(0^+)$ ואז נקבל ש $$\\align{\\int_0^\\varepsilon f(x)\\cdot \\frac{\\sin(Mx)}{\\pi x} \\dx & = \\int_0^\\varepsilon \\left(f(x)-f(0^+)+f(0^+)\\right)\\cdot \\frac{\\sin(Mx)}{\\pi x} \\dx \\\\ & \\int_0^\\varepsilon \\left[\\frac{f(x)-f(0^+)}{x}\\cdot\\frac{\\sin(Mx)}{\\pi} +f(0^+)\\cdot \\frac{\\sin(Mx)}{\\pi x}\\right] \\dx} $$ כמו שכבר אמרנו, למחובר השני אנחנו ממש יכולים לחשב את האינטגרל. במחובר הראשון עדיין יש בעיה, אבל אם $\\frac{f(x)-f(0^+)}{x}$ פונקציה מספיק יפה, למשל יש לה גבול בראשית, אז שוב נוכל להשתמש בלמה של רימן לבג. אבל בעצם כבר ראינו את הביטויים האלו - להגיד שיש גבול בראשית בדיוק שקול לכך שיש נגזרת מתואמת מצד ימין. התמרת פורייה ההפוכה עכשיו אפשר לחבר את כל החלקים כדי לקבל את התוצאה הבאה: התמרת פורייה ההפוכה תהא $f\\in E^1(\\RR)$ . בכל נקודה $x_0\\in \\RR$ בה יש לפונקציה נגזרות מתואמות, מתקיים ש $$\\limfi{M} \\int_{-M}^M \\hat{f}(\\omega)e^{i\\omega x_0}\\dom = \\frac{f(x_0^-)+f(x_0^+)}{2}$$ הוכחה: כרגיל, כדאי להתחיל עם פישוט הבעיה, למשל נרצה לעבור מ $x_0$ כללי לראשית. לשם כך נגדיר את $g(x)=f(x_0+x)$ ואז נקבל ש $\\hat{g}(\\omega)=\\hat{f}(\\omega)e^{i\\omega x_0}$ בסימון החדש הזה מספיק להראות ש $$\\align{\\frac{g(0^-)+g(0^+)}{2}& \\overset{?}{=}\\limfi{M} \\int_{-M}^M \\hat{g}(\\omega)\\dom = \\frac{1}{2\\pi}\\limfi{M} \\int_{-\\infty}^{\\infty} g(x)\\int_{-M}^M e^{-i\\omega x} \\dom \\dx \\\\ & = \\limfi{M} \\int_{-\\infty}^{\\infty} g(x) \\frac{\\sin(Mx)}{\\pi x} \\dx}$$ נחשב את האינטגרל על החלק החיובי של ציר ה $x$ . כמו מקודם נכתוב $g(x)=g(x)-g(0^+)-g(0^+)$ ואז האינטגרל למעלה (ללא הגבול) יהיה: $$\\int_{0}^{\\infty} \\frac{g(x)-g(0^+)}{\\pi x} \\sin(Mx) \\dx+g(0^+)\\int_{0}^{\\infty} \\frac{\\sin(Mx)}{\\pi x} \\dx$$ כבר ראינו שהאינטגרל השני שואף ל $\\frac{1}{2}$ כאשר $M\\to \\infty$ . עבור הפונקציה $\\frac{g(x)-g(0^+)}{\\pi x}$ היא ב $E^1(\\RR)$ כי $g(x)\\in E^1(\\RR)$ ולא הוספנו בעיה בראשית בחלוקה ב $x$ בגלל הנגזרת המתואמת. עכשיו אפשר להשתמש בלמה של רימן לבג על האינטגרל הראשון כדי לקבל שהגבול הוא אפס, ולכן סה”כ קיבלנו שהגבול הוא $\\frac{g(0^+)}{2}$ . חישוב דומה נעשה בחלק השלילי של ציר האיקס ויחד איתו מקבלים את התוצאה שרצינו. התמרה כפולה היא (כמעט) הזהות אם $f,\\hat{f}\\in E^1(\\RR)$ ול $f$ יש נגזרות מתואמות ב $x_0$ אז מתקיים ש $$\\hat{\\hat{f}}(x_0) = \\frac{1}{2\\pi} \\frac{f(-x_0^+)+f(-x_0^-)}{2}$$ ובפרט אם $f$ רציפה ב $x_0$ אז מתקיים ש $$\\hat{\\hat{f}}(x_0) = \\frac{1}{2\\pi} f(-x_0)$$ הוכחה: נשים לב תחילה שתחת ההנחה ש $f,\\hat{f} \\in E^1(\\RR)$ הביטוי $\\hat{\\hat{f}}(x)$ מוגדר היטב. בפרט, בגלל ש $\\hat{f}\\in E^1(\\RR)$ הקצב שאיפה ל $\\pm \\infty$ באינטגרל לא משנה, ולכן $$\\align{\\hat{\\hat{f}}(x_0) & = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\hat{f}(\\omega) e^{-i\\omega x_0} \\dom = \\frac{1}{2\\pi}\\limfi{M}\\int_{-M}^M \\hat{f}(\\omega) e^{i\\omega (-x_0)} \\dom \\\\ &= \\frac{1}{2\\pi} \\frac{f(-x_0^+)+f(-x_0^-)}{2}}$$ נזכיר שהראנו שאם $f\\in E^1(\\RR)$ ולכן יש לה התמרת פורייה, אז $\\norm {\\hat{f}}_\\infty \\leq \\frac{1}{2\\pi} \\norm f_1$ . עכשיו שיש לנו את ההתמרה ההפוכה בעצם גם נקבל ש מסקנה: אם $f,\\hat{f} \\in E^1(\\RR)$ ו $f$ רציפה עם נגזרות מתואמות, אז $f$ חסומה ומתקיים ש $$.\\norm f_\\infty \\leq \\norm {\\hat{f}}_1$$ דוגמאות דוגמא: הפונקציה $e^{-\\abs{x}}$ כבר ראינו ש $\\cf\\left[e^{\\abs{x}}\\right] = \\frac{1}{\\pi (\\omega^2+1)}$ . מאחר ושתי הפונקציות $e^{-\\abs{x}}, \\frac{1}{\\pi (\\omega^2+1)}\\in E^1(\\RR)$ , הן גזירות ברציפות בכל מקום פרט לראשית, אבל שם יש גזירות מתואמות, אז ממשפט ההתמרה ההפוכה נקבל ש $$.e^{-\\abs{x}}=\\int_{-\\infty}^\\infty \\frac{1}{\\pi (\\omega^2+1)} e^{i \\omega x} \\dom$$ שימו לב שביטלנו את החלוקה ב $2\\pi$ בשני האגפים, ולא צריך לקחת $-x$ באגף שמאל, כי הוא גם ככה מופיע בערך מוחלט. נוכל עכשיו להשתמש בזהות אוילר $e^{i \\omega x} = \\cos(\\omega x)+i\\sin(\\omega x)$ יחד עם העובדה שהפונקציה $\\frac{1}{\\pi (\\omega^2+1)}$ היא ממשית וזוגית, כדי להסיק ש: $$.e^{-\\abs{x}}=\\int_{-\\infty}^\\infty \\frac{\\cos(\\omega x)}{\\pi (\\omega^2+1)} \\dom$$ דוגמא: הפונקציה המציינת $\\chi_{[-a,a]}$ ראינו ש $\\cf[\\chi_{[-a,a]}](\\omega) = \\frac{\\sin(\\omega a)}{\\omega \\pi}$ . הפונקציה הזאת היא בעצם הפונקציה שהשתמשנו כדי להוכיח את קיום ההתמרה ההפוכה! כלומר, כאשר $a\\to \\infty$ ההתמרה של הפונקציה שואפת לפונקציית דלתא, בעוד שהפונקציה המקורית עצמה “שואפת” לפונקציה הקבועה $1$ . אם נחשוב עליה עכשיו בתור ההתמרה, ונבצע לה התמרה הפוכה, אז בגלל שזו פונקציה זוגית, אז כמו בדוגמא הקודמת נקבל ש: $$\\limfi{M}\\frac{1}{2\\pi}\\int_{-M}^M \\frac{\\sin(\\omega a)}{\\omega \\pi} \\cos(\\omega x) \\dom = \\frac{\\chi_{[-a,a]}(x^+)+\\chi_{[-a,a]}(x^-)}{2}$$ שוב נשתמש בכך שהפונקציה באינטגרל היא זוגית, כדי לקבל ש $$\\frac{1}{\\pi}\\int_{0}^\\infty \\frac{\\sin(\\omega a)}{\\omega \\pi} \\cos(\\omega x) \\dom = \\cases{1&\\abs{x}a}$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_plancharel": {
    "title": "נוסחת פלנשרל",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_plancharel",
    "body": "נוסחת פלנשרל אם נחזור שוב לעולם של טורי פורייה, אז יכולנו להשתמש שם בשוויון פרסיבל ולקבל שעבור פונקציה $f:[-\\pi,\\pi]\\to\\CC$ רציפה למקוטעין עם מקדמי פורייה $a_n,b_n$ מתקיים ש $$.\\norm{f}_2^2=\\frac{\\abs{a_0}^2}{2}+ \\sum_1^\\infty \\left(\\abs{a_n}^2+\\abs{b_n}^2\\right)$$ כאשר המקדמים שלנו הם ההתמרה $\\hat{f}(\\omega)$ שמוגדרים לכל מספר ממשי, לכן היינו מצפים לקבל שוויון (עד כדי קבוע) מהצורה: $$.\\norm{f}_2^2=C\\int_{-\\infty}^\\infty \\abs{\\hat{f}(\\omega)}^2\\dom=C\\norm{\\hat{f}}_2^2$$ אבל, מייד יש לנו בעיות עם ההגדרה הזאת - עד עכשיו ההתמרה הוגדרה עבור פונקציות שעבורן $\\norm f_1 והן לא בהכרח מקיימות ש $\\norm {f}_2 וגם הכיוון ההפוך לא נכון. למזלנו יש הרבה פונקציות שעבורן שתי הנורמות האלו סופיות, ואז הטענה הזאת תהיה נכונה. משפט פלנשרל: יהיו $f,g\\in E^1(\\RR)\\cap E^2 (\\RR)$ אז: ההתמרות $\\hat{f}, \\hat{g}$ גם נמצאות ב $E^2(\\RR)$ , מתקיים ש: $$\\int_{-\\infty}^\\infty \\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)} \\dom = \\frac{1}{2\\pi} \\int _{-\\infty}^\\infty f(x)\\overline{g(x)} \\dx$$ או בכתיב של מכפלות פנימיות $2\\pi\\angles{\\hat{f}, \\hat{g}}=\\angles{f,g}$ , ובפרט מתקיים ש $\\norm{f}_2=\\norm{\\hat{f}}_2$ . הוכחה: הרעיון באופן כללי הוא החלפת סדרי אינטגרציה, ואם מותר לנו לעשות אותם אז “קל” להראות שהטענה נכונה. מבחינה פורמלית, נראה תחילה שכאשר ל $f,g$ יש תומכים סופיים, כלומר קיים $M>0$ מספיק גדול כך ש $f(x)=g(x)=0$ עבור $|x|>M$ , אז ההחלפות מותרות. באופן כללי, יש צורך להשתמש בקירובים של פונקציות כלליות ע”י צמצום שלהם לקטעים סופיים, כלומר $f_M(x):=f(x)\\cdot \\chi_{[-M,M]}$ אבל לא נכנס לפרטים האלו כאן. בנוסף, נתחיל גם עם ההנחה שגבולות אינטגרציה ששואפים ל $\\pm \\infty$ באותו קצב, ואז נראה איך נפתרים מזה. תחת ההנחות האלו נקבל ש $$\\align{\\int_{-N}^N \\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)} \\dom & = \\frac{1}{4\\pi^2} \\int_{-N}^N \\left(\\int_{-\\infty}^\\infty f(x)e^{-i \\omega x} \\dx \\overline{\\int_{-\\infty}^\\infty g(y)e^{-i \\omega y} \\dy} \\right)\\dom \\\\ & = \\frac{1}{4\\pi^2} \\int_{-N}^N \\left(\\int_{-M}^M f(x)e^{-i \\omega x} \\dx \\overline{\\int_{-M}^M g(y)e^{-i \\omega y} \\dy} \\right)\\dom \\\\ & = \\frac{1}{2\\pi} \\int_{-M}^M \\int_{-M}^M f(x)\\overline{g(y)} \\left( \\int_{-N}^N\\frac{e^{-i \\omega (x-y)}}{2\\pi} \\dom \\right)\\dx \\dy} $$ אם נשאיף את $N\\to\\infty$ אז נקבל את פונקצית דירק בנקודה $x-y$ באינטגרל הפנימי, לכן סה”כ האינטגרל למעלה יהיה שווה ל $$.(*)=\\frac{1}{2\\pi} \\int_{-M}^M f(x)\\overline{g(x)} \\dx=\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f(x)\\overline{g(x)} \\dx$$ אם נבחר $f=g$ אז נקבל ש $$.\\limfi{N}\\int_{-N}^N \\abs{\\hat{f}(\\omega)}^2 \\dom = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\abs{f(x)}^2 \\dx$$ מאחר ופה האינטגרלים הם על פונקציות אי שליליות, קצב השאיפה ל $\\pm \\infty$ בגבולות האינטגרציה לא משנה את ההתכנסות, ולכן נקבל ש $\\norm {\\hat{f}}_2^2= \\frac{1}{2\\pi}\\norm{f}_2^2 , וזה כבר מראה ש $\\hat{f}\\in E^2(\\RR)$ , וכמובן גם $\\hat{g}\\in E^2(\\RR)$ . אם נחזור עכשיו לחישוב הראשון למעלה, אז נוכל להשתמש באי שוויון קושי שוורץ (או בכך שהמכפלה הפנימית מוגדרת היטב על $E^2(\\RR)$ ), כדי לקבל ש $$.\\int_{-N}^N \\abs{\\hat{f}(\\omega) \\overline{\\hat{g}(\\omega)}} \\dom =\\angles{|\\hat{f}|,|\\hat{g}|} \\leq \\norm{\\hat{f}}_2 \\norm{\\hat{g}}_2 = \\frac{1}{4\\pi^2}\\norm{f}_2 \\norm{g}_2 זה אומר שהאינטגרל מתכנס בהחלט, ולכן גם בלי הערך המוחלט הוא מתכנס, ללא קשר לקצב שאיפת הגבולות אינטגרציה ל $\\pm \\infty$ . זה מסיים את ההוכחה עבור פונקציות עם תומך סופי. דוגמאות דוגמא: הפונקציה $e^{-a|x|}$ עבור $a>0$ ראינו כבר שההתמרה של הפונקציה הזאת היא $\\cf[e^{-a|x|}](\\omega)=\\frac{a}{\\pi(\\omega^2+a^2)}$ . אם נחשב את הנורמה של הפונקציה נקבל ש $$.\\norm{e^{-a|x|}}_2^2 = \\int_{-\\infty}^\\infty e^{-2a|x|}\\dx=2 \\int_{0}^\\infty e^{-2ax}\\dx=\\frac{e^{-2ax}}{-a}\\mid_0^\\infty = \\frac{1}{a}$$ מצד שני, שימוש בפלנשרל נותן ש $$.\\int_{-\\infty}^\\infty \\left(\\frac{a}{\\pi(\\omega^2+a^2)}\\right)^2 \\dom = \\frac{1}{2\\pi} \\frac{1}{a}$$ אם נעביר אגפים ונשתמש בזוגיות של הפונקציה, נקבל ש $$.\\dboxed{\\int_{0}^\\infty \\frac{1}{(\\omega^2+a^2)^2} \\dom = \\frac{\\pi}{4a^3}} $$ דוגמא: פונקציות מדרגה סימטריות עבור $a>0$ נסתכל על הפונקציה $$.f_a(x)=\\chi_{[-a,a]}(x)=\\cases{1 & |x|\\leq a \\\\ 0 & |x| > a}$$ עבור הפונקציות האלו ראינו ש $\\hat{f}_a(\\omega)=\\frac{\\sin(\\omega a)}{\\omega \\pi}$ . שימוש בפלנשרל עבור המכפלה הפנימית יתן לנו ש: $$\\align{\\int_{-\\infty}^\\infty \\frac{\\sin(\\omega a)}{\\omega \\pi} \\cdot \\frac{\\sin(\\omega b)}{\\omega \\pi} \\dom &= \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f_a(x) f_b(x) \\dx = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty f_{\\min(a,b)}(x) \\dx \\\\&= \\frac{\\min(a,b)}{\\pi}}$$ התמרות פורייה ב $E^2 (\\RR)$ אם נתונה פונקציה $f\\in E^2(\\RR)$ שלא ידוע מראש שהיא גם ב $E^1(\\RR)$ , אז באופן כללי אין סיבה שהאינטגרל בהתמרת פורייה יתכנס. למרות זאת, יש דרך להרחיב את ההגדרה של התמרת פורייה גם לפונקציות כאלו. התמרת פורייה ב $E^2(\\RR)$ תהא $f\\in E^2(\\RR)$ ועבור $M>0$ נסמן $$.f_M(x):=f(x)\\cdot \\chi_{[-M,M]}(x)=\\cases{f(x) & |x|\\leq M \\\\ 0 & |x|>M}$$ הפונקציות $f_M(x)$ נמצאות ב $E^1(\\RR) \\cap E^2(\\RR)$ ונגדיר את התמרת פורייה של $f$ להיות $$\\hat{f}(\\omega) = \\limfi{M} \\hat{f}_M(\\omega)= \\limfi{M} \\frac{1}{2\\pi} \\int_{-M}^M f(x)e^{-i \\omega x}\\dx$$ כאשר הגבול הוא בנורמת $\\norm{\\cdot}_2$ . הערה: דבר ראשון כדאי לשים לב שקל להראות ש $f_M$ אינטגרביליות בהחלט, כי $$\\norm{f_M}_1=\\int_{-\\infty}^\\infty \\abs{f_M} \\dx=\\int_{-M}^M \\abs{f}\\cdot 1 \\dx\\overset{(*)}{\\leq} \\sqrt{\\int_{-M}^M |f|^2(x)\\dx \\int_{-M}^M |1|^2(x)\\dx}\\leq \\norm{f}_2 \\sqrt{2M} $$ כאשר האי שוויון ב $(*)$ הוא אי שוויון קושי שוורץ. החלק היותר מסובך בהגדרה, הוא שאם $f$ הייתה מראש ב $E^1(\\RR)$ אז בעצם עכשיו יש לנו שתי הגדרות להתמרה שלה, וצריך להראות ששתיהן מחזירות את אותה פונקציה. הטענה הזאת נכונה, אבל לא נוכיח אותה פה. לבסוף, ההתמרה של פונקציה ב $E^2(\\RR)$ כמו שמוגדרת למעלה, מחזירה גבול של פונקציות אינטגרבילית בריבוע, שהוא בפרט פונקציה אינטגרבילית בריבוע שהן לא מוגדרות נקודתית. כלומר כאשר אנחנו אומרים ששתי פונקציות כאלו הן שוות, אז הן שוות כמעט לכל $x$ . תחת ההגדרה הזאת, מקבלים שהתמרת פורייה לוקחת פונקציות מהמרחב הפונקציות האינטגרביליות בריבוע לעצמו והתכונות שתיארנו עד עכשיו להתמרה עדיין נכונות, ובפרט ההתמרה ההפוכה ומשפט פלנשרל. דוגמא: הפונקציה $f(x)=\\frac{\\sin(x)}{x}$ הפונקציה $f(x)$ היא לא אינטגרבילית בהחלט אבל היא כן אינטגרבילית בריבוע כי $|f(x)|^2\\leq \\frac{1}{x^2}$ והאינטגרל של הפונקציות הזאת מתכנס באינסוף. לפי ההגדרה החדשה שלנו מתקיים ש $$.\\hat{f}(\\omega)=\\limfi{M} \\frac{1}{2\\pi} \\int_{-M}^M \\frac{\\sin(x)}{x}e^{-i \\omega x}\\dx$$ החישוב עצמו הוא די מסובך, אבל למזלנו כבר נתקלנו בפונקציה הזאת ואנחנו יודעים ש $\\cf[\\chi_{[-T,T]}](\\omega)=\\frac{\\sin(\\omega T)}{\\omega \\pi}$ , ולכן בפרט $\\pi\\cf[\\chi_{[-1,1]}](\\omega)=\\frac{\\sin(\\omega)}{\\omega}$ . אם נשתמש במשפט ההתמרה ההפוכה נקבל ש $$.\\cf\\left[\\frac{\\sin(\\omega)}{\\omega}\\right](x) = \\pi \\cf\\left[\\cf\\left[\\chi_{[-1,1]}\\right]\\right](x) = \\frac{1}{2} \\chi_{[-1,1]}(-x) = \\frac{1}{2} \\chi_{[-1,1]}(x)$$ ובאופן כללי יותר נקבל ש $$.\\cf\\left[\\frac{\\sin(\\omega T)}{\\omega}\\right](x) = \\frac{1}{2} \\chi_{[-T,T]}(x)$$"
  },"/Fourier_Notes/notes/FourierHebrew/hebrew_convolution": {
    "title": "קונבולוציה",
    "keywords": "התמרות פורייה",
    "url": "/Fourier_Notes/notes/FourierHebrew/hebrew_convolution",
    "body": "מה זו קונבולוציה ראינו כבר הרבה תכונות מעניינות של התמרת פורייה, ובפרט ראינו שהיא לוקחת פעולות שאנו רגילים לעבור איתם כל חיבור, כפל, סיבוב, הצמדה וכו’ פחות או יותר לעצמן. פעולה נוספת מאוד חשובה היא כפל של פונקציות, ונרצה להראות איך לבטא את ההתמרה $\\widehat{f\\cdot g}$ באמצעות ההתמרות $\\hat{f},\\hat{g}$ . פה מגיעה פעולת הקונבולוציה, אבל לפני שנגדיר אותה ונראה אותה מופיעה בהתמרות האלו, בואו נראה שבעצם אנחנו מכירים אותה כבר. אחת ממשפחות הפונקציות הכי פשוטה שאנחנו מכירים היא משפחת הפולינומים, כלומר פונקציות מהצורה: $$.f(x)=a_0+a_1 x + a_2 x^2 +\\cdots +a_d x^d = \\sum_0^d a_k x^k$$ גם פה במובן מסויים יש לנו “התמרה”. אם מתחילים עם הפונקציה $f$ אז המקביל למקדמי פורייה אלו פשוט המקדמים $a_k$ ו”ההתמרה ההפוכה” זו הקומבינציה שלהם שבאגף ימין בביטוי למעלה. $$\\align{f&\\overset{'\\cf'}{\\longrightarrow}(a_0,a_1,...,a_d)\\\\. \\sum_0^da_kx^k & \\overset{'\\cf^{-1}\\;'}{\\longleftarrow}(a_0,a_1,...,a_d)}$$ אם נתון לנו פולינום נוסף $$,g(x)=b_0+b_1 x + b_2 x^2 +\\cdots +b_d x^d = \\sum_0^d b_k x^k$$ אז הפונקציה $f(x)g(x)$ היא גם פולינום, ולכן נוכל לשאול איך מבטאים את המקדמים שלה ע”י המקדמים $a_k,b_k$ של $f(x),g(x)$ . במקרה הזה נקבל ש : $$\\align{f(x)g(x) & = \\left(\\sum_{i=0}^d a_i x^i\\right)\\left(\\sum_{j=0}^d b_j x^j\\right) = \\sum_{i=0}^d\\sum_{j=0}^d a_i b_j \\cdot x^{i+j} = \\sum_{k=0}^{2d} \\left(\\sum_{i+j=k} a_i b_j\\right) \\cdot x^{k} = \\sum_{k=0}^{2d} \\left(\\sum_{i} a_i b_{k-i}\\right) \\cdot x^{k} \\\\ &= a_0b_0+(a_0b_1+a_1b_0)x+(a_0b_2+a_1b_1+a_2b_0)x^2+\\cdots +(a_{d-1}b_d+a_db_{d-1})x^{2d-1}+a_db_dx^{2d}}$$ כלומר, המקדם של $x^k$ הוא סכום של מכפלות של המקדמים של $f,g$ כך שהאינדקסים שלהם נסכמים ל $k$ . הסיבה למבנה המעניין הזה הוא בעצם חוקי החזקות שהמונומים מקיימים, כלומר $x^i x^j = x^{i+j}$ . כדי לקבל קצת יותר אינטואיציה לכפל הזה, ולקונבולוציה שעוד מעט נגדיר, נניח שהפולינומים הם מדרגה $d=2$ ורוצים למצוא את המקדם החופשי. נכתוב מקדמי הפולינומים בטבלה, כאשר המקדמים של $f$ גדלים באינדקס כאשר זזים ימינה, והמקדמים של $g$ קטנים באינדקס, ואם נשים אותה כך שהמקדם החופשי של $f$ יהיה מעל המקדם החופשי של $g$ אז רק צריך לכפול את התא בשורה הראשונה (של $f$ ) בתא מתחתיו (של $g$ ) לכתוב בתא בשורה השלישית ואז לסכום את השורה השלישית: $a_2x^2$ $a_1x^1$ $a_0 x^0$ 0 0 0 0 $b_0 x^0$ $b_1 x^1$ $b_2x^2$ 0 0 $a_0b_0x^0$ 0 0 אם נרצה את המקדם של $x$ , אז זה יהיה אותו הדבר, רק צריך להזיז את כל המקדמים בשורה הראשונה תא אחד שמאלה: $a_2x^2$ $a_1x^1$ $a_0 x^0$ 0 0 $b_0 x^0$ $b_1 x^1$ $b_2x^2$ 0 $a_1b_0x^1$ $a_0b_1x^1$ 0 ואם נרצה את המקדם של $x^2$ אז נזיז אותם עוד תא שמאלה וכך הלאה: 0 $a_2x^2$ $a_1x^1$ $a_0 x^0$ 0 0 $b_0 x^0$ $b_1 x^1$ $b_2x^2$ 0 0 $a_2b_0x^2$ $a_1b_1x^2$ $a_0b_2x^2$ 0 מאחר וחוקי החזקות האלו מתקיים גם עבור האקספוננטים שלנו, נקבל משהו דומה, פרט לכך שבמקום מקדמים דיסקרטים, יש לנו מקדמים רציפים. קונבולוציה של פונקציות הגדרה: קונבולוציה: עבור שתי פונקציות $f,g:\\RR \\to \\CC$ נגדיר את הקונבולוציה שלהן $(f*g)$ להיות האינטגרל (כאשר הוא מוגדר): $$.(f*g)(x)=\\int_{-\\infty}^\\infty f(x-y)g(y) \\dy$$ נשים לב תחילה ש”סכום האינדקסים” בתוך הקונבולוציה הוא תמיד $(x-y)+y=x$ בדיוק כמו שבכפל פולינומים הסכום תמיד היה $(k-i)+i=k$ . ההגדרה של הקונבולוציה דומה להגדרה של מכפלה פנימית, רק במקום לכפול באינטגרנד ב $f(y)$ אנחנו עושים שיקוף $y\\mapsto-y$ ואז הזזה ב $x$ . בדומה למכפלה הפנימית, יש שני מקרים בהם הקונבולוציה מוגדרת לכל $x$ עם הוכחות דומה לאלו מהמכפלה הפנימית: אם אחת מהפונקציות היא חסומה והשנייה אינטגרבילית בהחלט (כלומר ב $E^\\infty(\\RR)$ ו $E^1(\\RR)$ בהתאם). אם שתי הפונקציות נמצאות ב $E^2(\\RR)$ . מקרה נוסף מעניין שדורש קצת יותר מאמץ הוא כאשר שתי הפונקציות הן ב $E^1(\\RR)$ אך לפני שנוכיח את זה, בואו נראה דוגמא בסיסית בשביל האינטואיציה. דוגמא: קונבולוציה של פונקציות מציינות נסתכל על שתי פונקציות מציינות של קטעים, הראשונה של $[-1,1]$ והשנייה של $[-h,h]$ : $$\\align{f(x)=\\chi_{[-1,1]}(x)=\\cases{1&|x|\\leq 1 \\\\ 0 & |x|>1} \\\\ .g(x)=\\chi_{[-h,h]}(x)=\\cases{1&|x|\\leq h \\\\ 0 & |x|>h}}$$ בחישוב של הקונבולוציה, אנחנו מכפילים בין שתי הפונקציות ומבצעים אינטגרציה. הפונקציה $g(y)$ שמופיעה באינטגרל “לא זזה” כאשר $x$ משתנה, בעוד שהפונקציה $f(x-y)$ כן. מאחר ושתיהן מתארות קטע, המכפלה שלהן מתארת את החיתוך, ואז האינטגרל מתאר את אורך הקטע. בתכנה למטה מדסמוס אפשר לראות באדום את הפונקציה $g$ בירוק את $f$ והגרף השחור הוא של הקונבולוציה. נסו לשחק עם הפרמטרים $x,h$ ולראות שאתם מבינים למה גרף הקונבולוציה נראה כמו שהוא נראה. תרגיל: חשבו את הקונבולוציה של שתי פונקציות מציינות $f(x)=\\chi_{[-a,a]}$ ו $g(x)=\\chi_{[-b,b]}$ כפונקציה של הפרמטרים $a,b\\geq 0$ . תכונות של קונבולוציה לפני שנתחיל עם תכונות, נזכיר את משפט פוביני טונלי שעוזר בחישוב אינטגרלים: משפט פוביני טונלי. תהא $H(x,y):\\RR^2\\to \\CC$ פונקציה מדידה, אז $$\\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty |H(x,y)| \\dx\\right)\\dy = \\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty |H(x,y)| \\dy\\right)\\dx = \\int_{\\RR^2} |H(x,y)| \\operatorname{d(x,y)}$$ בנוסף, אם האינטגרלים האלו סופיים אז $$\\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty H(x,y) \\dx\\right)\\dy = \\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty H(x,y) \\dy\\right)\\dx = \\int_{\\RR^2} H(x,y) \\operatorname{d(x,y)}$$ באמצעות משפט פוביני טונלי, אנחנו יכולים להראות סגירות לכפל קונבולוציה. משפט: קונבולוציה של פונקציות ב $E^1(\\RR)$ . יהיו $f,g\\in E^1(\\RR)$ . אז הקונבולוציה $(f*g)(x)$ מוגדרת כמעט לכל $x$ והיא אינטגרבילית בהחלט. הוכחה: אם נניח שהקונבולוציה מוגדרת לכל $x$ , ונרצה להראות שהיא אינטגרבילית בהחלט, אז בגלל ש $$\\int_{-\\infty}^\\infty \\abs{(f*g)(x)}\\dx = \\int_{-\\infty}^\\infty \\abs{\\int_{-\\infty}^\\infty f(x-y)g(y)\\dy}\\dx \\leq \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\abs{f(x-y)}\\abs{g(y)} \\dy \\dx$$ מספיק שנראה שהאינטגרל האחרון הוא חסום ולא אינסוף. מצד שני, אם האינטגרל האחרון הוא סופי, זה אומר שהאינטגרל הראשון הוא סופי ובפרט $(f*g)(x)$ מוגדר וסופי כמעט לכל $x$ . נשים לב שהאינטגרנד בצד ימין הוא על פונקציה אי שלילית ולכן ניתן להפעיל את פוביני ולקבל שהוא שווה ל $$.\\int_{-\\infty}^\\infty \\abs{g(y)}\\left(\\int_{-\\infty}^\\infty \\abs{f(x-y)} \\dx\\right) \\dy\\overset{t=x-y}{=}\\int_{-\\infty}^\\infty \\abs{g(y)}\\left(\\int_{-\\infty}^\\infty \\abs{f(t)} \\dt\\right) \\dy = \\norm{f}_1\\norm{g}_1 הערה: כדאי לשים לב שאינטגרל של פונקציה יכול להיות לא קיים לא רק בגלל אי התכנסות בגבולות $\\pm \\infty$ של האינטגרציה, אלא גם בגלל שהפונקציה עצמה לא אינטגרבילית. למשל, עבור אינטגרל רימן, פונקציית דיריכלה המוגדרת ע”י $D(x)=\\cases{1 & x\\in \\QQ \\\\ 0 & else}$ היא כלל לא אינטגרבילית ללא קשר לגבולות האינטגרציה. דילגנו על החלק הזה בהוכחה, אבל באופן כללי זה גם משהו שצריך להראות. בנוסף, חשוב לשים לב שמה שמראים למעלה זה ש $\\abs{(f*g)(x)}$ מוגדרת וסופית כמעט לכל $x$ ולא לכל $x$ . למשל, נסתכל על הפונקציה $f(x)=\\frac{1}{\\sqrt{|x|+x^{10}}}$ . זוהי פונקציה רציפה שאינטגרבילית בהחלט מאחר ועבור $x$ -ים קטנים מתקיים ש $f(x)\\sim \\frac{1}{\\sqrt{|x|}}$ שהאינטגרל שלה מתכנס ליד הראשית, ועבור $x$ -ים גדולים מתקיים ש $f(x)\\sim \\frac{1}{\\abs{x}^5}$ שהאינטגרל שלה מתכנס באינסוף. המשפט למעלה מראה שהקונבולוציה $(f*f)(x)$ קיימת כמעט לכל $x$ , אבל אם נחשב את הקונבולוציה בראשית נקבל ש $$,(f*f)(0)=\\int_{-\\infty}^\\infty f(-y)f(y)\\dy=\\int_{-\\infty}^\\infty f(y)^2\\dy=\\int_{-\\infty}^\\infty \\frac{1}{|y|+y^{10}}\\dy=\\infty$$ וזה בגלל שעבור $y$ -ים קטנים מתקיים ש $f(y)^2 \\sim \\frac{1}{|y|}$ . לעומת זאת, לכל $x\\neq 0$ הקבונולוציה $(f*f)(x)$ מוגדרת וסופית. ניתן לראות את זה בתכנת דסמוס למטה: בסגול רואים את הפונקציה $f(y)$ בשחור את $f(x_0-y)$ ובאדום את המכפלה שלהם שעליה רוצים לעשות אינטגרציה. שימו לב שניתן להזיז את הנקודה האדומה למטה וכך לשלוט על $x_0$ . כל עוד $x_0\\neq 0$ השטח יחסית קטן, ורק כאשר $x_0=0$ אז שני המקומות בהן $f(y),f(x_0-y)$ גדלים לאינסוף נופלים אחד על השני, ואז גם השטח הוא אינסופי. בשלב הזה אנחנו יודעים שעבור פונקציות במרחב $E^1(\\RR)$ ניתן לחבר אותן ולכפול בסקלר (זה מרחב ווקטורי) ועכשיו גם אפשר לעשות בינהן קונבולוציה. כבר ראינו במקרה של פולינומים איך אפשר לכפל קונבולוציה מתוך מכפלה, ומסתבר שיש לה גם תכונות שאנחנו רגילים לשייך לכפל. תכונות אריתמטיות של קונבולוציה יהיו $f,g,h\\in E^1(\\RR)$ , ו $\\alpha \\in \\CC$ סקלר, אז: אסוציאטיביות: $f*(g*h)=(f*g)*h$ . קומוטטיביות: $f*g=g*f$ . לינאריות (דיסטריביוטיביות + כפל בסקלר): $f*(\\alpha g + h) = \\alpha f*g + f*h$ . נגזרת: אם $f$ גזירה ואם $f*g$ קיימת וגזירה, אז $(f*g)'(x)=(f'*g)(x)$ . התמרת פורייה של קונבולוציה משפט הקונבולוציה יהיו $f,g\\in E^1(\\RR)$ , אז $$.\\widehat{f*g}(\\omega) = 2\\pi\\hat {f}(\\omega) \\cdot \\hat{g}(\\omega)$$ הוכחה: נשים לב תחילה שבגלל ש $f,g\\in E^1(\\RR)$ אז אנחנו יודעים ש $f*g$ אינטגרבילית בהחלט ולכן יש לה התמרת פורייה, ששווה ל: $$.\\widehat{f*g}(\\omega)= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} (f*g)(x)e^{-i\\omega x}\\dx= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x-y)g(y)e^{-i\\omega x}\\dy\\dx$$ כבר ראינו שהאינטגרל הזה מתכנס בהחלט ולכן מותר לנו להחליף סדר אינטגרציה ולקבל את $$.\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} g(y) \\int_{-\\infty}^{\\infty} f(x-y)e^{-i\\omega x}\\dx\\dy\\overset{x=t+y}{=}\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} g(y)e^{-i\\omega y} \\int_{-\\infty}^{\\infty} f(t)e^{-i\\omega t}\\dt\\dy=2\\pi \\hat{f}(\\omega) \\hat{g}(\\omega)$$ דוגמא: הקונבולוציה של $e^{-x^2}*e^{-x^2}$ . הרעיון פה הוא שבעוד קונבולוציה זו פעולה “מסובכת” לחשב, אם נבצע לה התמרה אז זה יהפוך לפשוט כפל פונקציות, ואז אפשר לעשות התמרה הפוכה כדי לחזור לפונקציה שרצינו לחשב, או במילים אחרות $$.(f*f)(x)=2\\pi\\cf\\left[\\cf[f*f]\\right](-x)$$ מבחינה פורמלית, נסמן ב $f(x)=e^{-x^2}$ נזכיר שראינו ש $\\cf[f](\\omega)=\\frac{1}{2\\sqrt{\\pi}} e^{-\\frac{\\omega^2}{4}}$ . שימוש במשפט הקונבולוציה יתן לנו ש $$.\\cf[f*f](\\omega)=2\\pi\\cf[f]^2(\\omega)=\\frac{1}{2} e^{-\\frac{\\omega^2}{2}}=\\frac{1}{2}f(\\frac{\\omega}{\\sqrt{2}})$$ נחשב התמרה נוספת, תוך שימוש בתכונת ההתמרה של כפל המשתנה בסקלר ונקבל ש $$.2\\pi\\cf[\\cf[f*f]](-x)=\\pi \\cf\\left[f(\\frac{\\omega}{\\sqrt{2}})\\right](-x)=\\sqrt{2}\\pi \\cf\\left[f\\right](-\\sqrt{2}x)=\\sqrt{\\frac{\\pi}{2}} e^{-\\frac{x^2}{2}}$$ דוגמא: חסימות בתדר אנחנו יודעים שבהינתן $f\\in E^1(\\RR)$ הלמה של רימן לבג אומרת ש $\\hat{f}(\\omega)\\to 0$ כש $\\abs{\\omega} \\to \\infty$ . כלומר, במובן מסויים רב ה”אנרגיה” של הפונקציה נמצאת בתדרים הנמוכים (כלומר $\\abs{\\omega}$ קטן). האם נוכל להפוך את הפונקציה ליותר “נקייה” במובן שנשתיק את התדרים המאוד גדולים? או בצורה יותר פורמלית, האם עבור $M>0$ קיימת פונקציה $f_M$ כך ש $$? \\;\\;\\;\\widehat{f_M}(\\omega)=\\cases{\\hat{f}(\\omega) & |\\omega|\\leq M \\\\ 0 & |\\omega|>M}$$ נשים לב, שהדרישות על $\\widehat{f_M}$ בעצם אומרות שההתמרה שלה צריכה להיות $\\widehat{f_M}=\\hat{f}\\cdot \\chi_{[-M,M]}$ ופה נוכל להעזר במשפט הקונבולוציה. נזכיר ש $\\cf[\\chi_{[-M,M]}](\\omega)=\\frac{\\sin(\\omega M)}{\\omega \\pi}$ , ואם נשתמש בהתמרה ההפוכה (עבור פונקציות ב $E^2$ ) נקבל ש $\\cf[\\frac{2\\sin(xM)}{x}] = \\chi_{[-M,M]}$ ולכן $$\\cf\\left[ (f*\\frac{\\sin(xM)}{\\pi x})\\right](\\omega) = \\cf\\left[ f\\right](\\omega) \\cdot \\cf\\left[ \\frac{2\\sin(xM)}{x}\\right]= \\cf\\left[ f\\right](\\omega) \\cdot \\chi_{[-M,M]}(\\omega)$$"
  }}
